<!DOCTYPE html><html lang="en" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Jifeng Wu"><meta name="keywords" content=""><meta name="description" content="Despite the intuitive appeal of variational autoencoders (VAEs), their underlying principles can be elusive. After extensive research across papers and online resources, I will summarize the core insi"><meta property="og:type" content="article"><meta property="og:title" content="Understanding the Name, Structure, and Loss Function of the Variational Autoencoder"><meta property="og:url" content="https://jifengwu2k.github.io/2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/index.html"><meta property="og:site_name" content="Jifeng Wu&#39;s Personal Website"><meta property="og:description" content="Despite the intuitive appeal of variational autoencoders (VAEs), their underlying principles can be elusive. After extensive research across papers and online resources, I will summarize the core insi"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2023-09-30T04:00:00.000Z"><meta property="article:modified_time" content="2025-08-13T04:31:00.461Z"><meta property="article:author" content="Jifeng Wu"><meta name="twitter:card" content="summary_large_image"><title>Understanding the Name, Structure, and Loss Function of the Variational Autoencoder - Jifeng Wu&#39;s Personal Website</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"jifengwu2k.github.io",root:"/",version:"1.9.3",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Jifeng Wu's Personal Website" type="application/atom+xml">
</head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Jifeng Wu</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> Home</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> Categories</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> Archives</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">Understanding the Name, Structure, and Loss Function of the Variational Autoencoder</span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> Jifeng Wu </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-09-30 00:00" pubdate>September 30, 2023</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.3k words </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 45 mins</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">Understanding the Name, Structure, and Loss Function of the Variational Autoencoder</h1><div class="markdown-body"><p>Despite the intuitive appeal of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variational_autoencoder">variational autoencoders (VAEs)</a>, their underlying principles can be elusive. After extensive research across papers and online resources, I will summarize the core insights behind the VAE's name, structure, and loss function and try to explain <strong>how the mathematical formulas used to describe the VAE came into being from first principles</strong>, as opposed to simply providing interpretations for them.</p><h2 id="basics-of-vaes">Basics of VAEs</h2><p>VAEs are probabilistic generative models, when trained on a dataset <span class="math inline">\(X\)</span>, allow us to sample from a latent variable <span class="math inline">\(Z\)</span> and generate output resembling samples in <span class="math inline">\(X\)</span> through a trained neural network <span class="math inline">\(f: Z \rightarrow X\)</span>.</p><p>This can be formulated as making the probability of generating <span class="math inline">\(X = x\)</span> as close as possible to the actual <span class="math inline">\(P(X = x)\)</span> (known quality) under the entire generative process.</p><h2 id="ideal-training-goal">Ideal Training Goal</h2><p>In the <strong>ideal situation</strong>, based on the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal distribution formula</a> we have <span class="math inline">\(P(X = x) = \int{P(X = x | Z = z) P(Z = z) dz}\)</span>. Thus, the training goal of variational autoencoders is to <strong>make the actual <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> as close to <span class="math inline">\(P(X = x)\)</span> as possible</strong>.</p><h2 id="latent-variable-distribution">Latent Variable Distribution</h2><p>VAEs select a <strong>multivariate normal distribution</strong> for the latent variable <span class="math inline">\(Z\)</span> based on the principle that <a target="_blank" rel="noopener" href="https://doi.org/10.1145/318242.318443">any distribution in <span class="math inline">\(d\)</span> dimensions can be generated by mapping normally distributed variables through a sufficiently complicated function</a>, which could be approximated using the neural network <span class="math inline">\(f: Z \rightarrow X\)</span> we train.</p><h2 id="approximation-challenge">Approximation Challenge</h2><p>Having reasonably decided <span class="math inline">\(Z \sim N(0, I)\)</span>, we may calculate the actual <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span>. This is straightforward to approximate: we can randomly sample a large number of <span class="math inline">\(Z\)</span> values <span class="math inline">\(\{z_1, \dots, z_n\}\)</span>, and approximate <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> as <span class="math inline">\(\sum_{j}^{n}{P(X = x | Z = z_j)}\)</span>.</p><p>However, for most <span class="math inline">\(Z\)</span> values, <span class="math inline">\(P(X = x | Z)\)</span> will be nearly zero, contributing almost nothing to our calculation. This is especially the case in high dimensional spaces, for which an extremely large number of samples of <span class="math inline">\(Z\)</span> may be required.</p><p>To address the problem, we can attempt to <strong>sample values of <span class="math inline">\(Z\)</span> that are likely to have produced <span class="math inline">\(X = x\)</span> and compute <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> just from those</strong>.</p><h2 id="the-variational-aspect">The "Variational" Aspect:</h2><p>To do so, we can <strong>fit another parametrized function</strong> <span class="math inline">\(Q(Z | X = x)\)</span>, which can give us a distribution over <span class="math inline">\(Z\)</span> values that are likely to produce <span class="math inline">\(X = x\)</span> through <span class="math inline">\(f: Z \rightarrow X\)</span> given <span class="math inline">\(X = x\)</span>. This is an example of a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayesian method</a>, which involves finding an "optimal" function (a task known as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Calculus_of_variations">variational calculus</a>) and is the source of the word "variational" in variational autoencoders.</p><h2 id="minimizing-divergence">Minimizing Divergence</h2><p>Theoretically, the values of <span class="math inline">\(Z\)</span> that are likely to have produced <span class="math inline">\(X = x\)</span> follow the conditional distribution <span class="math inline">\(P(Z | X = x)\)</span>. <strong>Thus, our original goal of making the actual <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> as close to <span class="math inline">\(P(X = x)\)</span> as possible can be transformed to minimizing the Kullback-Leibler divergence between <span class="math inline">\(P(Z | X = x)\)</span> and <span class="math inline">\(Q(Z | X = x)\)</span></strong>:</p><p><span class="math display">\[KL(Q(Z | X = x) || P(Z | X = x)) = \int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x)}{P(Z = z | X = x)}} dz}\]</span></p><p>According to Bayes' Law,</p><p><span class="math display">\[P(Z = z | X = x) = \frac{P(X = x | Z = z) P(Z = z)}{P(X = x)}\]</span></p><p>Thus, we have:</p><p><span class="math display">\[\int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x) P(X = x)}{P(X = x | Z = z) P(Z = z)}} dz}\]</span></p><p><span class="math display">\[= \int{Q(Z = z | X = x) (\log{\frac{Q(Z = z | X = x)}{P(Z = z)}} + \log{P(X = x)} - \log{P(X = x | Z = z)}) dz}\]</span></p><p><span class="math display">\[= \int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} + \int{Q(Z = z | X = x) \log{P(X = x)} dz} - \int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\]</span></p><p>Note that:</p><p><span class="math display">\[\int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} = KL(Q(Z | X = x) || P(Z))\]</span></p><p><span class="math display">\[\int{Q(Z = z | X = x) \log{P(X = x)} dz} = \log{P(X = x)} \int{Q(Z = z | X = x)} dz = \log{P(X = x)}\]</span></p><p>Thus, we have:</p><p><span class="math display">\[KL(Q(Z | X = x) || P(Z | X = x)) = KL(Q(Z | X = x) || P(Z)) + \log{P(X = x)} - \int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\]</span></p><p>As <span class="math inline">\(\log{P(X = x)}\)</span> is constant, if we were to minimize <span class="math inline">\(KL(Q(Z | X = x) || P(Z | X = x))\)</span>, we should minimize:</p><p><span class="math display">\[KL(Q(Z | X = x) || P(Z)) - \int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\]</span></p><p>To further transfer that into a calculatable function, we need to be more specific about the form that <span class="math inline">\(Q(Z | X)\)</span> will take. The usual choice is to say that <span class="math inline">\(Q(Z | X = x) = N(Z | \mu(X = x), \Sigma(X = x))\)</span>, i.e., <span class="math inline">\(Q(Z | X = x)\)</span> follows a Gaussian distribution where the mean and covariance matrix are calculated by <strong>parameterized functions (trained neural networks)</strong> given <span class="math inline">\(X = x\)</span>. In this case, <strong>fitting <span class="math inline">\(Q(Z | X = x)\)</span> involves training these neural networks</strong>.</p><figure><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*kXiln_TbF15oVg7AjcUEkQ.png" srcset="/img/loading.gif" lazyload alt="Q(Z | X = x) = N(Z | \mu(X = x), \Sigma(X = x))"><figcaption aria-hidden="true"><span class="math inline">\(Q(Z | X = x) = N(Z | \mu(X = x), \Sigma(X = x))\)</span></figcaption></figure><p>The advantages of this choice are <em>computational</em>, as <span class="math inline">\(KL(Q(Z | X = x) || P(Z)) + \log{P(X = x)}\)</span> is now <strong>a KL-divergence between two multivariate Gaussian distributions</strong>, which can be computed in <strong>closed form</strong>.</p><p>As for <span class="math inline">\(\int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\)</span>, it depicts the expected log-likelihood of generating <span class="math inline">\(X = x\)</span> as the VAE's output through <span class="math inline">\(f(Z)\)</span> when sampling from <span class="math inline">\(Q(Z = z | X = x)\)</span> given <span class="math inline">\(X = x\)</span>. Thus, it can be treated as the "reconstruction loss" of the VAE, and different closed-form indices, such as mean square error, may be used as proxies of it depending on the project domain.</p><h2 id="why-autoencoders">Why "Autoencoders"?</h2><p>Despite the mathematical basis of VAEs being quite different from classical autoencoders, they are named "autoencoders" due to their final training objective involving an encoder (the neural networks <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> determining mean and covariance) and a decoder (the neural network <span class="math inline">\(f\)</span>), which resembles a traditional autoencoder in structure.</p><h2 id="references">References</h2><ul><li>https://arxiv.org/abs/1606.05908</li><li>https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/</li><li>https://arxiv.org/abs/1312.6114</li><li>https://arxiv.org/abs/1907.08956</li><li></li><li>https://stats.stackexchange.com/questions/485488/should-reconstruction-loss-be-computed-as-sum-or-average-over-input-for-variatio</li><li>https://stats.stackexchange.com/questions/540092/how-do-we-get-to-the-mse-in-the-loss-function-for-a-variational-autoencoder</li><li>https://stats.stackexchange.com/questions/464875/mean-square-error-as-reconstruction-loss-in-vae</li><li>https://stats.stackexchange.com/questions/323568/help-understanding-reconstruction-loss-in-variational-autoencoder</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Mathematics/" class="category-chain-item">Mathematics</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>Understanding the Name, Structure, and Loss Function of the Variational Autoencoder</div><div>https://jifengwu2k.github.io/2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>Jifeng Wu</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>September 30, 2023</div></div><div class="license-meta-item"><div>Licensed under</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/" title="Pre-MICCAI Workshop@UBC Observations and Gained Insights"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Pre-MICCAI Workshop@UBC Observations and Gained Insights</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/2023/09/24/My-Software-Engineering-Philosophy/" title="My Software Engineering Philosophy"><span class="hidden-mobile">My Software Engineering Philosophy</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><script type="text/javascript">Fluid.utils.loadComments("#comments",(function(){var t=document.documentElement.getAttribute("data-user-color-scheme");t="dark"===t?"github-dark":"github-light",window.UtterancesThemeLight="github-light",window.UtterancesThemeDark="github-dark";var e=document.createElement("script");e.setAttribute("src","https://utteranc.es/client.js"),e.setAttribute("repo","abbaswu/utterances"),e.setAttribute("issue-term","pathname"),e.setAttribute("label","utterances"),e.setAttribute("theme",t),e.setAttribute("crossorigin","anonymous"),document.getElementById("comments").appendChild(e)}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><script>Fluid.utils.createScript("https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js",(function(){mermaid.initialize({theme:"default"}),Fluid.events.registerRefreshCallback((function(){"mermaid"in window&&mermaid.init()}))}))</script><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Search</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Keyword</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript></body></html>