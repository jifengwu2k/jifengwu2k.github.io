{"meta":{"title":"Jifeng Wu's Personal Website","subtitle":"Jifeng Wu's Personal Website","description":"Jifeng Wu's Personal Website","author":"Jifeng Wu","url":"https://jifengwu2k.github.io","root":"/"},"pages":[{"title":"Home","date":"2025-05-20T04:00:00.000Z","updated":"2025-09-26T17:24:23.507Z","comments":false,"path":"index.html","permalink":"https://jifengwu2k.github.io/index.html","excerpt":"","text":"I am a computer science researcher focusing on making systems more verifiable, efficient, and accessible by bridging programming languages, formal methods, software engineering, and compilers. I obtained my Master's of Science in Computer Science from UBC working with Caroline Lemieux. My Master's thesis \"QuAC: Quick Attribute-Centric Type Inference for Python\" (OOPSLA'24) implemented QuAC, a novel Python type inference tool that leverages attribute sets and information retrieval techniques to predict types. Before that, I obtained my Bachelor of Engineering from the School of Computer Science, Wuhan University under the guidance of Qingan Li. My Bachelor's thesis \"Effective Stack Wear Leveling for NVM\" (TCAD'23) proposed Loop2Recursion, a compiler-assisted stack wear leveling technique implemented as an LLVM pass for increasing the lifespan of NVM by converting wear-heavy loops into recursive functions. I have also conducted research in graph and trajectory data mining, advised by Yuanyuan Zhu. Outside research, I enjoy blogging, contributing to open-source projects, and mentoring students. Blog | Email | Facebook | GitHub | Instagram | LinkedIn | PyPI | Resume | WeChat | X Utilities: PyPI Packages Clipboard to Markdown Webcam Viewer References: 座右铭 简易食谱 简易运动 Python Coding Guidelines C++ Coding Guidelines High-Frequency Git Operations for Everyday Development Emoji Keyboard/Display Test Data for UTS #51 Papers QuAC: Quick Attribute-Centric Type Inference for Python (Master's Thesis) We implemented QuAC, a novel type inference tool for Python that collects attribute sets for Python expressions and uses information retrieval techniques to predict classes. Compared to baseline methods, QuAC efficiently handles rare non-builtin types and container type parameters and improves performance by an order of magnitude. Accepted to: OOPSLA 2024 Mentor: Prof. Caroline Lemieux Time: July 2023 - August 2024 Paper GitHub Repository Lessons Learned Effective Stack Wear Leveling for NVM (Bachelor's Thesis) We increase the lifespan of non-volatile memory with limited write durability by implementing an LLVM pass that can convert wear-heavy loops in programs into recursive functions. Published in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2023) Authors: Jifeng Wu, Wei Li, Libing Wu, Mengting Yuan, Chun Jason Xue, Jingling Xue, Qingan Li Mentor: Prof. Qingan Li Time: August 2021 - August 2022 Paper GitHub Repository Essays Community Detection Using Social Relations and Trajectories Community detection is an essential task in social network analysis, but many friends on social networks are not close to one another in the real world. We introduce a novel approach that utilizes user trajectories to identify cohesive groups of users who frequently hang out together and presents algorithms for efficiently calculating spatiotemporal similarity between trajectories and community detection. Mentor: Prof. Yuanyuan Zhu Time: September 2019 - June 2021 Essay GitHub Repository Friends Zhaowei Zhang @ Peking University Bingchu Zhao @ KTH Ziqi Rong @ University of Michigan Jianhao Wu @ CUHK Hao Wang @ HKUST"},{"title":"","date":"2023-08-22T15:47:02.623Z","updated":"2023-08-22T15:47:02.623Z","comments":false,"path":"categories/index.html","permalink":"https://jifengwu2k.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2023-08-22T15:47:02.623Z","updated":"2023-08-22T15:47:02.623Z","comments":false,"path":"projects/index.html","permalink":"https://jifengwu2k.github.io/projects/index.html","excerpt":"","text":""},{"title":"","date":"2023-08-22T15:47:02.623Z","updated":"2023-08-22T15:47:02.623Z","comments":false,"path":"tags/index.html","permalink":"https://jifengwu2k.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"C++ Coding Guidelines","slug":"C-Coding-Guidelines","date":"2025-09-15T04:00:00.000Z","updated":"2025-09-15T15:54:29.815Z","comments":true,"path":"2025/09/15/C-Coding-Guidelines/","link":"","permalink":"https://jifengwu2k.github.io/2025/09/15/C-Coding-Guidelines/","excerpt":"","text":"Guiding Philosophy Prioritize modularity, clarity, and maintainability over premature optimization or legacy patterns. Code should be readable, easily understood, and safe by default. Favor Python-style expressiveness with modern C++ strengths: Use strong type systems, STL containers, concepts, range-based and higher-order functions. Avoid C-style loops, macros, and pointer tricks that hide intent. Make the compiler's job as easy as possible: Clear ownership, resource lifetime, and data flow allow compilers to optimize code aggressively. By avoiding hidden dependencies and using STL idioms, enable the compiler to perform superior vectorization, inlining, and memory/layout optimizations. Design for trivial portability to GPU, FPGA, and other accelerators: Functional, range-based, and stateless/RAII idioms map directly to bulk-parallel and dataflow hardware. State machines using std::variant or explicit types, instead of enums and scattered flags, make auto-parallelization and offloading straightforward. Consistent type size/platform rules yield code predictable for high-level synthesis and cross-ISA portability. By steering clear of manual low-level hacks, code lifts cleanly to modern frameworks like CUDA, SYCL, or high-level FPGA toolchains. C++ is treated as a platform-level language like Python. The guidelines do not merely govern surface syntax and C++ standard features, but unambiguously specify expectations for the entire execution environment: ABI (Application Binary Interface) OS and system libraries (e.g. POSIX semantics, thread/process/signaling behavior) Consistency of type sizes, object layout, exception handling, RTTI, and dynamic loading Deterministic handling and semantics for all STL and language/library features Lexical &amp; Filing Conventions Indentation Always use 4 spaces per indentation level (Python style). Tabs, 2-space or 8-space indents are forbidden. Functions &amp; Variables All function and variable names (including members): snake_case, all lowercase. int max_value;, double amount_sum; No CamelCase, PascalCase, or mixing. Types/Classes/Structs Class, struct, and enum names: CamelCase (first letter of each word capitalized), no underscores. class FileReader &#123; ... &#125;;, struct UserInfo &#123; ... &#125;; File Extensions .c: C-style source code. .m, .mm: Objective-C style source code. We explicitly allow and encourage the usage of POSIX-compliant Objective-C OUTSIDE the Cocoa framework, whenever Python-style \"look up by name\" is required. .cpp: C++-style source code. .h: Only interface declarations, never function/template bodies. .hpp: Can include full implementations, templates, inline functions. No other file extensions. Include Guards Every header (.h, .hpp) must use an include guard named as the file name in all caps, dots replaced with underscores, with no added prefix or suffix, e.g., foo_bar.h - #ifndef FOO_BAR_H my_class.hpp - #ifndef MY_CLASS_HPP File Naming All source/header files: snake_case, all lowercase, use underscores, e.g.: network_manager.h, data_loader.cpp No CamelCase, hyphens, or uppercase. Type System and Data Model Type Size Assumptions Type Bytes char 1 short 2 int 4 wchar_t 4 float 4 long 8 double 8 Any violation makes the platform \"unsupported\". Struct vs Class Semantics Struct: For plain data (POD). Structs NEVER own resources, nor clean up. Always default-initializable to valid state. No custom Big Five (destructor, copy constructor, etc.) Never allocate struct on the heap (no new StructType). They must live on stack or inside containers/smart pointers. Do not store pointers/references to struct instance for long-term use. Class: For resource management. Must define explicit constructor(s) and Big Five. No raw pointers escape outside internal implementation; prefer smart pointers/containers. Function Parameters If struct/class is 16 bytes: always pass by reference (const T&amp;/T&amp;/T&amp;&amp; and std::move). Value passing only for simple types and small STL types. Strings &amp; Unicode std::string: Always UTF-8 encoded. std::wstring (POSIX): Each wchar_t holds a decoded unicode codepoint; use for character-based logic. Serialize or send as UTF-8 (std::wstring -&gt; std::string) for output/network. Subtyping Use C++20 concepts and duck typing with templates, NOT class inheritance, whenever possible. Prioritize using the standard library's concepts, and use them according to Python conventions. See Appendix for details. Functors Always use functors (including lambdas) for generalization/high-order logic. C-style function pointers are only allowed for low-level ABI/C-interfacing. Container &amp; Data Structure Policy Always prefer STL: vector, map, set, optional, variant, etc. Only use third-party containers (boost, absl, folly) with full documentation and technical review. Never hand-write data structures for \"preemptive optimization\". Operational Semantics Control Flow Strongly discourage the use of counting-based for-loops. Hard to read, hard to optimize. Only allow goto for control flow in plain-POD non-owner code, never with objects needing destruction. Use modern functional idioms (map/reduce/filter/apply_if) whenever possible. Use explicit patterns like State Machine whenever possible. Important program states must use concrete types and variants, never enums+switches+separate local variables, e.g., struct IdleState &#123; /* ... */ &#125;; struct RunningState &#123; /* ... */ &#125;; using State = std::variant&lt;IdleState, RunningState&gt;; Use C++20 coroutines for complex control flow. Follow Python generator conventions (co_yield = yield, \"delegating\" = yield from). Document coroutines in Pythonic style for zero cognitive gap. Exceptions &amp; Error Handling Exception Policy Use exceptions as standard; do not rewrite everything to error codes or forbid exceptions. Only exclude exceptions in hard-constrained \"special\" cases (must be fully justified &amp; documented). Fail Fast, Fail Loudly On detection of error/inconsistency, immediately abort/throw/assert with full context. Never clip, silence, or tolerate errors unless in controlled, reviewed, and documented edge cases. Platform &amp; Build System Platform, ABI Only POSIX-conformant platforms with full POSIX C API for file/IO, threads, sockets, signals, etc. are supported. Only Itanium C++ ABI is supported. Build Toolchain All build/link flows use GCC-style CLI tools, preferably LLVM/Clang. Assume availability of all GCC extensions. Always specify all needed options/libraries, e.g.: -pthread (threads) -lc++ (clang libc++) -ldl (dlopen) Intermediate Representation Always use LLVM IR (.ll, .bc) as the intermediate layer for cross-platform, analysis, optimization. Never use native assembly. Appendix: C++ Concepts vs Python Typing Constructs Equality, Ordering C++ Concept Example Meaning Python Typing Equivalent std::equality_comparable&lt;T&gt; equality_comparable&lt;MyType&gt; Can test equality (==, !=) typing.SupportsRichComparison std::totally_ordered&lt;T&gt; totally_ordered&lt;MyType&gt; All &lt;, &lt;=, &gt;, &gt;= comparisons typing.SupportsRichComparison Numeric and Mathematical Types C++ Concept Example Meaning Python Typing Equivalent std::integral&lt;T&gt; integral&lt;int&gt; Is an integral (integer) type numbers.Integer std::signed_integral&lt;T&gt; signed_integral&lt;int&gt; Is a signed integer type std::unsigned_integral&lt;T&gt; unsigned_integral&lt;uint32_t&gt; Is an unsigned integer type std::floating_point&lt;T&gt; floating_point&lt;double&gt; Is a floating point type numbers.Real Containers C++ Concept Example Meaning Python Typing Equivalent std::ranges::range&lt;T&gt; range&lt;std::vector&lt;int&gt;&gt; Iterable range concept typing.Iterable[T] std::ranges::input_range&lt;T&gt; input_range&lt;std::istream&gt; Readable, single pass typing.Iterator[T] std::ranges::sized_range&lt;T&gt; sized_range&lt;std::array&lt;int, 3&gt;&gt; Has a known size (.size()) typing.Sized std::ranges::output_range&lt;T, V&gt; Writable range (output iterators) typing.MutableSequence[T] std::ranges::view&lt;T&gt; Lightweight, non-owning range std::input_iterator&lt;T&gt; input_iterator&lt;Iter&gt; Supports ++, deref, read typing.Iterator[T] std::forward_iterator&lt;T&gt; forward_iterator&lt;Iter&gt; Multi-pass input iterator typing.Iterator[T] std::bidirectional_iterator&lt;T&gt; bidirectional_iterator&lt;Iter&gt; Forward/backward iteration typing.Sequence[T] std::random_access_iterator&lt;T&gt; random_access_iterator&lt;Iter&gt; Supports it[n] indexing, etc. typing.Sequence[T] std::contiguous_iterator&lt;T&gt; contiguous_iterator&lt;Iter&gt; Underlying data is contiguous in memory typing.Sequence[T] Callables C++ Concept Example Meaning Python Typing Equivalent std::invocable&lt;F, Args...&gt; invocable&lt;Fn, int&gt; Callable object (function, lambda, etc.) typing.Callable[..., T] std::predicate&lt;F, Args...&gt; predicate&lt;Fn, int&gt; Callable returns bool typing.Callable[..., bool] Type Identity and Conversion C++ Concept Example (C++) Meaning Python typing Equivalent std::same_as&lt;T, U&gt; same_as&lt;int, int&gt; Types are exactly the same std::convertible_to&lt;From, To&gt; convertible_to&lt;int, float&gt; Can be converted (static_cast&lt;To&gt;(from)) std::derived_from&lt;D, B&gt; derived_from&lt;class A, class Base&gt; D inherits from B std::constructible_from&lt;T, Args...&gt; constructible_from&lt;std::string, const char*&gt; Constructible from given args typing.Callable for constructors std::default_initializable&lt;T&gt; Default (no-arg) constructable Copying, Moving, Assignment &amp; Swap C++ Concept Example Meaning Python Typing Equivalent std::move_constructible&lt;T&gt; move_constructible&lt;MyType&gt; Supports move semantics std::copy_constructible&lt;T&gt; copy_constructible&lt;MyType&gt; Can be copy-constructed std::assignable_from&lt;T, U&gt; assignable_from&lt;MyType&amp;, int&gt; lhs can be assigned rhs std::swappable&lt;T&gt; swappable&lt;MyType&gt; Can exchange values (swap) std::swappable_with&lt;T, U&gt; swappable_with&lt;A, B&gt; Can swap values with another type","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"古诗词现代解读","slug":"古诗词现代解读","date":"2025-09-12T04:00:00.000Z","updated":"2025-09-13T00:01:27.065Z","comments":true,"path":"2025/09/12/古诗词现代解读/","link":"","permalink":"https://jifengwu2k.github.io/2025/09/12/%E5%8F%A4%E8%AF%97%E8%AF%8D%E7%8E%B0%E4%BB%A3%E8%A7%A3%E8%AF%BB/","excerpt":"","text":"朝搴阰之木兰兮，夕揽洲之宿莽。 乘骐骥以驰骋兮，来吾道夫先路！ 朝饮木兰之坠露兮，夕餐秋菊之落英。 回朕车以复路兮，及行迷之未远。 步余马于兰皋兮，驰椒丘且焉止息。 忽反顾以游目兮，将往观乎四荒。 驷玉虬以桀鹥兮，溘埃风余上征。 朝发轫于苍梧兮，夕余至乎县圃。 览相观于四极兮，周流乎天余乃下。 朝发轫于天津兮，夕余至乎西极。 凤皇翼其承旗兮，高翱翔之翼翼。 驾八龙之婉婉兮，载云旗之委蛇。 抑志而弭节兮，神高驰之邈邈。 陟升皇之赫戏兮，忽临睨夫旧乡。 ——屈原 现代解读： 沿江河步行， 驾车翻越山岗， 然后飞上三万五千英尺的高空， 俯瞰荆楚大地郁郁葱葱的照叶树林。 穷发之北，有冥海者，天池也……背若泰山，翼若垂天之云，抟扶摇羊角而上者九万里，绝云气，负青天，然后图南，且适南冥也。 天之苍苍，其正色邪？其远而无所至极邪？其视下也，亦若是则已矣。且夫水之积也不厚，则其负大舟也无力……风之积也不厚，则其负大翼也无力。故九万里，则风斯在下矣…… ——庄子 北海虽赊，扶摇可接。东隅已逝，桑榆非晚。 ——王勃 现代解读： 从中原到贝加尔湖乃至更远的北极地区，是跨越数千公里的超远距离。 “扶摇”是想象中的、威力无穷的自然之力，而科技将古典文学中最瑰丽的想象变成了现实。 在百万英尺的高空，空气稀薄，阻力极小，飞机可以高效地进行高超音速巡航，实现极速远距离飞行。 在这个高度，飞行员所见正是永恒的夜色，脚下是云层或微茫的大地，星辰仿佛触手可及。 Deniz üstü köpürür ... Benim de şu cihana gelişim ... Deniz üstü yelkenden ... Denizin ortasında ... Benim de bu cihandan gidişim Kayığa da binsem götürür ... Bir güzelden ötürü ... Ecel geldi erkenden ... Mum yanar sofrasında ... Memleket sevdasından Hey canım, hey ... Hey canım, hey ... Hey canım, hey ... Hey canım, hey ... Hey canım, hey Cem Karaca 现代解读： 运用比兴及复沓的手法，一唱三叹，表达其“道不行，乘桴浮于海”，政治上落魄失意。","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"Running Local LLMs with Ollama","slug":"Running-Local-LLMs-with-Ollama","date":"2025-09-05T04:00:00.000Z","updated":"2025-09-06T22:09:59.400Z","comments":true,"path":"2025/09/05/Running-Local-LLMs-with-Ollama/","link":"","permalink":"https://jifengwu2k.github.io/2025/09/05/Running-Local-LLMs-with-Ollama/","excerpt":"","text":"Large Language Models (LLMs) have revolutionized AI, but cloud-based solutions often come with privacy concerns and usage limitations. Ollama provides an elegant solution by enabling you to run LLMs locally on your machine. This guide will walk you through the entire process from installation to interaction. What is Ollama? Ollama is a lightweight, extensible framework that allows you to run various open-source LLMs on your local hardware. It handles model weights, configurations, and provides a simple API interface similar to OpenAI's. Installation Process Download Ollama Open your terminal and execute the following command to download the latest version of Ollama: 1wget https://ollama.com/download/ollama-linux-amd64.tgz Install Ollama Extract the downloaded archive to your system directory: 1sudo tar -C /usr -xzvf ollama-linux-amd64.tgz Starting the Ollama Service Launch the Ollama service with this simple command: 1ollama serve Keep this terminal session active to maintain the service. Verifying Your Installation Open a new terminal window and verify that Ollama is properly installed and running: 1ollama -v This command will display the installed version of Ollama, confirming successful installation. Finding the Right Model Ollama supports numerous models with different capabilities and sizes. Browse the Ollama Model Library to explore available options. Downloading a Model For this example, we'll download the gemma3:4b model: 1ollama pull gemma3:4b The download process may take considerable time depending on your Internet connection speed and the model size. Terminal-Based Conversation Once your model is downloaded, you can start interacting with it directly through the terminal: 1ollama run gemma3:4b This command launches an interactive chat session with your model. Press Ctrl-D to exit the conversation when finished. Programmatic Access via Python API Ollama provides an OpenAI-compatible API, making it easy to integrate with existing applications and scripts. Install a client library: 1pip install chat-completions-conversation Use the client library: 12345678910from chat_completions_conversation import ChatCompletionsConversationconversation = ChatCompletionsConversation( api_key=&#x27;&#x27;, base_url=&#x27;http://localhost:11434/v1&#x27;, model=&#x27;gemma3:4b&#x27;)conversation.send_and_receive_response(&#x27;Hello, how are you?&#x27;)# Model returns a string","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://jifengwu2k.github.io/categories/Data-Science/"}],"tags":[]},{"title":"穿搭技巧","slug":"穿搭技巧","date":"2025-09-03T04:00:00.000Z","updated":"2025-09-03T18:00:03.306Z","comments":true,"path":"2025/09/03/穿搭技巧/","link":"","permalink":"https://jifengwu2k.github.io/2025/09/03/%E7%A9%BF%E6%90%AD%E6%8A%80%E5%B7%A7/","excerpt":"","text":"怎么穿“显高”！","categories":[{"name":"Fashion","slug":"Fashion","permalink":"https://jifengwu2k.github.io/categories/Fashion/"}],"tags":[]},{"title":"PyPI Packages","slug":"PyPI-Packages","date":"2025-08-31T04:00:00.000Z","updated":"2025-09-28T20:59:45.789Z","comments":true,"path":"2025/08/31/PyPI-Packages/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/31/PyPI-Packages/","excerpt":"","text":"graph LR posix_or_nt --&gt; textcompat textcompat --&gt; chat_completions_conversation intersperse --&gt; chat_completions_conversation canonical_interval --&gt; canonical_range canonical_range --&gt; determine_slice_assignment_action canonical_interval --&gt; determine_slice_assignment_action fixed_width_int --&gt; tuplehash canonical_range --&gt; cowlist determine_slice_assignment_action --&gt; cowlist tuplehash --&gt; cowlist cowlist --&gt; tree_traversals rawattr --&gt; less_than_key cowlist --&gt; prefix_free_sorted_cowlist_set put_back_iterator --&gt; prefix_free_sorted_cowlist_set canonical_range --&gt; sorted_fractionally_indexed_cowlist_set cowlist --&gt; sorted_fractionally_indexed_cowlist_set generalized_range --&gt; sorted_fractionally_indexed_cowlist_set posix_or_nt --&gt; read_unicode_environment_variables_dictionary posix_or_nt --&gt; get_unicode_shell read_unicode_environment_variables_dictionary --&gt; get_unicode_shell posix_or_nt --&gt; get_unicode_home read_unicode_environment_variables_dictionary --&gt; get_unicode_home posix_or_nt --&gt; find_unicode_executable read_unicode_environment_variables_dictionary --&gt; find_unicode_executable find_unicode_executable --&gt; get_unicode_arguments_to_launch_editor posix_or_nt --&gt; get_unicode_arguments_to_launch_editor read_unicode_environment_variables_dictionary --&gt; get_unicode_arguments_to_launch_editor split_command_line --&gt; get_unicode_arguments_to_launch_editor file_to_unicode_base64_data_uri send_recv_json cowlist --&gt; escape_nt_command_line_argument hwc_chw_ndarray_conversion escape_nt_command_line_argument --&gt; ctypes_unicode_proclaunch find_unicode_executable --&gt; ctypes_unicode_proclaunch posix_or_nt --&gt; ctypes_unicode_proclaunch read_unicode_environment_variables_dictionary --&gt; ctypes_unicode_proclaunch send_recv_json --&gt; ctypes_unicode_proclaunch ctypes_unicode_proclaunch --&gt; get_unicode_multiline_input_with_editor get_unicode_arguments_to_launch_editor --&gt; get_unicode_multiline_input_with_editor","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"座右铭","slug":"座右铭","date":"2025-08-31T04:00:00.000Z","updated":"2025-09-14T14:23:10.951Z","comments":true,"path":"2025/08/31/座右铭/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/31/%E5%BA%A7%E5%8F%B3%E9%93%AD/","excerpt":"","text":"Freedom &amp; Perception The best slave is the one who thinks he is free. Every accusation is a confession. Change &amp; Action Philosophers have only interpreted the world, in various ways; the point, however, is to change it. Vision &amp; Strategy 左牵黄，右擎苍，锦帽貂裘，千骑卷平冈。 不谋万世者，不足谋一时；不谋全局者，不足谋一域。 It is hallowed by no traditions; it is hampered by none. Its finger posts all point forward. 剧是必须从序幕开始的，但序幕还不是高潮。 一张白纸，没有负担，好写最新最美的文字，好画最新最美的画图。 ……结合各自具体实际开拓创新，特别是在前沿实践、未知领域，鼓励大胆探索、敢为人先，寻求有效解决新矛盾新问题的思路和办法，努力创造可复制、可推广的新鲜经验。 Practical Wisdom &amp; Problem Solving Smart data structures and dumb code works a lot better than the other way around. Premature optimization is the root of all evil. The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. Building in how we think we think does not work in the long run. The only thing that matters in the long run is the leveraging of computation. AI researchers have often tried to build knowledge into their agents, this always helps in the short term, and is personally satisfying to the researcher, but in the long run it plateaus and even inhibits further progress, and breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"Python Coding Guidelines","slug":"Python-Coding-Guidelines","date":"2025-08-29T04:00:00.000Z","updated":"2025-09-28T19:12:25.327Z","comments":true,"path":"2025/08/29/Python-Coding-Guidelines/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/29/Python-Coding-Guidelines/","excerpt":"","text":"Philosophy: We have exceptionally strict coding standards. But we explicitly use them in tandem with LLMs. No brittle linters. The LLM is the living compliance checker, code reviewer, and first-pass automator. We achieve uncompromising quality control, reliability, and portability - while moving faster than ever before. Guidelines 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214## General Principles- **One file = one module = one purpose**: Each file must be explicitly importable as a module and serve exactly one main purpose.- **Portability to C++**: Code must be directly portable to C++ (avoid dynamic, Python-specific idioms).## Compatibility- **Python 2 &amp; 3**: All code must run and be tested under both Python 2 and 3.- **POSIX &amp; NT**: All code must work on both UNIX-like and Windows systems.## File and Folder Structure- Import all files (modules) via absolute import. No relative import, no `sys.path` manipulation.- All files (modules) must only have public functions and classes - no private/internal APIs.- All directories must include an explicit `__init__.py` within them.### Utility Code: No &quot;utils.py&quot; - Publish Generalized Tools- No local &quot;utils.py&quot; files: Do not keep grab-bag or miscellaneous utility functions/classes in a project-private `utils.py` file.- **General-purpose tools must be published as standalone packages to PyPI**, each focusing on one well-defined function, class, or concept. - Move helpers that would otherwise go into &quot;utils.py&quot; into their own properly documented, versioned packages. - These utility packages should be: - Named after their actual purpose; - Well-tested and actively maintained; - Equipped with a README, proper docstrings, testable usage examples, semantic versioning, and a clear license.- All projects share utilities via explicit dependencies rather than duplicating or copying helpers. - When a utility is improved or a bug is fixed, updating the package ensures all dependent projects benefit automatically - &quot;write once, run everywhere.&quot; - This approach avoids hidden technical debt and promotes code quality, documentation, reuse, and maintainability across your entire codebase. - It also contributes to the wider Python ecosystem.## Testing &amp; Documentation- Tests must: - Run successfully on both Python 2 and 3, POSIX and NT. - Be suitable for inclusion in `README.md` under &quot;Usage&quot; or &quot;Quickstart&quot;. - Simultaneously serve as usage documentation (idiomatic examples). - Self-contained and runnable as a script or documentation block.## Syntax &amp; Language Features### Division/Print- Use `from __future__ import division, print_function` if using `/` or `print()`### String Types- **Always be explicit:** - `b&#x27;&#x27;` = byte string - `u&#x27;&#x27;` = Unicode string (as `typing.Text`) - Plain `str` only for ambiguous APIs (filesystem operations, `argparse`).### String Formatting- Only use the legacy `%` formatting syntax. No `.format()` or f-strings.### Typing- All APIs fully typed with type **comments only** (`# type: ...`) - No inline type annotations, no `AnnAssign`.- Use only typing features as in Python 3.5 / PEP 484.- Absolutely no dependent typing. The return type of a function must not vary with different parameter types and/or values.- No use of `@overload` permitted.### Classes- No `attrs`, `dataclasses`, or `namedtuple`.- All classes must have: - Declared `__slots__` - Explicit `object` base class - Use `six.with_metaclass(meta, *bases)` for metaclasses - Mutable: `__init__`; Immutable: `__new__### Enums- Only manual/explicit values; do not use `auto()` even with `enum34`.### Version Checks &amp; Imports- Use only `sys.version_info` for version-conditional code.- No `except ImportError` or `sys.version` checks.### Restricted Language Features- Never use: `async`, `await`, `yield from`, walrus (`:=`), structural pattern matching (`match/case`).## Input &amp; Argument Parsing### Input Handling- For interactive input, always `import readline`.- Use `open(...)` for binary files; `codecs.open(...)` for Unicode text files.### Regular Expressions- Regular expressions **only for simple parsing**.- Use **Unix-style/basic** regex features: - `.`: any single character - `[ ]`: character set/class - `[^ ]`: negated class - `^, $`: line start/end - `( )`: grouping/subexpression - `*`: zero or more- For **complicated input**: Use a context-free grammar parser (e.g. [Lark](https://github.com/lark-parser/lark)), hand-written parser, or an LLM.### Argument Parsing- Use `argparse`.- Use ambiguous `str` for all argument values (default API behavior).- All arguments should have a `help=...` string.#### Flag Arguments- **Presence only:** `action=&#x27;store_true&#x27;` -&gt; `bool`- **Counted:** `action=&#x27;count&#x27;` -&gt; `int`- **Single value:** `type=str`, `required=True/False`, explicit `default=...`- **Multiple occurrences:** `action=&#x27;append&#x27;` -&gt; `Optional[List[str]]`#### Positional Arguments- **Exactly one:** simple positional- **0/1 (optional):** `nargs=&#x27;?&#x27;`, must be last positional argument- **0 or more/1 or more:** `nargs=&#x27;*&#x27;`/`nargs=&#x27;+&#x27;`, with plural argument name and singular `metavar`, must be last positional argument## Python Packaging &amp; Distribution### File Layout- All files start with a copyright and license block: - Boilerplate: `# Copyright (c) 2025 Jifeng Wu\\n# Licensed under the &lt;license&gt; License. See LICENSE file in the project root for full license information.` - simple infrastructure: MIT/BSD - complex infra: Apache-2.0 - applications: AGPL-3.0### Required Files &amp; Metadata- `README.md` with standard boilerplate (see below)- `LICENSE`- `requirements.txt` (see example)- `pyproject.toml` (see template)- `setup.cfg` (see template)### `README.md` Boilerplate```&lt;Project Description&gt;## Installation...## Usage...## ContributingContributions are welcome! Please submit pull requests or open issues on the GitHub repository.## LicenseThis project is licensed under the [&lt;license&gt; License](LICENSE).```### Example `requirements.txt````enum34; python_version &lt; &#x27;3.4&#x27;pyreadlinesixtyping; python_version &lt; &#x27;3.5&#x27;```### `pyproject.toml` Template```[build-system]requires = [&quot;setuptools&quot;]build-backend = &quot;setuptools.build_meta&quot;[project]name = &quot;&lt;project-name&gt;&quot;version = &quot;&lt;version&gt;&quot;description = &quot;&lt;Project Description&gt;&quot;readme = &quot;README.md&quot;requires-python = &quot;&gt;=2&quot;license = &quot;&lt;license&gt;&quot;authors = [ &#123; name=&quot;Jifeng Wu&quot;, email=&quot;jifengwu2k@gmail.com&quot; &#125;]classifiers = [ &quot;Programming Language :: Python :: 2&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Operating System :: OS Independent&quot;,]dependencies = [ &quot;&lt;requirements.txt line 1&gt;&quot;, &quot;&lt;requirements.txt line 2&gt;&quot;, &quot;&lt;requirements.txt line 3&gt;&quot;][project.urls]&quot;Homepage&quot; = &quot;https://github.com/jifengwu2k/&lt;project-name&gt;&quot;&quot;Bug Tracker&quot; = &quot;https://github.com/jifengwu2k/&lt;project-name&gt;/issues&quot;```Replace `&lt;project-name&gt;`, `&lt;version&gt;`, `&lt;license&gt;`, and requirements as appropriate.### `setup.cfg` Template```[bdist_wheel]universal = 1``` Checking Ensure you meet the following prerequisites: Your Python project is a Git repository. You have pbpaste properly set up. Execute the following to generate an LLM prompt: 123456789101112131415LLM_PROMPT_FILE=&#x27;llm_prompt.txt&#x27;echo &quot;This is my Python project:&quot; &gt; &quot;$&#123;LLM_PROMPT_FILE&#125;&quot;echo &gt;&gt; &quot;$&#123;LLM_PROMPT_FILE&#125;&quot;git ls-files --others --exclude-standard --cached | grep -v &#x27;.gitignore&#x27; | grep -v &quot;$&#123;LLM_PROMPT_FILE&#125;&quot; | while read filedo echo &quot;\\`$&#123;file&#125;\\`:&quot; echo cat &quot;$&#123;file&#125;&quot; echodone &gt;&gt; &quot;$&#123;LLM_PROMPT_FILE&#125;&quot;echo &quot;Please do a code review and assess whether the code complies with these guidelines:&quot; &gt;&gt; &quot;$&#123;LLM_PROMPT_FILE&#125;&quot;echo &gt;&gt; &quot;$&#123;LLM_PROMPT_FILE&#125;&quot; Copy some or all of the above guidelines. 1pbpaste &gt;&gt; &quot;$&#123;LLM_PROMPT_FILE&#125;&quot; Feed that LLM prompt into your LLM of choice. Then rm $&#123;LLM_PROMPT_FILE&#125;.","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"简易运动","slug":"简易运动","date":"2025-08-29T04:00:00.000Z","updated":"2025-09-05T17:28:03.633Z","comments":true,"path":"2025/08/29/简易运动/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/29/%E7%AE%80%E6%98%93%E8%BF%90%E5%8A%A8/","excerpt":"","text":"全身运动 八段锦 八部金刚功 面部运动 瘦鼻训练 改善双下巴 双下巴不是胖！每天3分钟下颌线越练越清晰！（播音训练） 臀腿运动 这种腿型的都去练‼️这个动作很炸裂🔥 凯格尔运动（仅供教育用途，请手动替换链接） 基础凯格尔运动 /view_video.php?viewkey=ph62f3e9b2bc880 /view_video.php?viewkey=ph611acfca7e5be /view_video.php?viewkey=ph61ea4e4b90ef0 凯格尔复合运动 /view_video.php?viewkey=ph6197d0beeff54&amp;pkey=10810581","categories":[{"name":"Health","slug":"Health","permalink":"https://jifengwu2k.github.io/categories/Health/"}],"tags":[]},{"title":"Manipulating PDFs from the Unix Command Line","slug":"Manipulating-PDFs-from-the-Unix-Command-Line","date":"2025-08-27T16:48:43.000Z","updated":"2025-08-27T16:54:50.024Z","comments":true,"path":"2025/08/27/Manipulating-PDFs-from-the-Unix-Command-Line/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/27/Manipulating-PDFs-from-the-Unix-Command-Line/","excerpt":"","text":"The Poppler utilities provide many Unix command-line tools for manipulating PDFs. Install via: sudo apt-get install poppler-utils (Ubuntu) sudo apk add poppler-utils (Alpine) brew install poppler (macOS) Extract Text from a PDF 1pdftotext input.pdf output.txt Merge PDFs Together 1pdfunite file1.pdf file2.pdf merged.pdf Split (Extract) Pages from a PDF This will split each page of input.pdf to output_1.pdf, output_2.pdf, etc. 1pdfseparate input.pdf output_%d.pdf","categories":[{"name":"Multimedia","slug":"Multimedia","permalink":"https://jifengwu2k.github.io/categories/Multimedia/"}],"tags":[]},{"title":"Philosophical Idioms","slug":"Philosophical-Idioms","date":"2025-08-27T04:00:00.000Z","updated":"2025-08-27T16:45:12.837Z","comments":true,"path":"2025/08/27/Philosophical-Idioms/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/27/Philosophical-Idioms/","excerpt":"","text":"Cutting Through the Nonsense “艾玛，这不就是咱们屯子里的那个……吗？” \"Ain't that just that ... from our neck o' the woods?\" \"... is by definition ...\" Literal: “……从定义上讲/从本质上说是…… Idiomatic: “……说白了就是……” \"That's a feature, not a bug.\" Literal: “那是功能，不是缺陷。” Idiomatic: “这是设计好的，不是整劈叉了。” Your Situation Shapes Your Views “屁股决定脑袋。” Literal: \"Your ass determines your head.\" Idiomatic: \"Where you stand depends on where you sit.\" (A classic U.S. political saying with the same idea.) Pragmatism and Results-Driven Thinking “实践是检验真理的唯一标准。” Literal: \"Practice is the sole criterion for testing truth.\" Idiomatic: \"Actions are the ultimate test of truth.\" “不管黑猫白猫，能捉到老鼠就是好猫。” Literal: \"No matter whether it's a black cat or a white cat, as long as it catches mice, it's a good cat.\" Idiomatic: \"If it gets the job done, it's good enough.\" Knowing Who You Are &amp; Doing Your Own Thing “认清自己几斤几两。” “撒泡尿照照自己。” Literal: \"Recognize how many pounds and ounces you weigh.\" / \"Take a piss and look at yourself in it.\" Idiomatic: \"Who do you think you are?\" “你们没有资格说，你们从实力地位出发同……谈话。” \"You do not have the qualification to say that you want to speak to ... from a position of strength.\" “我蛮夷也，不与中国之号谥。” Literal: \"I'm a barbarian; I do not accept the designations of the Central Kingdom.\" Idiomatic: \"I'm a barbarian. I don't play by your rules.\" “入关之后自有大儒为我辩经。” Literal: \"After entering the pass, there will naturally be great scholars to argue scriptures on my behalf.\" Idiomatic: \"Let the philosophers debate after I've made my move.\"","categories":[{"name":"Linguistics","slug":"Linguistics","permalink":"https://jifengwu2k.github.io/categories/Linguistics/"}],"tags":[]},{"title":"Compiling C++ Code in a Conda Environment Using Conda-managed Headers and Libraries","slug":"Compiling-C-Code-in-a-Conda-Environment-Using-Conda-managed-Headers-and-Libraries","date":"2025-08-24T04:00:00.000Z","updated":"2025-08-25T03:54:17.623Z","comments":true,"path":"2025/08/24/Compiling-C-Code-in-a-Conda-Environment-Using-Conda-managed-Headers-and-Libraries/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/24/Compiling-C-Code-in-a-Conda-Environment-Using-Conda-managed-Headers-and-Libraries/","excerpt":"","text":"Prerequisites Common Build Variables Directly Used by gcc/clang These environment variables are automatically used by gcc/clang themselves. CPATH Colon-separated list of directories to search for headers before built-in include paths. Like passing multiple -I options. LIBRARY_PATH Colon-separated list of directories to search for libraries (.so/.a) at link time. Like passing multiple -L options. Doesn't affect how the executable finds shared libraries when running. For Build Systems (make, cmake, etc.) These environment variables are not used by compilers unless your build tool (make, cmake, etc.) or script expands them. CC / CXX: Which C/C++ compiler to use. CFLAGS / CXXFLAGS: Extra flags for compiling C or C++ respectively, e.g., -O2 -g -Wall. LDFLAGS: Extra flags when linking, such as -L/path/to/lib, -lfoo, -Wl,-rpath,/my/libs. 🚫 Never stuff include/library/link flags inside $CXX or $CC. Always set them as separate variables, or pass them directly on the command line to the compiler. C++ Libraries in Conda Environments When you install a C++ library from conda-forge, it's placed inside your current environment, not in a system-wide directory. Headers: $CONDA_PREFIX/include Libraries: $CONDA_PREFIX/lib Exception: The C runtime (libc.so, libm.so, etc.) always comes from the system, not Conda. Example Create a Conda Environment for C++ Compilation 123conda create -n clangxx-env -yconda activate clangxx-envconda install -c conda-forge clang clangxx libcxx libcxxabi libcxx-devel -y libcxx, libcxxabi, libcxx-devel: LLVM's C++ standard library and headers. Tell the Compiler Where to Look for Headers and Libraries 12export CPATH=&quot;$CONDA_PREFIX/include&quot;export LIBRARY_PATH=&quot;$CONDA_PREFIX/lib&quot; This automatically adds Conda's include and lib directories to clang's search paths. Set Common Build Variables for Build Systems 123export CC=&quot;clang&quot;export CXX=&quot;clang++ -stdlib=libc++&quot;export LDFLAGS=&quot;-Wl,-rpath,$LIBRARY_PATH&quot; LDFLAGS sets an rpath such that the generated executable or shared library will hard-code LIBRARY_PATH as a shared library search path. Compile a Program Let's say you have hello_world.cpp. 1$CXX hello_world.cpp -o hello_world $LDFLAGS Check Your Binary You can check what dynamic libraries your binary depends on using ldd hello_world: 1234567$ ldd hello_world linux-vdso.so.1 (0x00007ffd05ead000) libc++.so.1 =&gt; /home/youruser/miniconda3/envs/clangxx-env/lib/libc++.so.1 (0x000071f5b0e86000) libc++abi.so.1 =&gt; /home/youruser/miniconda3/envs/clangxx-env/lib/libc++abi.so.1 (0x000071f5b0e48000) libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x000071f5b0d44000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x000071f5b0a00000) ... Good sign: For anything above libc, it should point inside your Conda environment (e.g., libc++.so.1), not the system.","categories":[{"name":"Development Environments","slug":"Development-Environments","permalink":"https://jifengwu2k.github.io/categories/Development-Environments/"}],"tags":[]},{"title":"Manipulating Videos Using `ffmpeg`","slug":"Manipulating-Videos-Using-ffmpeg","date":"2025-08-23T04:00:00.000Z","updated":"2025-09-13T00:23:23.779Z","comments":true,"path":"2025/08/23/Manipulating-Videos-Using-ffmpeg/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/23/Manipulating-Videos-Using-ffmpeg/","excerpt":"","text":"Extract a Portion of the Video 1ffmpeg -ss &quot;$start_time_in_seconds&quot; -i &quot;$input&quot; -to &quot;$end_time_in_seconds&quot; -c copy &quot;$output&quot; Reduce a Video's Resolution by Half 1ffmpeg -i &quot;$input&quot; -vf &quot;scale=iw/2:ih/2&quot; -c:a copy &quot;$output&quot; Use the scale filter with expressions to halve the width and height: iw = input width, ih = input height. -c:a copy copies the audio stream without re-encoding.","categories":[{"name":"Multimedia","slug":"Multimedia","permalink":"https://jifengwu2k.github.io/categories/Multimedia/"}],"tags":[]},{"title":"简易食谱","slug":"简易食谱","date":"2025-08-22T04:00:00.000Z","updated":"2025-09-03T18:00:43.698Z","comments":true,"path":"2025/08/22/简易食谱/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/22/%E7%AE%80%E6%98%93%E9%A3%9F%E8%B0%B1/","excerpt":"","text":"切忌用电饭煲在没有饭的情况下焖菜或炖菜，电饭煲盖会有大量难以清洗的残留物。 禁止像用水炖菜一样用牛奶炖菜，会导致牛奶中的蛋白凝结，显得比较恶心。 轻食 玉米鸡蛋火龙果 微波炉土豆鸡蛋西兰花圣女果虾仁沙拉 饮料 微波炉焦糖奶茶 🇨🇺 Cuba libre (kuβa ˈliβɾe) 120 ml cola 50 ml white rum 10 ml Fresh lime juice 🇨🇺 Mojito (moˈxito) 45 ml white rum 20 ml fresh lime juice 6 sprigs of mint 2 teaspoons white cane sugar (or 20 ml of sugar syrup) Soda water 🇵🇷 Piña colada (ˈpiɲa koˈlaða) 50 ml white rum 30 ml coconut cream 50 ml fresh pineapple juice 主食 杂粮 微波炉土豆泥 微波炉南瓜、红薯、玉米、土豆、山药 微波炉蒸蜜薯 微波炉酸辣土豆丝 粥品 皮蛋瘦肉粥 面点 绞肉机和面蒸馒头 电饭煲恰巴塔 电饭煲馅饼 肉馅饼 素菜 绿叶蔬菜 微波炉蒜蓉空心菜 微波炉火腿肠娃娃菜 微波炉手撕包菜 其他蔬菜 微波炉花菜 微波炉青椒擂皮蛋 微波炉香菇烧青椒 混合蔬菜 微波炉茄辣西（1） 微波炉茄辣西（2） 电饭煲地三鲜（1） 电饭煲地三鲜（2） 电饭煲炖土豆、胡萝卜、洋葱、玉米、香菇、青红椒 微波炉懒人地三鲜 荤菜 肉类 微波炉盐焗鸡 微波炉黄焖鸡 电饭煲黄焖鸡 电饭煲水蒸蛋、丝瓜蒸鸡腿 电饭煲蜜汁叉烧 微波炉鸡蛋肉饼汤 电饭煲土豆番茄焖牛肉 电饭煲番茄炖牛腩 菠萝牛肉粒 电饭煲蚂蚁上树 电饭煲西红柿炖牛腩 微波炉红烧鸡腿 微波炉鸡块 微波炉洋葱牛肉 海鲜类 微波炉清蒸鱼 微波炉白灼虾 电饭煲肉蟹煲 微波炉鱼片、金针菇 饭菜一锅出 米饭类 电饭煲一颗番茄焖饭 电饭煲土豆胡萝卜洋葱玉米香菇青椒鸡腿焖饭 电饭煲火锅焖饭 电饭煲豆角土豆焖饭 电饭煲豆角焖饭 电饭煲煲仔饭 电饭煲土豆、茄子、番茄、豆角焖饭 番茄鸡蛋火腿饭 番茄、鸡蛋、火腿、熟米 面条类 微波炉豆角五花肉焖面 电饭煲豆角焖面 其他面食类 电饭煲洋葱、蘑菇、香肠、番茄、虾仁、芝士意大利面 电饭煲西红柿土豆泥火鸡面 食材准备 微波炉豆豉、蒜末、小米辣酱 微波炉葱油 微波炉熬猪油","categories":[{"name":"Food and Drink","slug":"Food-and-Drink","permalink":"https://jifengwu2k.github.io/categories/Food-and-Drink/"}],"tags":[]},{"title":"High-Frequency Git Operations for Everyday Development","slug":"High-Frequency-Git-Operations-for-Everyday-Development","date":"2025-08-22T00:43:36.000Z","updated":"2025-08-22T00:46:36.945Z","comments":true,"path":"2025/08/21/High-Frequency-Git-Operations-for-Everyday-Development/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/21/High-Frequency-Git-Operations-for-Everyday-Development/","excerpt":"","text":"Whether you're joining a new project, dealing with ignored files, or wanting to clean up your commit history, you'll often need to execute a set of Git commands. Here are some of the most frequent Git operations you may need in day-to-day development. Connect Your Local Folder to a GitHub Repository Scenario You have an existing local folder with files and want to connect it to an already existing GitHub repository. Steps Initialize Git in your folder: 1git init Add your GitHub repository (replace URL with your repo): 1git remote add origin https://github.com/username/repo.git Fetch remote history: 1git fetch origin Merge remote and local files, allowing unrelated histories (change main to your repo's default branch if necessary): 1git pull origin main --allow-unrelated-histories If you see an error like: The following untracked working tree files would be overwritten by merge... We recommend: 123456789# 1. Move local conflicting files to a temporary directory# For example, back them up to a non-conflicting `backup` folder in the current directorymkdir backupmv conflicting-file-1 conflicting-file-2 backup/# 2. Try pulling the remote repository content againgit pull origin main --allow-unrelated-histories# 3. Manually merge the backed up files into the pulled files Commit after resolving all conflicts: 12git add .git commit -m &quot;Merge local files with remote repository&quot; Push to the remote repository (if needed): 1git push --set-upstream origin main The --set-upstream origin main option is only needed for the first push; you don't need it afterward. List What Files git add . Would Stage 1git status --short Example Output: 12M file1.txt # modified file?? newfile.txt # untracked file Breakdown: M = Modified (but not staged) ?? = Untracked (new files) Remove Previously Committed Files Now in .gitignore Scenario You added rules to .gitignore but some files were already committed. Steps Stage removal of all currently tracked files (but not delete locally): 1git rm -r --cached . Add everything back to the repo (\"add\" skips gitignored files): 1git add . Commit the change: 1git commit -m &quot;Remove ignored files from the repository&quot; At this point, the files are only removed from the repository from this commit onward (they'll still exist in older commits/history). If you want them removed from previous commits as well, consider squashing multiple commits into a single commit, as explained below. Squash Multiple Commits into a Single Commit Scenario You have made several small commits (some of which may be faulty or embarassing) and want to clean up history by squashing them into one. Steps Review commit history: 1git log Decide how many previous commits you want to squash. Start an interactive rebase, e.g. with the last 3 commits: 1git rebase -i HEAD~3 In the opened editor: Leave pick for the first commit. For the others, change pick to squash (or just s). Save and close. You will then enter a second editor session. Edit the combined commit message. Save and close. Force-push the branch to rewrite history on GitHub: 1git push --force Note Use squashing carefully if collaborating, as force push overwrites history.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://jifengwu2k.github.io/categories/DevOps/"}],"tags":[]},{"title":"Eating in the Americas - Down-to-earth Foodstuffs","slug":"Eating-in-the-Americas-Down-to-earth-Foodstuffs","date":"2025-08-21T04:00:00.000Z","updated":"2025-09-19T02:22:11.296Z","comments":true,"path":"2025/08/21/Eating-in-the-Americas-Down-to-earth-Foodstuffs/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/21/Eating-in-the-Americas-Down-to-earth-Foodstuffs/","excerpt":"","text":"吃在美洲——接地气儿的食材 Eating in the Americas - Down-to-earth Foodstuffs 这些植物和动物在美洲有原生族群： These plants and animals have populations native to the Americas: 植物 Plants 天门冬目 Asparagales 天门冬科 Asparagaceae 龙舌兰属 Agave 兰科 Orchidaceae 香荚兰属 Vanilla (香草) 菊目 Asterales 菊科 Asteraceae 向日葵属 Helianthus 向日葵 H. annuus (Sunflower) 松果菊属 Echinacea (紫锥花 / 紫锥菊; Echinacea) 十字花目 Brassicales 番木瓜科 Caricaceae 番木瓜属 Carica 番木瓜 C. papaya (Papaya) 石竹目 Caryophyllales 苋科 Amaranthaceae (苋菜 / 藜麦; Amaranth / Quinoa) 仙人掌科 Cactaceae (仙人掌 / 火龙果 / 梨果仙人掌; Cactus / Pitaya / Strawberry Pear / Dragon Fruit / Prickly Pear) 葫芦目 Cucurbitales 葫芦科 Cucurbitaceae 南瓜属 Cucurbita (南瓜 / 西葫芦; Squash / Pumpkin / Zucchini) 葫芦属 Lagenaria 葫芦 L. siceraria (Gourd) 杜鹃花目 Ericales 柿树科 Ebenaceae 柿属 Diospyros (柿子 / 君迁子; Persimmon / Date-plum) 杜鹃花科 Ericaceae 越橘屬 Vaccinium (蓝莓 / 笃斯越橘 / 蔓越莓; Blueberry / Cranberry) 豆目 Fabales 豆科 Fabaceae 落花生属 Arachis 花生 A. hypogaea (Peanut) 豆薯属 Pachyrhizus 凉薯 P. erosus (Jícama) 菜豆属 Phaseolus 菜豆 P. vulgaris (芸豆 / 架豆 / 刀豆 / 肾豆 / 四季豆 / 豆角; Bean) 壳斗目 Fagales 桦木科 Betulaceae 榛属 Corylus (Hazelnut) 壳斗科 Fagaceae 栗属 Castanea (Chestnut) 胡桃目 Juglandales 胡桃科 Juglandaceae 山核桃属 Carya 长山核桃 C. illinoinensis (碧根果 / 胡桃; Pecan) 胡桃属 Juglans (核桃; Walnut) 唇形目 Lamiales 唇形科 Lamiaceae 鼠尾草属 Salvia (Chia / Sage; 奇亚籽) 樟目 Laurales 樟科 Lauraceae 鳄梨属 Persea 鳄梨 P. americana (牛油果; Avocado) 金虎尾目 Malpighiales 大戟科 Euphorbiaceae 木薯属 Manihot 木薯 M. esculenta (Cassava / Tapioca) 西番莲科 Passifloraceae 西番莲属 Passiflora 西番莲 P. edulis (百香果 / 鸡蛋果; Passion Fruit) 锦葵目 Malvales 锦葵科 Malvaceae 棉花属 Gossypium (Cotton) 可可属 Theobroma 可可 T. cacao (Cacao / Cocoa) 桃金娘目 Myrtales 桃金娘科 Myrtaceae 番石榴属 Psidium 番石榴 P. guajava (Guava) 禾本目 Poales 竹亚科 Bambusoideae (Bamboo / Giant Cane / River Cane) 凤梨科 Bromeliaceae 凤梨属 Ananas 凤梨 A. comosus (菠萝; Ananas / Pineapple) 禾本科 Poaceae 稻属 Oryza (大米; Rice) 稷属 Panicum (黍 / 粟 / 糜子 / 黍稷 / 糜黍 / 稷 / 黄米 / 稷米 / 粢米; Millet / Panicgrass) 玉米属 Zea 玉米/玉蜀黍 Z. mays (Maize / Corn) 菰属 Zizania 菰 Z. latifolia (菰米 / 茭白; Wild Rice) 山龙眼目 Proteales 莲科 Nelumbonaceae 莲属 Nelumbo (Lotus) 蔷薇目 Rosales 桑科 Moraceae 桑属 Morus (Mulberry) 蔷薇科 Rosaceae 山楂属 Crataegus (Hawthorn / Mayhaw / May-tree) 草莓属 Fragaria (Strawberry) 李属 Prunus (Plum / Plune) 悬钩子属 Rubus (树莓 / 覆盆子 / 黑莓; Raspberry / Blackberry) 无患子目 Sapindales 漆树科 Anacardiaceae 腰果属 Anacardium (Cashew) 芸香科 Rutaceae 花椒属 Zanthoxylum (Prickly Ash) 无患子科 Sapindaceae 枫属 Acer (枫树 / 槭树; Maple) 茄目 Solanales 旋花科 Convolvulaceae 番薯属 Ipomoea 番薯 I. batatas (红薯 / 紫薯 / 白薯; Sweet Potato) 烟草属 Nicotiana 烟草 N. tabacum (Tobacco) 茄科 Solanaceae 茄属 Solanum 马铃薯 S. tuberosum (Potato) 番茄 S. lycopersicum (Tomato) 辣椒属 Capsicum (甜椒 / 辣椒; Bell Pepper / Chili Pepper) 灯笼果属 Physalis 灯笼果 P. peruviana (姑娘果; Cape gooseberry / Goldenberry / Groundcherry) 葡萄目 Vitales 葡萄科 Vitaceae 葡萄属 Vitis (Grape) 动物 Animals 日鲈目 Centrarchiformes 太阳鱼科 Centrarchidae (中国市面上常见的“鲈鱼”实为该科的 Micropterus 黑鲈属; Bass) 鲈形目 Perciformes 鲈科 Percidae 鲈属 Perca (Perch) 鲑形目 Salmoniformes 鲑科 Salmonidae 麻哈鱼属 Oncorhynchus (大麻哈鱼 / 虹鳟; Chinook Salmon / Chum Salmon / Coho Salmon / Dog Salmon / Pacific Salmon / Pink Salmon / Sockeye Salmon / Rainbow Trout) 鲶形目 Siluriformes (鲶鱼 / 塘虱 / 巴沙鱼; Catfish / Basa / Pangasius) 雁形目 Anseriformes 鸭科 Anatidae (鸭 / 鸳鸯 / 凫 / 雁 / 鹅 / 天鹅; Duck / Goose / Swan) 鸡形目 Galliformes 雉科 Phasianidae 火鸡属 Meleagris (Turkey)","categories":[{"name":"Food and Drink","slug":"Food-and-Drink","permalink":"https://jifengwu2k.github.io/categories/Food-and-Drink/"}],"tags":[]},{"title":"Setting Up a Headless VNC Server for Remote Desktop Access","slug":"Setting-Up-a-Headless-VNC-Server-for-Remote-Desktop-Access","date":"2025-08-21T04:00:00.000Z","updated":"2025-08-21T21:29:13.070Z","comments":true,"path":"2025/08/21/Setting-Up-a-Headless-VNC-Server-for-Remote-Desktop-Access/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/21/Setting-Up-a-Headless-VNC-Server-for-Remote-Desktop-Access/","excerpt":"","text":"While SSH is a staple tool and almost universally understood among Linux users, setting up VNC for remote desktop access - especially headlessly or with virtual framebuffers - remains mysterious to many, and for good reasons: Requires understanding of X11 and graphical sessions (DISPLAY variables, X servers, etc.) Needs both a VNC server and a desktop environment/window manager Involves properly launching graphical sessions However, for many professionals, researchers, and IT administrators, secure remote desktop access is not just a convenience - it's an inelastic requirement. Unlike other features you can work around or delay, robust GUI access to a remote host is sometimes the only way to get mission-critical work done. For example: Work-from-home mean you need to access graphical applications (like IDEs, simulators, and visualization tools) installed on remote lab or office machines. Remote servers without physical monitors demand a headless-only setup - you can't just \"plug in a monitor and keyboard\" to start a desktop session. Modern GUI applications sometimes can't run effectively via X11 forwarding (like over slow networks) and absolutely require a persistent desktop session. In these scenarios, there is no substitute - the need is inelastic. You must have a reliable, secure way to run and manage desktops remotely. Frustratingly, most available documentation is scattered, outdated, or omits crucial details. That's why this guide exists: to provide you with a direct, practical roadmap for setting up a robust remote desktop workflow, no matter where you work. The Three Components of Headless VNC Three key components work together to present a remote desktop via VNC: Xvfb (X Virtual Framebuffer) Xvfb is a \"headless\" X server that implements the X11 protocol and acts like a display server, but renders everything to memory instead of a physical monitor. x11vnc This utility acts as a bridge, allowing VNC clients to view and interact with an existing X server session. It: Connects to the X server's framebuffer (in this case, Xvfb's in-memory screen) Reads pixels from the X server, and writes mouse/keyboard events Presents those images to a VNC client over the network, and listens for events GUI Applications These render to Xvfb's in-memory screen instead of a physical monitor via the DISPLAY environment variable. Step-by-Step Setup Guide Prerequisites (Remote Host) First, install the necessary packages on the remote host: xvfb x11vnc xterm, icewm (for demonstration purposes in this tutorial) We would have to run xvfb, x11vnc, and xterm concurrently. For this purpose, I recommend using tmux (or any other comparable tool) to manage multiple terminal windows and persistent sessions. Start the Virtual Display (Remote Host) In your first terminal window on the remote host, start Xvfb: 1Xvfb :1 -screen 0 800x600x16 This creates a virtual X11 display number :1 (using TCP port 6001) with resolution 800x600 and 16-bit color depth. Xvfb has properly started only if the above command neither logs output nor exits after being invoked. If it logs messages or exits immediately, there's a problem to solve before moving on. Common issues include: Port 6001 already in use (try a different display number like :2) Permission issues with /tmp/.X11-unix directory. The /tmp/.X11-unix directory must be readable and writable. In some environments like WSL, this directory might be read-only, which will prevent Xvfb from starting properly. Launch the VNC Server (Remote Host) In a second terminal window on the remote host, start x11vnc: 1x11vnc -display :1 -rfbport 5901 -localhost -forever This command: Connects to the virtual X server's framebuffer at display :1 Listens for VNC connections on port 5901 Restricts connections to localhost only (for security, VNC is not encrypted) Never expose VNC directly to the public Internet! With -forever, x11vnc won't exit after the last client disconnects After running the command: It may report authentication or password-related errors (which is acceptable). But it should NOT exit after running for a while. If it exits after running for a while, there's a problem to solve before moving on. Secure Access with SSH Tunneling (Local Machine) VNC is not encrypted, so we'll use SSH tunneling on your local machine for security. You can either use basic SSH port forwarding (which may be confusing), or set up the pull_remote_port.sh wrapper on your local machine to pull the remote host's port 5901 to localhost:5901. After cloning its GitHub repository and completing its prerequisites, run: 1bash pull_remote_port.sh [-p &lt;ssh_port&gt;] -u &lt;remote_user&gt; -h &lt;remote_host&gt; -r 5901 -l 5901 This makes the remote host's VNC port available on your local machine via a secure SSH tunnel. Test with a Simple Application (Remote Host) In a third terminal window on the remote host, launch a test application: 1234# The `DISPLAY=:1` environment variable must be set# For all GUI apps you wish to appear in the VNC sessionexport DISPLAY=:1xterm Now connect to your VNC server on your local machine using a VNC client (like TigerVNC, RealVNC, or Remmina) pointing to localhost:5901. You should see the xterm window. Using Desktop Environments (Remote Host) For a full desktop experience rather than just single applications, you can install a desktop environment. Lightweight Options Openbox Fluxbox IceWM WindowMaker Full Desktop Environments GNOME KDE Plasma MATE Cinnamon LXDE LXQt Example: Installing and Using IceWM (Remote Host) In the third terminal window on the remote host (the one used to launch xterm), quit xterm and start an IceWM session: 12export DISPLAY=:1icewm-session Now your VNC client on your local machine will show the complete IceWM desktop environment. IceWM desktop environment in VNC client Important Caveat: Single-Instance Applications Many desktop applications are designed to run only one instance per user account: Browsers: Firefox, Chrome/Chromium Advanced editors: gedit, kate, geany (configurable), code/VSCode File managers: nautilus, pcmanfm (configurable), dolphin Document viewers: evince Many modern GTK/Qt apps that use D-Bus for activation If your remote host has a physical desktop, and such an app is already running on your physical desktop, new instances will try to communicate with the existing instance via D-Bus, creating windows on your physical desktop instead of in your virtual framebuffer, even after export DISPLAY=:1. Solution: Ensure applications aren't already running on your main desktop before launching them in your VNC session. Or use a different user account for VNC access. Conclusion Setting up a headless VNC server might seem complex at first, but by understanding the three components (Xvfb, x11vnc, and your GUI applications) and following this recipe, you can create a reliable remote desktop solution. Remember to always use SSH tunneling for security and be mindful of single-instance applications. With this setup, you can enjoy full graphical desktop access to your remote Linux machines, opening up possibilities for remote administration, development, and troubleshooting.","categories":[{"name":"System Administration","slug":"System-Administration","permalink":"https://jifengwu2k.github.io/categories/System-Administration/"}],"tags":[]},{"title":"How to Make VS Code's Language Detection Sane (and Deterministic)","slug":"How-to-Make-VS-Code-s-Language-Detection-Sane-and-Deterministic","date":"2025-08-12T04:00:00.000Z","updated":"2025-08-25T03:48:18.165Z","comments":true,"path":"2025/08/12/How-to-Make-VS-Code-s-Language-Detection-Sane-and-Deterministic/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/12/How-to-Make-VS-Code-s-Language-Detection-Sane-and-Deterministic/","excerpt":"","text":"Anyone who's used Visual Studio Code long enough has probably run into some surprisingly silly (and maddening) mistakes from its automatic language detection. Things get especially annoying with temporary or scratch files: VS Code tries too hard to be clever and ends up insisting your random notes are \"Groovy\" or \"Shell\" scripts. If you're like me, you just want files with clear, non-ambiguous extensions mapped correctly, and everything else opened as plain text. Simpler, saner, less frustrating. The Fix: Use Explicit File Associations By explicitly listing out the extension - language bindings in your settings.json, you can make VS Code behave in a much more predictable way. For every common, unambiguous extension (.js, .py, .cpp, etc.), set the language association directly. For ambiguous or tricky cases (like .m for both Objective-C and MATLAB), don't specify anything - you can always override them manually. For all other files (including all extensionless files and temporary files), force them to open as plain text. This means no more unwanted language features popping up, and every common language just works. How to Set It Up Open the command palette (Ctrl+Shift+P / Cmd+Shift+P). Type and select: Preferences: Open Settings (JSON) Replace any existing \"files.associations\" block in your global settings.json: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495&#123; &quot;files.associations&quot;: &#123; &quot;Dockerfile&quot;: &quot;dockerfile&quot;, &quot;Makefile&quot;: &quot;makefile&quot;, &quot;*.abap&quot;: &quot;abap&quot;, &quot;*.bat&quot;: &quot;bat&quot;, &quot;*.bib&quot;: &quot;bibtex&quot;, &quot;*.c&quot;: &quot;c&quot;, &quot;*.cc&quot;: &quot;cpp&quot;, &quot;*.clj&quot;: &quot;clojure&quot;, &quot;*.cljc&quot;: &quot;clojure&quot;, &quot;*.cljs&quot;: &quot;clojure&quot;, &quot;*.cmd&quot;: &quot;bat&quot;, &quot;*.coffee&quot;: &quot;coffeescript&quot;, &quot;*.cpp&quot;: &quot;cpp&quot;, &quot;*.cs&quot;: &quot;csharp&quot;, &quot;*.cshtml&quot;: &quot;razor&quot;, &quot;*.css&quot;: &quot;css&quot;, &quot;*.cu&quot;: &quot;cuda-cpp&quot;, &quot;*.cuh&quot;: &quot;cuda-cpp&quot;, &quot;*.cxx&quot;: &quot;cpp&quot;, &quot;*.d&quot;: &quot;d&quot;, &quot;*.dart&quot;: &quot;dart&quot;, &quot;*.diff&quot;: &quot;diff&quot;, &quot;*.erl&quot;: &quot;erlang&quot;, &quot;*.fs&quot;: &quot;fsharp&quot;, &quot;*.fsi&quot;: &quot;fsharp&quot;, &quot;*.fsx&quot;: &quot;fsharp&quot;, &quot;*.go&quot;: &quot;go&quot;, &quot;*.groovy&quot;: &quot;groovy&quot;, &quot;*.haml&quot;: &quot;haml&quot;, &quot;*.handlebars&quot;: &quot;handlebars&quot;, &quot;*.hbs&quot;: &quot;handlebars&quot;, &quot;*.hpp&quot;: &quot;cpp&quot;, &quot;*.hrl&quot;: &quot;erlang&quot;, &quot;*.hs&quot;: &quot;haskell&quot;, &quot;*.htm&quot;: &quot;html&quot;, &quot;*.html&quot;: &quot;html&quot;, &quot;*.ini&quot;: &quot;ini&quot;, &quot;*.jade&quot;: &quot;jade&quot;, &quot;*.java&quot;: &quot;java&quot;, &quot;*.jl&quot;: &quot;julia&quot;, &quot;*.js&quot;: &quot;javascript&quot;, &quot;*.json&quot;: &quot;json&quot;, &quot;*.jsonc&quot;: &quot;jsonc&quot;, &quot;*.jsx&quot;: &quot;javascriptreact&quot;, &quot;*.less&quot;: &quot;less&quot;, &quot;*.lua&quot;: &quot;lua&quot;, &quot;*.markdown&quot;: &quot;markdown&quot;, &quot;*.md&quot;: &quot;markdown&quot;, &quot;*.ml&quot;: &quot;ocaml&quot;, &quot;*.mli&quot;: &quot;ocaml&quot;, &quot;*.mm&quot;: &quot;objective-cpp&quot;, &quot;*.p6&quot;: &quot;raku&quot;, &quot;*.pas&quot;: &quot;pascal&quot;, &quot;*.patch&quot;: &quot;diff&quot;, &quot;*.php&quot;: &quot;php&quot;, &quot;*.php4&quot;: &quot;php&quot;, &quot;*.php5&quot;: &quot;php&quot;, &quot;*.phtml&quot;: &quot;php&quot;, &quot;*.pl&quot;: &quot;perl&quot;, &quot;*.pl6&quot;: &quot;raku&quot;, &quot;*.pm&quot;: &quot;perl&quot;, &quot;*.ps1&quot;: &quot;powershell&quot;, &quot;*.psm1&quot;: &quot;powershell&quot;, &quot;*.pug&quot;: &quot;pug&quot;, &quot;*.py&quot;: &quot;python&quot;, &quot;*.r&quot;: &quot;r&quot;, &quot;*.raku&quot;: &quot;raku&quot;, &quot;*.rakumod&quot;: &quot;raku&quot;, &quot;*.rb&quot;: &quot;ruby&quot;, &quot;*.rs&quot;: &quot;rust&quot;, &quot;*.sass&quot;: &quot;sass&quot;, &quot;*.scss&quot;: &quot;scss&quot;, &quot;*.sh&quot;: &quot;shellscript&quot;, &quot;*.shader&quot;: &quot;shaderlab&quot;, &quot;*.slim&quot;: &quot;slim&quot;, &quot;*.sql&quot;: &quot;sql&quot;, &quot;*.styl&quot;: &quot;stylus&quot;, &quot;*.svelte&quot;: &quot;svelte&quot;, &quot;*.swift&quot;: &quot;swift&quot;, &quot;*.tex&quot;: &quot;tex&quot;, &quot;*.ts&quot;: &quot;typescript&quot;, &quot;*.tsx&quot;: &quot;typescriptreact&quot;, &quot;*.txt&quot;: &quot;plaintext&quot;, &quot;*.vb&quot;: &quot;vb&quot;, &quot;*.vue&quot;: &quot;vue&quot;, &quot;*.xml&quot;: &quot;xml&quot;, &quot;*.xsl&quot;: &quot;xsl&quot;, &quot;*.xslt&quot;: &quot;xsl&quot;, &quot;*.yaml&quot;: &quot;yaml&quot;, &quot;*.yml&quot;: &quot;yaml&quot;, &quot;*&quot;: &quot;plaintext&quot; &#125;&#125; Notice the last line: \"*\": \"plaintext\" - this forces every file not matched above, including all files with no extension, to open as plain text. Why this is so much better No more weird guesses: Scratch files stay as plain text. You get what you expect: Every major extension gets its proper language features. You can still override manually: For rare ambiguous cases, you can still set the language from the bottom right and VS Code remembers per file. Conclusion VS Code's default language detection tries to be smart, but often outsmarts itself. By making associations deterministic and catching all other files as plain text, you make your workflow saner and more predictable. Try it out and enjoy a quieter, less-annoying VS Code!","categories":[{"name":"Development Environments","slug":"Development-Environments","permalink":"https://jifengwu2k.github.io/categories/Development-Environments/"}],"tags":[]},{"title":"Using DevTools Console for Web Scraping","slug":"Using-DevTools-Console-for-Web-Scraping","date":"2025-08-12T04:00:00.000Z","updated":"2025-08-13T04:31:00.461Z","comments":true,"path":"2025/08/12/Using-DevTools-Console-for-Web-Scraping/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/12/Using-DevTools-Console-for-Web-Scraping/","excerpt":"","text":"Convert an HTML Table to a Pandas-compatible JSON If you want to convert an HTML table to a Pandas-compatible JSON: 1234&#123; &quot;column1&quot;: [value1, value2, ...], &quot;column2&quot;: [value1, value2, ...]&#125; you can do this in a browser using DOM manipulation: Extract headers: Get the header text from the &lt;th&gt; elements. Build the output object: Each header is a key pointing to an array. Fill columns: Loop over the rows, pushing cell values to the appropriate key/array. 1234567891011121314151617181920212223242526272829function tableToPandasJson(table) &#123; // Get the headers from the first row of the table head var thead = table.getElementsByTagName(&#x27;thead&#x27;)[0]; var headerCells = thead.getElementsByTagName(&#x27;th&#x27;); var headers = []; for (var i = 0; i &lt; headerCells.length; i++) &#123; headers.push(headerCells[i].innerText); &#125; // Initialize the result object, one array per header var result = &#123;&#125;; for (var j = 0; j &lt; headers.length; j++) &#123; result[headers[j]] = []; &#125; // Go through each row in tbody var tbody = table.getElementsByTagName(&#x27;tbody&#x27;)[0]; var rows = tbody.getElementsByTagName(&#x27;tr&#x27;); for (var r = 0; r &lt; rows.length; r++) &#123; var cells = rows[r].getElementsByTagName(&#x27;td&#x27;); for (var c = 0; c &lt; headers.length; c++) &#123; // Always treat as text var cellText = cells[c].innerText; result[headers[c]].push(cellText); &#125; &#125; return result;&#125; Example Example HTML table 123456789101112131415161718&lt;table id=&quot;myTable&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Name&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Alice&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bob&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; JavaScript code 1234var table = document.getElementById(&#x27;myTable&#x27;);var pandasJson = tableToPandasJson(table);console.log(JSON.stringify(pandasJson));// Output: &#123;&quot;Name&quot;:[&quot;Alice&quot;,&quot;Bob&quot;],&quot;Age&quot;:[&quot;25&quot;,&quot;30&quot;]&#125;","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://jifengwu2k.github.io/categories/Data-Science/"}],"tags":[]},{"title":"Copying Files via `cat` and `dd`","slug":"Copying-Files-via-cat-and-dd","date":"2025-08-12T04:00:00.000Z","updated":"2025-08-13T04:49:33.070Z","comments":true,"path":"2025/08/12/Copying-Files-via-cat-and-dd/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/12/Copying-Files-via-cat-and-dd/","excerpt":"","text":"cat and dd are standard Unix utilities for handling file data. cat outputs the contents of a file to stdout. dd reads stdin (if no if=) and writes to stdout or a file. To copy a file, you can use a Unix pipe (|) to send cat's output to dd, then write to a destination file: 1cat sourcefile | dd of=destinationfile Potential Advantages of cat and dd Over cp Better progress/statistics dd with the status=progress (GNU dd) option shows live copy statistics: 1cat bigfile | dd of=outfile status=progress Working Around cp Limitations Some device files, file descriptors, or pseudo-files (like /proc or /sys) do not support cp, but streaming with cat + dd may work.","categories":[{"name":"System Administration","slug":"System-Administration","permalink":"https://jifengwu2k.github.io/categories/System-Administration/"}],"tags":[]},{"title":"Generating Google Calendar Event URLs","slug":"Generating-Google-Calendar-Event-URLs","date":"2025-08-12T04:00:00.000Z","updated":"2025-08-13T04:47:45.848Z","comments":true,"path":"2025/08/12/Generating-Google-Calendar-Event-URLs/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/12/Generating-Google-Calendar-Event-URLs/","excerpt":"","text":"You can generate your own Google Calendar event URLs following this general structure: Google Calendar Event URL Format 123456https://calendar.google.com/calendar/render?action=TEMPLATE&amp; text=EVENT_TITLE&amp; dates=START_DATE_TIME/END_DATE_TIME&amp; details=EVENT_DESCRIPTION&amp; location=EVENT_LOCATION&amp; ctz=TIMEZONE Parameter Breakdown Parameter Purpose Format/Example Required? text The event title (URL-encoded) text=Grad%20Connect%202025 Yes dates Start/end date and time, in YYYYMMDDTHHMMSS dates=20250821T100000/20250821T130000 (T for time) Yes details Event description (URL-encoded) details=Description%20here... No location Event location (URL-encoded) location=Ramin%20Room%2C%20Bartels%20Hall... No ctz IANA timezone name (URL-encoded) ctz=America/New_York No Create Your Own To generate these URLs yourself, you can use the following Python function: 12345678910111213141516171819202122232425262728293031323334353637383940import datetimeimport sysfrom collections import OrderedDictfrom typing import Optionalif sys.version_info &lt; (3,): from urllib import quoteelse: from urllib.parse import quotedef generate_google_calendar_event_url( title, # type: str start_datetime, # type: datetime.datetime end_datetime, # type: datetime.datetime description=None, # type: Optional[str] location=None, # type: Optional[str] iana_timezone_name=None # type: Optional[str]): base_url = &quot;https://calendar.google.com/calendar/render&quot; query_string_fragments = [ &#x27;action=TEMPLATE&#x27;, &#x27;text=%s&#x27; % quote(title), &#x27;dates=%s/%s&#x27; % (start_datetime.strftime(&#x27;%Y%m%dT%H%M%S&#x27;), end_datetime.strftime(&#x27;%Y%m%dT%H%M%S&#x27;)) ] if description is not None: query_string_fragments.append(&#x27;details=%s&#x27; % quote(description)) if location is not None: query_string_fragments.append(&#x27;location=%s&#x27; % quote(location)) if iana_timezone_name is not None: query_string_fragments.append(&#x27;ctz=%s&#x27; % quote(iana_timezone_name)) query_string = &#x27;&amp;&#x27;.join(query_string_fragments) return &#x27;%s?%s&#x27; % (base_url, query_string) Example Suppose you want an event: - Title: Sample Event - Date/Time: June 10, 2024, 2pm to 3:30pm - Description: Don't miss this important meeting! - Location: 123 Main St, New York, NY - Time zone: America/New_York Here's how you'd create the URL: 1234567891011from zoneinfo import ZoneInfogenerate_google_calendar_event_url( title=&#x27;Sample Event&#x27;, start_datetime=datetime.datetime(2024, 6, 10, 14, 00), end_datetime=datetime.datetime(2024, 6, 10, 15, 30), description=&quot;Don&#x27;t miss this important meeting!&quot;, location=&#x27;123 Main St, New York, NY&#x27;, iana_timezone_name=&#x27;America/New_York&#x27;)","categories":[{"name":"Process Automation","slug":"Process-Automation","permalink":"https://jifengwu2k.github.io/categories/Process-Automation/"}],"tags":[]},{"title":"Manipulating `DataFrame`s Using `pandas`","slug":"Manipulating-DataFrame-s-Using-pandas","date":"2025-08-12T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2025/08/12/Manipulating-DataFrame-s-Using-pandas/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/12/Manipulating-DataFrame-s-Using-pandas/","excerpt":"","text":"One DataFrame has the columns A, B and another has the columns A, C. How to merge into one DataFrame with columns A, B, and C? You can achieve this using pd.merge() in pandas with the how='outer' argument. This will merge on the common column A and include all rows from both DataFrames, filling in missing values (as NaN) where the data does not exist. Here's an example: 1234567891011121314151617import pandas as pd# Example datadf1 = pd.DataFrame(&#123; &#x27;A&#x27;: [1, 2, 3], &#x27;B&#x27;: [&#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;]&#125;)df2 = pd.DataFrame(&#123; &#x27;A&#x27;: [2, 3, 4], &#x27;C&#x27;: [&#x27;P&#x27;, &#x27;Q&#x27;, &#x27;R&#x27;]&#125;)# Merge on column &#x27;A&#x27;merged = pd.merge(df1, df2, on=&#x27;A&#x27;, how=&#x27;outer&#x27;)print(merged) Result: 12345 A B C0 1 X NaN1 2 Y P2 3 Z Q3 4 NaN R Iterate over rows and access columns in a DataFrame If the column names are valid Python identifiers, using itertuples() to yield namedtuples is fastest: 12for row in df.itertuples(): print(row.Index, row.A, row.B) # Access columns with dot notation If not all column names are valid Python identifiers (e.g., some column names contain spaces), use iterrows() to yield an index and a Series for each row: 12for index, row in df.iterrows(): print(index, row[&#x27;A&#x27;], row[&#x27;B&#x27;]) # Access columns via indexing","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"https://jifengwu2k.github.io/categories/Data-Science/"}],"tags":[]},{"title":"Powering `clangd`-based C++ IDEs with `compile_commands.json`","slug":"Powering-clangd-based-C-IDEs-with-compile-commands-json","date":"2025-08-11T04:00:00.000Z","updated":"2025-08-25T03:48:39.649Z","comments":true,"path":"2025/08/11/Powering-clangd-based-C-IDEs-with-compile-commands-json/","link":"","permalink":"https://jifengwu2k.github.io/2025/08/11/Powering-clangd-based-C-IDEs-with-compile-commands-json/","excerpt":"","text":"What is compile_commands.json? clangd, the C++ language server that powers IDE features in VS Code, CLion, etc. such as code navigation, linting and error detection, and refactoring, requires compile_commands.json, a JSON file that records exactly how each source file in your project should be compiled. The example shows a simple structure: 1234567[ &#123; &quot;directory&quot;: &quot;/path/to/project&quot;, &quot;command&quot;: &quot;clang++ -std=c++11 -g -Og main.cpp -o main&quot;, &quot;file&quot;: &quot;main.cpp&quot; &#125;] Each entry contains: directory: The absolute path of where the compilation occurs command: The full compilation command (Shell features such as variable and command substitution are NOT supported) file: The relative path of the source file being compiled Generating compile_commands.json You can create one manually as shown in the following Shell script: 12345678910111213141516171819# The absolute path of the project directory (where the compilation occurs)DIRECTORY=&quot;$(realpath .)&quot;# The relative path of the source file being compiledFILE=&#x27;main.cpp&#x27;# The full compilation command (precompute variable and command substitutions)COMMAND=&quot;clang++ -std=c++11 -g -Og -fprofile-instr-generate -fcoverage-mapping main.cpp -o main $(python3-config --includes) $(python3-config --ldflags)&quot;# Generate `compile_commands.json` under the project directorycat &gt; &quot;$DIRECTORY/compile_commands.json&quot; &lt;&lt;EOF[ &#123; &quot;directory&quot;: &quot;$DIRECTORY&quot;, &quot;command&quot;: &quot;$COMMAND&quot;, &quot;file&quot;: &quot;$FILE&quot; &#125;]EOF This approach works well for small projects. For larger ones, consider using CMake or bear (for make-based projects).","categories":[{"name":"Development Environments","slug":"Development-Environments","permalink":"https://jifengwu2k.github.io/categories/Development-Environments/"}],"tags":[]},{"title":"英语名词新译","slug":"英语名词新译","date":"2025-07-30T04:00:00.000Z","updated":"2025-09-20T22:44:11.191Z","comments":true,"path":"2025/07/30/英语名词新译/","link":"","permalink":"https://jifengwu2k.github.io/2025/07/30/%E8%8B%B1%E8%AF%AD%E5%90%8D%E8%AF%8D%E6%96%B0%E8%AF%91/","excerpt":"","text":"“哎妈呀，这不就是咱们屯子里的那个……吗！” 专有名词 Berkeley 桦林 古英语的 *beorc* 意为桦树，*leah* 意为林地、空地 Carmichael 米哈伊尔堡 苏格兰盖尔语的 caer 意为“要塞”或“堡垒” Cornell 角斗士 古凯尔特语的 corn 意为“角”或“突出物 ”，wall 意为“统治者”或“勇士” Dunbar 山头堡 苏格兰盖尔语的 dùn 意为“堡垒”，barr 意为“顶部”或“山” Hudson River 双流江 在欧洲人到来之前，莱纳佩人将这条河称为 Muhheakantuck 这个名字的含义是 “两向流动的河”。 由于大西洋潮汐的影响，河水在入海口附近会随着潮涨潮落而改变流向，这对原住民的航行和生活至关重要。 Manhattan 山洲 莱纳佩人（Lenape，也称为德拉瓦人，Delaware）的语言。 Manah- ：意为“山”、“岛”、“或“山丘”。 -ata 或 -hatin ：意为“的”。 Mississippi 大江 源自奥吉布瓦语（Ojibwe） Misi （或 michi）：意为“大的”、“伟大的”。 ziibi：意为“河”。 St. Lawrence River 通衢江 Kaniatarowanenneh 源自莫霍克语（Mohawk） 直译：“大水道” 或 “大独木舟之路” Kaniá- 指的是“船”或“独木舟”。 -taro- 意为“湖”或“广阔的水体”。 -wanenneh 意为“大的”或“伟大的”。 Wegman 路人 Middle High German word weg (\"way\" or \"road\") combined with mann (\"man\") 政治名词 county 县 美国无封建传统，不得将 county 译为“郡”。 英国的“郡”英语为 shire。因此，可以将 New Hampshire 翻译为“新汉普郡”。 borough 镇（“武汉三镇”的“镇”） 从词源上，borough 意为“a fortified town”，因此翻译为“镇”而不是“区”。 district 区 capitol 国会山 sheriff 治安官 英国市制单位（起源于农业生产生活实践） 长度 inch 英寸 foot 英尺 yard 英步 mile 英里 中国古代的长度单位主要包括以人体测量为基础的寸、尺、咫、丈、寻、仞，以及更小的单位如分、厘、毫、秒、忽。此外，还有用于较长距离的步、里、舍等单位。 寸：《说文解字》定义为十分之一尺，以手掌后退一寸的动脉位置为“寸口”而命名，并以“手”字为部首。 尺：最初指代人的身体长度，通常为十寸，与人体的一臂之长相关。 咫：特指妇女手伸展后拇指和中指之间的距离，比尺稍短，后用来与“尺”连用，表示距离很近。 丈：等于十尺，是一个较大的长度单位。 寻：与仞相当，大约为八尺。 仞：指人伸开双臂的长度。 步：在古代常用于测量地面距离。 里：一个源于周代的长度单位，用于测定距离。 面积 acre 英亩 重量/体积/容积 ounce 英两 pound 英斤 pint 英合 quart 英升 gallon 英斗 bushel 英斛 古代的重量计量单位中，1钧= 30斤、1斤= 16两、1两=24铢、1铢= 10圭。 在古代，除了用钧、圭、铢、斤、两代表重量外，较重、较多量的物的多少更多的是采用容积来代表。 从宋朝开始，我国古代的计量单位固定位石、斛、斗、升、合。它们之间的换算都是1石 = 2斛、1斛 = 5斗、1斗 = 10升、1升 = 10合。不过各时期所代表的实际的量不同。","categories":[{"name":"Linguistics","slug":"Linguistics","permalink":"https://jifengwu2k.github.io/categories/Linguistics/"}],"tags":[]},{"title":"Metaclass Fundamentals","slug":"Metaclass-Fundamentals","date":"2025-07-25T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2025/07/25/Metaclass-Fundamentals/","link":"","permalink":"https://jifengwu2k.github.io/2025/07/25/Metaclass-Fundamentals/","excerpt":"","text":"Metaclasses are one of Python's most powerful yet least understood features. They enable patterns that would be difficult or impossible with regular class definitions. In this post, we'll explore metaclass fundamentals through simple examples. What are Metaclasses? A class C's metaclass is basically type(C). If you define class Class(metaclass=ClassMeta) in Python, then type(Class) is ClassMeta. If we don't explicitly set a metaclass for a class, then its metaclass is type by default. Thus, ClassMeta should inherit from type. Just like given o = Class(...), and that Class defines a method def f(self, ...), o.f(...) would result in calling Class.f(o, ...), if ClassMeta defines a method def g(self, ...), Class.g(...) would result in calling ClassMeta.g(...). We'll present a concrete example below. 12345678910111213class ClassMeta(type): def __call__(self, *args, **kwargs): instance = type.__call__(self, *args, **kwargs) return instanceclass Class(object, metaclass=ClassMeta): def __new__(cls, *args, **kwargs): instance = object.__new__(cls) return instance def __init__(self, *args, **kwargs): object.__init__(self) What happens when we run: 1c = Class(0, 1, 2, message=&#x27;Hello World&#x27;) Class(0, 1, 2, message='Hello World') is syntactic sugar for Class.__call__(0, 1, 2, message='Hello World'). If we haven't set a metaclass for Class, then this in turn invokes type.__call__(Class, 0, 1, 2, message='Hello World'). However, we have set Class's metaclass to ClassMeta, whose ClassMeta.__call__ overrides type.__call__. Thus, Class.__call__(0, 1, 2, message='Hello World') would invoke ClassMeta.__call__(Class, 0, 1, 2, message='Hello World') instead. With a few print statements added, we can see the function calls: 123456in ClassMeta.__call__(self, *args, **kwargs), self = &lt;class &#x27;__main__.Class&#x27;&gt;, args = (0, 1, 2), kwargs = &#123;&#x27;message&#x27;: &#x27;Hello World&#x27;&#125;, calling type.__call__(self, *args, **kwargs)...in Class.__new__(cls, *args, **kwargs), cls = &lt;class &#x27;__main__.Class&#x27;&gt;, args = (0, 1, 2), kwargs = &#123;&#x27;message&#x27;: &#x27;Hello World&#x27;&#125;, calling object.__new__(cls)...in Class.__new__(cls, *args, **kwargs), after calling object.__new__(cls), instance: &lt;__main__.Class object at 0x7ea1253346e0&gt;in Class.__init__(self, *args, **kwargs), self = &lt;__main__.Class object at 0x7ea1253346e0&gt;, args = (0, 1, 2), kwargs = &#123;&#x27;message&#x27;: &#x27;Hello World&#x27;&#125;, calling object.__init__(self)...in Class.__init__(self, *args, **kwargs), after calling object.__init__(self)in ClassMeta.__call__(self, *args, **kwargs), after calling type.__call__(self, *args, **kwargs), instance: &lt;__main__.Class object at 0x7ea1253346e0&gt; Metaclass Inheritance Metaclasses are \"contagious\" - when you inherit from a class with a metaclass, the child class gets the same metaclass: 12345678910111213141516171819202122class ParentMeta(type): def __call__(self, *args, **kwargs): instance = type.__call__(self, *args, **kwargs) return instanceclass Parent(object, metaclass=ParentMeta): def __new__(cls, *args, **kwargs): instance = object.__new__(cls) return instance def __init__(self, *args, **kwargs): object.__init__(self)class Child(Parent): def __new__(cls, *args, **kwargs): instance = Parent.__new__(cls, *args, **kwargs) return instance def __init__(self, *args, **kwargs): Parent.__init__(self, *args, **kwargs) In this case, 1c = Child(0, 1, 2, message=&#x27;Hello World&#x27;) would still result in invoking ParentMeta.__call__(Child, 0, 1, 2, message='Hello World'). Practical Application 1: Singleton Pattern Metaclasses provide an elegant way to implement the Singleton pattern: 1234567891011121314151617181920212223class SingletonMeta(type): _instances = &#123;&#125; def __call__(self, *args, **kwargs): if cls not in cls._instances: cls._instances[cls] = super().__call__(*args, **kwargs) return cls._instances[cls]# SingletonBase(...) gets rerouted to SingletonMeta(SingletonBase, ...)class SingletonBase(metaclass=SingletonMeta): pass# SingletonChild(...) also gets rerouted to SingletonMeta(SingletonChild, ...)class SingletonChild(SingletonBase): passa = SingletonBase()b = SingletonBase()print(a is b) # Truex = SingletonChild()y = SingletonChild()print(x is y) # True Practical Application 2: Template Metaprogramming We can simulate C++-style templates using metaclasses: In C++: 12345templace &lt;V VALUE = default&gt; class C &#123; // Use `VALUE` here&#125;;C&lt;v&gt; *c = new C&lt;v&gt;(...); In Python: 123456789101112131415161718192021class ClassMeta(type): _values_to_instantiations = &#123;&#125; value = None def __getitem__(self, value): if value in self._values_to_instantiations: instantiation = self._values_to_instantiations[value] else: # Dynamically create `self[value]` containing the class variable VALUE as a subclass of `self` instantiation = type( &#x27;%s[%s]&#x27; % (self.__name__, value), (self,), &#123; &#x27;VALUE&#x27;: value &#125; ) self._values_to_instantiations[value] = instantiation return instantiationclass Class(object, metaclass=ClassMeta): # Use `self.VALUE` or `cls.VALUE` here pass Then, after we run: 12345678# Dynamically create `Class[True]`class_true = Class[True]# Create an instance of `Class[True]`c1 = class_true()# Dynamically create `Class[False]`class_false = Class[False]# Create an instance of `Class[False]`c2 = class_false() we can get something like: 1234&gt;&gt;&gt; c1&gt;&gt;&gt; &lt;__main__.Class[True] at 0x796bbb89d3a0&gt;&gt;&gt;&gt; c2&gt;&gt;&gt; &lt;__main__.Class[False] at 0x796bbc53fb60&gt;","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Python","slug":"Software-Design/Python","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Python/"}],"tags":[]},{"title":"Creating Custom Data Structures from Abstract Containers in `collections.abc`","slug":"Creating-Custom-Data-Structures-from-Abstract-Containers-in-collections-abc","date":"2025-07-24T04:00:00.000Z","updated":"2025-08-13T04:31:00.447Z","comments":true,"path":"2025/07/24/Creating-Custom-Data-Structures-from-Abstract-Containers-in-collections-abc/","link":"","permalink":"https://jifengwu2k.github.io/2025/07/24/Creating-Custom-Data-Structures-from-Abstract-Containers-in-collections-abc/","excerpt":"","text":"Creating Custom Data Structures from Abstract Containers in collections.abc Python's collections.abc module provides a set of abstract base classes (ABCs) that define the interfaces for various container types. These ABCs serve as excellent starting points when you need to create custom data structures that behave like built-in containers. In this post, we'll explore the hierarchy of these abstract containers and explain why inheriting from them is beneficial. The Hierarchy of Abstract Containers The abstract container classes in collections.abc form a logical inheritance hierarchy that mirrors how we think about container relationships: Container (requires __contains__) Defines the in operator behavior Iterable (requires __iter__) Adds iteration capability Sized (requires __len__) Adds length measurement capability Collection (combines Container, Iterable, and Sized) A complete basic collection From these fundamental abstract container classes, more specialized containers branch out: Sequence (combines Reversible and Collection, and also requires __getitem__) Represents immutable sequences (like tuple) Provides index() and count() automatically Mapping (inherits from Collection, and also requires __getitem__) Represents read-only dictionaries Provides get(), items(), keys(), values() automatically Set (inherits from Collection, requires no more methods) Represents immutable sets Provides isdisjoint() and support for &amp;, |, -, ^, &lt;, &lt;=, &gt;, &gt;=, == operations automatically The inheritance hierarchy of all abstract container classes is as follows: classDiagram Awaitable &lt;|-- Coroutine Iterable &lt;|-- Reversible Iterable &lt;|-- Iterator Iterator &lt;|-- Generator Sized &lt;|-- MappingView Iterable &lt;|-- Collection Sized &lt;|-- Collection Container &lt;|-- Collection Collection &lt;|-- ValuesView MappingView &lt;|-- ValuesView Collection &lt;|-- Mapping Collection &lt;|-- Sequence Reversible &lt;|-- Sequence Sequence &lt;|-- ByteString Sequence &lt;|-- MutableSequence Collection &lt;|-- Set Set &lt;|-- ItemsView MappingView &lt;|-- ItemsView Set &lt;|-- MutableSet Mapping &lt;|-- MutableMapping AsyncIterable &lt;|-- AsyncIterator AsyncIterator &lt;|-- AsyncGenerator To get a list of all abstract methods you must implement when inheriting an abstract container class, you can use the following function: 12345678910111213import abcdef get_all_abstract_methods(abstract_base_class): # type: (type) -&gt; list[str] all_abstract_methods = set() for mro_entry in abstract_base_class.__mro__: if isinstance(mro_entry, abc.ABCMeta) or hasattr(mro_entry, &#x27;__abstractmethods__&#x27;): for abstract_method in mro_entry.__abstractmethods__: all_abstract_methods.add(abstract_method) return sorted(all_abstract_methods) Why Inherit from Abstract Containers? When you create a custom data structure by inheriting from these abstract container classes, you get several benefits: Interface Clarity: The abstract container classes clearly document what abstract methods your class needs to implement to be a proper container (see get_all_abstract_methods above). Type Checking Compatibility: Your custom class will be recognized as the proper type by isinstance() and issubclass() checks, as well as by third-party type-checking tools such as mypy. Automatic Method Implementation: The abstract container classes provide many methods automatically once you implement the required abstract methods (see the examples of Sequence, Mapping, and Set above). Consistent Behavior: Your custom container will behave like Python developers expect it to.","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Python","slug":"Software-Design/Python","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Python/"}],"tags":[]},{"title":"Python Packaging: Managing Package Files, Compiling Extension Modules When Building a Wheel, and Uploading to PyPI","slug":"Python-Packaging-Managing-Package-Files-Compiling-Extension-Modules-When-Building-a-Wheel-and-Uploading-to-PyPI","date":"2025-07-11T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2025/07/11/Python-Packaging-Managing-Package-Files-Compiling-Extension-Modules-When-Building-a-Wheel-and-Uploading-to-PyPI/","link":"","permalink":"https://jifengwu2k.github.io/2025/07/11/Python-Packaging-Managing-Package-Files-Compiling-Extension-Modules-When-Building-a-Wheel-and-Uploading-to-PyPI/","excerpt":"","text":"Managing Package Files When creating a Python package, the location of files depends on your project structure. Basic pyproject.toml Configuration 1234567891011121314151617181920212223242526[build-system]requires = [&quot;setuptools&quot;]build-backend = &quot;setuptools.build_meta&quot;[project]name = &quot;your-package&quot; # PyPI name (can contain hyphens)version = &quot;0.1.0&quot;description = &quot;A description of your-package&quot;readme = &quot;README.md&quot;requires-python = &quot;&gt;=2&quot;license = &quot;MIT&quot;authors = [ &#123; name=&quot;Jane Doe&quot;, email=&quot;jane.doe@example.com&quot; &#125;]classifiers = [ &quot;Programming Language :: Python :: 2&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Operating System :: OS Independent&quot;]dependencies = [ &quot;numpy&quot;][project.urls]&quot;Homepage&quot; = &quot;https://github.com/janedoe/my-package&quot;&quot;Bug Tracker&quot; = &quot;https://github.com/janedoe/my-package/issues&quot; Single Python File (or Extension Module) as Module 12345your-package/ - pyproject.toml - your_package.py # or `your_package.cpython-312-x86_64-linux-gnu.so`, etc. - README.md - tests/ This single-file module will be copied into the site-packages directory during installation. ⚠️ This layout makes it difficult to include non-.py or non-extension data files (e.g., .json, .html). If you need to include such resources, use the folder-as-module approach described below. Folder as Module 12345678910111213141516171819202122your-package/ - pyproject.toml - your_package/ - __init__.py - module1.py - module2.py - subpackage1/ - __init__.py - module_a.py - subpackage2/ - __init__.py - module_b.py - utils/ - __init__.py - helpers.py - data/ - config.json - schema.sql - templates/ - default.html - README.md - tests/ Each subfolder intended as a submodule must include an __init__.py file (even if empty). Configure pyproject.toml: 1234[tool.setuptools]# Importable package name (no hyphens allowed)packages = [&quot;your_package&quot;]package-dir = &#123;&quot;your_package&quot; = &quot;your_package&quot;&#125; To include non-.py or non-extension data files (e.g., configs, templates): 123456789[tool.setuptools]include-package-data = true[tool.setuptools.package-data]your_package = [ &quot;data/*.json&quot;, &quot;templates/*.html&quot;, &quot;static/**/*&quot;, # Recursively include all files under `static`] Compiling Extension Modules When Building a Wheel To compile extension modules when building a wheel, implement a custom setuptools build_ext command. Project Structure 12345your-package/ - pyproject.toml - your_package/ - __init__.py - _build_ext_command.py # Custom `build_ext` command Custom build_ext Command (your_package/_build_ext_command.py) 1234567891011121314151617import osimport subprocessfrom setuptools.command.build_ext import build_extdef check_compiler(self): try: subprocess.check_call([&quot;c++&quot;, &quot;-v&quot;], stdout=subprocess.DEVNULL) return True except: return Falseclass BuildExtCommand(build_ext): def run(self): if not check_compiler(): raise RuntimeError(&quot;c++ compiler not found!&quot;) subprocess.check_call([&quot;c++&quot;, &quot;mymodule.cpp&quot;, &quot;-o&quot;, &quot;mymodule.so&quot;]) super().run() # Continue default build_ext flow (no-op) pyproject.toml Configuration 123456[tool.setuptools]# Declare the package as non-ZIP-safe to ensure files are extracted during installation# Required for custom build logic to execute properlyzip-safe = false# Bind the custom command to the `build_ext` phasecmdclass = &#123; &quot;build_ext&quot; = &quot;your_package._build_ext_command.BuildExtCommand&quot; &#125; If BuildExtCommand requires third-party libraries (e.g., pybind11), declare them under [build-system]: 12345678[build-system]requires = [ &quot;setuptools&gt;=42&quot;, &quot;wheel&quot;, &quot;pybind11&gt;=2.6&quot;, &quot;numpy&gt;=1.21&quot;, &quot;requests&gt;=2.25&quot;,] Uploading to PyPI Install Required Tools First, install build and twine: 1pip install build twine Build the Package Run the following command in your project's root directory: 1python -m build This generates .tar.gz and .whl files in the dist/ folder. Upload to PyPI Use twine to upload your package. Navigate to the dist/ directory and run: 1twine upload dist/* You'll be prompted for your PyPI API token. Refer to the official PyPI documentation for details. Verify Publication After uploading, check PyPI to see if your package is listed: https://pypi.org/ Search for your package name-it may take a few minutes to appear.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://jifengwu2k.github.io/categories/DevOps/"}],"tags":[]},{"title":"Managing Development Environments with Conda","slug":"Managing-Development-Environments-with-Conda","date":"2025-07-10T04:00:00.000Z","updated":"2025-08-25T03:48:53.852Z","comments":true,"path":"2025/07/10/Managing-Development-Environments-with-Conda/","link":"","permalink":"https://jifengwu2k.github.io/2025/07/10/Managing-Development-Environments-with-Conda/","excerpt":"","text":"When juggling multiple data science or machine learning projects, maintaining isolated, reproducible environments is crucial. conda has emerged as the de facto standard in Python for managing such environments. But why should you use conda over alternatives like venv, poetry, pipenv, system package managers like apt, or container tools like Docker? Let's find out - and get you set up! Why Conda? Conda offers several key advantages: Beyond Python Packages: Unlike venv, pipenv, or poetry, conda handles both Python packages and complex system-level dependencies, such as CUDA toolkits for GPU acceleration, C/C++ compiler toolchains, and other command-line utilities. No Admin Required: With system-wide package managers like apt, you'll often need root privileges to install software. In comparison, Conda lets you install everything in user-space. Better than Docker for Rapid Iteration: While Docker excels at packaging for production, it often means verbose Dockerfiles, long build times, and a less pleasant development experience. Conda offers a lightweight, rapid way to set up and switch between environments on your local machine. Installing Conda (Miniconda) The preferred way to get started is with Miniconda, a minimal conda installer. Here's a step-by-step guide: Create an Installation Directory: 1mkdir -p ~/miniconda3 Download the Miniconda Installer: Linux x86_64: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh Linux aarch64: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh -O ~/miniconda3/miniconda.sh Linux s390x: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-s390x.sh -O ~/miniconda3/miniconda.sh Linux ppc64le: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-ppc64le.sh -O ~/miniconda3/miniconda.sh Linux x86: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86.sh -O ~/miniconda3/miniconda.sh Linux armv7l: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-armv7l.sh -O ~/miniconda3/miniconda.sh macOS arm64: curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh -o ~/miniconda3/miniconda.sh macOS x86_64: curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o ~/miniconda3/miniconda.sh macOS x86: curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86.sh -o ~/miniconda3/miniconda.sh Run the Installer 1234bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 &amp;&amp; \\ rm ~/miniconda3/miniconda.sh &amp;&amp; \\ source ~/miniconda3/bin/activate &amp;&amp; \\ conda init --all What does this do? -b: Run in batch (no interactive prompts) -u: Unpack the installer -p ~/miniconda3: Install into this directory &amp;&amp; rm ...: Clean up the installer file &amp;&amp; source ~/miniconda3/bin/activate: Activate conda &amp;&amp; conda init --all: Set up conda for your shell (e.g., bash/zsh/fish) Managing Conda Environments List all environments: 1conda env list Remove an environment: 1conda env remove --name ENV_NAME Replace ENV_NAME with the actual name (e.g., myenv). This will delete the environment and all its files.","categories":[{"name":"Development Environments","slug":"Development-Environments","permalink":"https://jifengwu2k.github.io/categories/Development-Environments/"}],"tags":[]},{"title":"用汉语拼音解读国际音标，快速读出任何语言的发音","slug":"用汉语拼音解读国际音标，快速读出任何语言的发音","date":"2025-06-30T04:00:00.000Z","updated":"2025-08-21T23:04:07.394Z","comments":true,"path":"2025/06/30/用汉语拼音解读国际音标，快速读出任何语言的发音/","link":"","permalink":"https://jifengwu2k.github.io/2025/06/30/%E7%94%A8%E6%B1%89%E8%AF%AD%E6%8B%BC%E9%9F%B3%E8%A7%A3%E8%AF%BB%E5%9B%BD%E9%99%85%E9%9F%B3%E6%A0%87%EF%BC%8C%E5%BF%AB%E9%80%9F%E8%AF%BB%E5%87%BA%E4%BB%BB%E4%BD%95%E8%AF%AD%E8%A8%80%E7%9A%84%E5%8F%91%E9%9F%B3/","excerpt":"","text":"语言是用来交流的，不是用来炫耀、排斥、孤立的。 你是否遇到过这种情况？ 在土耳其旅游，想对店员说一句“Teşekkür ederim”，但看着字母完全不会读？ 在冰岛听到地名“Eyjafjallajökull”，直接放弃尝试？ 想学几句格鲁吉亚语、阿拉伯语，但连音标都看不懂？ 我们大多数人只学过汉语拼音和英语，但国际音标（IPA）里很多符号看起来像天书。其实，它们大多能用拼音的发音“近似替代”——不追求100%准确，但能让你开口，对方听得懂！ 辅音 ʔ - 汉语拼音a ʕ - 汉语拼音à，加强喉咙发声出气，降低音调 阿拉伯语ع 希伯来语ע p 突厥语、日耳曼语系（英语、德语等）：汉语拼音p（清辅音/送气音） 罗曼语系（法语、西班牙语等）：汉语拼音b（浊辅音/不送气音） ɸ - 汉语拼音f ɲ - 汉语拼音ni 西班牙语ñ t 突厥语、日耳曼语系（英语、德语等）：汉语拼音t（清辅音/送气音） 罗曼语系（法语、西班牙语等）：汉语拼音d（浊辅音/不送气音） ɳ - 汉语拼音na ɾ - 急促的汉语拼音l，快速往后收舌头 日语罗马音r 韩语ㄹ（常音译为\"r\"或\"l\"） 突厥语r 西班牙语r ɫ - 汉语拼音ou q, ɢ - 汉语拼音ga 突厥语q 阿拉伯语ق（常音译为\"q\"） 希伯来语ק k 突厥语、日耳曼语系（英语、德语等）：汉语拼音k（清辅音/送气音） 罗曼语系（法语、西班牙语等）：汉语拼音g（浊辅音/不送气音） ħ - 汉语拼音ha，加强喉咙发声出气，舌头往后卷，提高音调 阿拉伯语ح 希伯来语ח（发音之一） x, χ - 汉语拼音he，加强喉咙发声出气，降低音调 突厥语x 阿拉伯语خ（常音译为\"kh\"） 希伯来语ח（发音之一，常音译为\"ch\"） 希伯来语כ（发音之一） 俄语х 西班牙语j 西班牙语x（发音之一） 德语ch（发音之一） ɣ, ʁ - 汉语拼音e，加强喉咙发声出气，降低音调 突厥语ğ 阿拉伯语غ（常音译为\"gh\"） 法语r 德语r dʒ, dʑ, tɕ - 汉语拼音j ɕ, ç, ɬ - 汉语拼音x 德语ch（发音之一） ʈʂ, ɖʐ - 汉语拼音zh ʃ, ʂ - 汉语拼音sh ʒ, ʐ - 汉语拼音r r - 颤音，发不出来用上面的ɾ、ɣ、ʁ完美替换。 dz - 汉语拼音z ts - 汉语拼音c ʝ, ʎ, ʑ - 汉语拼音yà 西班牙语ll β - 汉语拼音v 西班牙语b（发音之一） 西班牙语v（发音之一） ʋ - 介于汉语拼音v和w之间 元音 ɑ - 汉语拼音a ʌ, ɒ - 汉语拼音à ɪ, ɛ - 汉语拼音ài æ - 美式英语apple的a o, ɞ - 汉语拼音ou ɔ - 汉语拼音ò ʏ, ø, œ - 介于汉语拼音o和ü之间，圆口 突厥语ö 德语ö（常音译为\"oe\"） ə, ʊ, ɤ - 汉语拼音e ɨ, ʉ, ɯ - 介于汉语拼音e和ü之间，扁口 日语う/ウ（常音译为\"u\"） 突厥语ı i - 汉语拼音i u - 汉语拼音u y - 汉语拼音ü","categories":[{"name":"Linguistics","slug":"Linguistics","permalink":"https://jifengwu2k.github.io/categories/Linguistics/"}],"tags":[]},{"title":"Lessons learned from Master's thesis","slug":"Lessons-learned-from-master-s-thesis","date":"2024-08-23T04:00:00.000Z","updated":"2025-08-13T04:31:00.453Z","comments":true,"path":"2024/08/23/Lessons-learned-from-master-s-thesis/","link":"","permalink":"https://jifengwu2k.github.io/2024/08/23/Lessons-learned-from-master-s-thesis/","excerpt":"","text":"Firmly define the research topic. Do literature research based on keywords, never limit the scope of journals and conferences, and continue literature research even after starting the project. Collect baselines and benchmarks as soon as possible; make sure the baselines and benchmarks are downloaded and can be run. Focus on the NOVELTY of the core aspects; don't be perfect, and don't support all cases or possibilities. Find a few small examples that prove the power of your novelty, and emphasize it in both the paper and the presentation. Avoid designing overly complex rule-based systems. With GenAI, aim to get a prototype and an illustrative presentation out as soon as possible. This will allow you to collect feedback early, empowering you to make necessary adjustments and improvements. Use functional programming architecture that is easy to test. Conduct thorough unit testing, document the coverage, and leave traces of development step by step. This will provide a solid foundation and instill confidence in the robustness of your research. Insert breakpoints where you're unsure how to code. Figure out what to do when encountering them, and refine the unit tests in the process. Don't be perfect; start writing your thesis when you're almost done.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Talking back and confidence","slug":"Talking-back-and-confidence","date":"2024-07-07T04:00:00.000Z","updated":"2025-08-13T04:31:00.460Z","comments":true,"path":"2024/07/07/Talking-back-and-confidence/","link":"","permalink":"https://jifengwu2k.github.io/2024/07/07/Talking-back-and-confidence/","excerpt":"","text":"Original (Chinese) 一个人在听到建议的时候喜欢顶嘴，有叛逆心理，某种意义上是想要证明自己，挽回一些自尊与自信，但又暂时没有强大的以自身实力为基础的自信。面对这种情形，我们虽然会不太舒服，但最好不要顶回去，或者是尝试说服对方，这些只会让对方感到更不愉快，进而也让自己感到更不愉快，既不治标也不治本。更好的做法是去承受，去服输，去退一步，不争了，而是借对方的某个观点转移话题——给对方认可、自信与陪伴，和对方一起去进步，去提升，去做需要做的事情。 Translation (DeepL.com) A person who likes to talk back when they hear a suggestion has a rebellious mentality and, in a sense, wants to prove themselves to regain some self-esteem and self-confidence, but for the time being, does not have a robust and deep-rooted self-confidence based on their strength. Although we will be uncomfortable in this situation, it is best not to talk back or try to persuade the other party. These will only make the other party feel more unpleasant and, in turn, make themselves feel more unpleasant - solving neither the symptoms nor the root cause of the problem. The better thing to do is to suffer, to give in, to step back, not to argue, but to take advantage of one of the other person's points to change the subject - to give the other person recognition, self-confidence, and companionship, and together with the other person to progress, to improve, to do what needs to be done.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Some thoughts about chatting","slug":"Some-thoughts-about-chatting","date":"2024-07-05T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2024/07/05/Some-thoughts-about-chatting/","link":"","permalink":"https://jifengwu2k.github.io/2024/07/05/Some-thoughts-about-chatting/","excerpt":"","text":"Original (Chinese) 关于聊天的一点思考： 开始之前预定一个时间，设置一个时间上的边界感；时间上没有边界感容易让我们没话找话，寻找一些无聊的话题，让对方失去兴趣，甚至不断地因此而擦出矛盾来。 对方做得不对，我们要简明扼要地点出来，点到为止，然后换下一个话题；切忌在这种情况下，主动地理论、解释，尝试说服对方。 Translation (DeepL.com) Some thoughts about chatting: Before starting, arrange a time and set a sense of time boundaries. Lacking such a sense makes us constantly look for boring topics when we have nothing to say, which may lead to the other party losing interest and result in conflicts. If the other party is not doing right, we have to point it out succinctly, avoid being overly verbose, and then promptly change to the next topic; do not take the initiative to theorize, explain, and try to persuade the other party in this situation.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Pay attention to the other person's easily overlooked emotional state when chatting online","slug":"Pay-Attention-to-the-Other-Person-s-Easily-Overlooked-Emotional-State-When-Chatting-Online","date":"2024-07-02T04:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2024/07/02/Pay-Attention-to-the-Other-Person-s-Easily-Overlooked-Emotional-State-When-Chatting-Online/","link":"","permalink":"https://jifengwu2k.github.io/2024/07/02/Pay-Attention-to-the-Other-Person-s-Easily-Overlooked-Emotional-State-When-Chatting-Online/","excerpt":"","text":"Original (Chinese) 线上聊天一定要关注对方容易被忽视的情绪状态。 与线下聊天相比，线上聊天少了很多关键的nonverbal context，其中就包括对方的情绪状态。线上聊天还是在和人说话，而不是在和人工智能说话，人的情绪状态会对聊天的走向产生极大的影响。但不是所有的人都擅于、习惯于或想要表达情绪。在这种情况下，我们极易在线上聊天的过程中，因为对方在某种我们尚不清楚的情绪的影响下，不按我们的预期聊天，而让我们做出很多误解误判。为了避免由此产生的一些不愉快的情形，我们可以： 在聊天一开始的时候就去了解对方的情绪状态，如可以直接在寒暄的过程中问。 在聊天进行的过程中时刻去关注对方的情绪状态。包括时不时主动问问对方感觉如何（哪怕对方没有任何反馈或暗示），绝不能自己一个人旁若无人滔滔不绝地讲。同时也要清楚地意识到，对方的任何表达都有可能是心境的投射。我们要从中捕获尽可能多的信息来。 Translation (DeepL.com) Pay attention to the other person's easily overlooked emotional state when chatting online. Compared to offline chatting, online chatting has a much less critical nonverbal context, including the other person's emotional state. Online chatting is still talking to a human being, not an AI, and a person's emotional state can have a huge impact on the direction of the chat. But not all people are good at, used to, or want to express emotions. In this case, it is very easy for us to make a lot of misunderstandings and misjudgments during an online chat because the other person, under the influence of some emotion we are not yet aware of, does not chat as we expect. These misunderstandings can significantly alter the course of the conversation, leading to potential conflicts or misinterpretations. In order to avoid some unpleasant situations arising from this, we can: At the beginning of the chat, it's crucial to understand the other party's emotional state. This can be achieved by asking directly during the initial small talk. When chatting, always consider the other party's emotional state. This includes taking the initiative to ask the other person from time to time how they are feeling (even if the other person does not have any feedback or hints) and never speaking on your own like there is no one else. It's also important to realize that any expression of the other person may be a projection of their state of mind. We need to capture as much information from it as possible.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Show, not tell","slug":"Show-not-tell","date":"2024-06-20T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2024/06/20/Show-not-tell/","link":"","permalink":"https://jifengwu2k.github.io/2024/06/20/Show-not-tell/","excerpt":"","text":"Original (Chinese) 导师在告诉我如何回答reviewers comments的时候曾经说过：“show, not tell”。意思是面对他们希望我们加baseline和metric的comments，在点到我们要加上他们之后，最好展现一些初步结果（比如说在部分数据集上评测那些baseline和metric），而不是一味地说服对方我们之后一定能加上去。 感觉这种思想用在日常生活中的说服上也是很值得借鉴的：在点出一件事之后，如果对方认可了，更好的做法是以身作则去示范他或者是展现自己怎么想，而不是一味地去说服对方应该怎么怎么做。 Translation (DeepL.com) My mentor once said \"show, not tell\" when he told me how to answer reviewers comments. The idea is that when faced with comments that they want us to add baselines and metrics to, it is better to show some preliminary results (e.g., evaluating those baselines and metrics on a part of the dataset) after pointing out that we want to add them to, instead of just convincing the other party that we will definitely be able to add them afterward. I feel that this idea is also useful for persuasion in daily life: after pointing out something, if the other person approves of it, it's better to set an example by demonstrating what he thinks or showing what he thinks, rather than just trying to convince the other person of what he should do.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Our minds are, indeed, the minds of engineers","slug":"Our-minds-are-indeed-the-minds-of-engineers","date":"2024-06-19T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2024/06/19/Our-minds-are-indeed-the-minds-of-engineers/","link":"","permalink":"https://jifengwu2k.github.io/2024/06/19/Our-minds-are-indeed-the-minds-of-engineers/","excerpt":"","text":"Original (Chinese) 我们的思维，的确是工程师的思维。这体现在我们日常生活的方方面面：不论是开车，还是做菜，还是写代码，还是制定计划，我们总是会非常关注计划，关注流程，关注优化，关注总结，关注反馈；而我们的头脑没有那么敏捷，不擅长理论创新——我们是罗马人，而非希腊人。 我们不应该误入理论创新的歧途。是的，我们确实对理论很感兴趣，但是那种兴趣不是形而上的对理论本身的兴趣，而是对理论有何价值以及如何落地的兴趣。 当下的热点是什么？我们应该如何结合某个具体的工业背景，有效而创造性地运用那些理论解决现实问题。这应该是我们的主业。而去研究理论，并思考如何创造性地使用它们，是我们的副业与兴趣爱好。 Translation (DeepL.com) Our minds are, indeed, the minds of engineers. This is reflected in every aspect of our daily lives: whether we are driving, cooking, writing code, or making plans, we will always pay great attention to the plan, to the process, to the optimization, to the summary, and to the feedback; and our minds are not as agile and not as good at theoretical innovation - we are Romans, not Greeks! We shouldn't get sidetracked into theoretical innovation. Yes, we do have an interest in theory, but that interest is not metaphysical interest in the theory itself, it is interest in what the theory is worth and how it can be implemented. What are the hot topics of the moment? How should we effectively and creatively apply those theories to solve real-world problems in a specific industrial context. That should be our main business. Studying theories and thinking about how to use them creatively is our side project and hobby.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Financial strategies during master's and PhD degrees (tentative)","slug":"Financial-strategies-during-master-s-and-PhD-degrees-tentative","date":"2024-06-16T04:00:00.000Z","updated":"2025-08-13T04:31:00.453Z","comments":true,"path":"2024/06/16/Financial-strategies-during-master-s-and-PhD-degrees-tentative/","link":"","permalink":"https://jifengwu2k.github.io/2024/06/16/Financial-strategies-during-master-s-and-PhD-degrees-tentative/","excerpt":"","text":"Original (Chinese) 记录每月工资，跟踪每月固定必需开销（房租、订阅服务等），分配每月可变必需开销（食品百货等）、可变可选开销（衣物、旅行等）与储蓄。 储蓄包括银行的储蓄账户和各种理财产品。 对我们而言，各种理财产品的职责首先是对抗通货膨胀的储蓄账户，而非盈利的手段——我们是斗不过高频交易算法的。不得花过多的时间经历关注理财产品的价格变化，严格禁止各种赌博心理。 充分利用各种税收和学费资助的“免费”服务： 规划好行程，充分利用公共交通，少打车。 充分利用学校提供的医疗保险； 通过学校订购的在线课程，不断提升与科研需求相辅相成的实用技能，如网页前端开发、数据科学等，并获取相应证书，既提升我们欠缺的implementation能力，又方便找实习与工作等。 做好灵活的食品百货采货计划。 关注超市传单； 书包中常备购物袋； 适量减少红肉采货量，适量增加鱼、蛋白粉、谷物、水果、蛋、奶采货量。 Translation (DeepL.com) Record monthly paychecks, track monthly fixed essential expenses (rent, subscription services, etc.), and allocate monthly variable essential expenses (food, groceries, etc.) and variable optional expenses (clothing, travel, etc.) with savings. Savings include savings accounts in banks and various financial products. For us, the duty of various financial products is, first and foremost, an inflation-fighting savings account, not a means of profit - we can't beat high-frequency trading algorithms. We must not spend excessive time paying attention to price changes in financial products and strictly prohibit all kinds of gambling mentality. Take advantage of tax and tuition assistance \"freebies\": Plan trips well, use public transportation, and take fewer cabs. Take advantage of the school's health insurance; Through online courses purchased by the school, improve practical skills that complement research needs, such as web front-end development, data science, etc., and obtain the appropriate certificates to improve our lack of implementation skills and facilitate the search for internships and jobs. Make a flexible food and grocery procurement plan. Pay attention to supermarket flyers; Always carry a shopping bag in your bag; Reduce the amount of red meat and increase the amount of fish, protein powder, cereals, fruits, eggs, and milk.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"On doing things with others","slug":"On-Doing-Things-with-Others","date":"2024-05-30T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2024/05/30/On-Doing-Things-with-Others/","link":"","permalink":"https://jifengwu2k.github.io/2024/05/30/On-Doing-Things-with-Others/","excerpt":"","text":"Original (Chinese) 和别人一起做事的时候，如果遇到了某件说好了要做的事别人不愿意开始做，我们不应该面露难色，变得焦虑、愤怒；我们更好的做法应该是给对方一个善意的提醒，然后重新规划时间——这时候规划的时间必须具体，不能是大而化之的“之后”“再过几天”等。 Translation (DeepL.com) When doing things with others, if we encounter a certain thing that we said we would do that others are not willing to start doing, we should not look embarrassed, become anxious and angry; our better approach should be to give each other a kind reminder, and then re-planning time - this time the planning time must be specific, not a big and generalized \"afterward,\" \"a few more days,\" etc.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Some thoughts on maintaining fitness habits and improving physical fitness in the future","slug":"Some-thoughts-on-maintaining-fitness-habits-and-improving-physical-fitness-in-the-future","date":"2024-02-29T05:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2024/02/29/Some-thoughts-on-maintaining-fitness-habits-and-improving-physical-fitness-in-the-future/","link":"","permalink":"https://jifengwu2k.github.io/2024/02/29/Some-thoughts-on-maintaining-fitness-habits-and-improving-physical-fitness-in-the-future/","excerpt":"","text":"Original (Chinese) 关于今后健身习惯的保持与身体素质的提升的若干思考 在今后漫长的人生之路上，我们要让健身成为一种像散步、跑步、爬山一样深入我们日常生活并且随时都有心情做的消遣活动。 结合我们初高中时期跑步、跳绳、坐位体前屈通过强制性的体育课逐渐成为我们习惯去做并能在不断进步中获得成就感的活动这样一段经历，我们也可以充分利用我们在UBC的最后一学期，每一次去学校学习时，在离开之前尽可能去一趟健身房——且不论时间长短，但还是尽可能去一趟。近日的实践证明，在脑力劳动之后，这样的“体力劳动”在有效缓解我们精神上的疲惫感的同时，又有助于提升我们的身体素质，可谓一举两得。 与之相伴的另一个问题是如何在坚持健身的同时在力量上取得突破。如果有人陪伴我们锻炼，那么这将不是一个难题。但当我们一个人锻炼的时候，我们又将怎么做呢？我们或许也可以从初高中时期独自练习跑步、跳绳、坐位体前屈的经历中获得灵感。 首先，我们必须有耐心。进步必定是一个漫长的过程，这个过程急不得。 其次，我们可以做到以巩固为主，提升为辅——在一段较长的时间段内，尽可能做到不倒退，稳扎稳打，保持并巩固当前来之不易的训练成果。为此，我们需要记录我们做各项锻炼是我们能接受的最大强度，从而做到每次锻炼时心中有数。 最后，还要时不时像扰动一样尝试突破自己能接受的最大强度。每一次的突破，哪怕只能做一两下，都将成为我们下一阶段巩固的目标。 Translation (DeepL.com) Some thoughts on maintaining fitness habits and improving physical fitness in the future In the long road of life ahead, we want to make fitness a pastime that is as deep into our daily lives as walking, running, and hiking and that we are always in the mood to do. Combined with our experiences in junior and senior high school, where running, jumping rope, and seated forward bends became habitual through mandatory gym classes, and we gained a sense of accomplishment through continuous improvement,we can also make the most of our last semester at UBC, and make it a point to go to the gym every time we go to school before we leave - regardless of the length of time. Recently, it has been proven that this kind of \"physical labor\" effectively relieves our mental fatigue and improves our physical fitness after mental work, so it is a double win. Another issue with this is how to make a breakthrough in strength while staying fit. If we have someone to accompany us in our workouts, then this will not be a problem. But what will we do when we work out alone? We might also draw inspiration from our middle and high school years when we practiced running, jumping rope, and seated forward bends alone. First, we must be patient. Progress is bound to be a long process that cannot be rushed. Secondly, we can do this by consolidating and improving - as much as possible over a longer period- without regressing, building steadily, maintaining and consolidating our current hard-won training gains. To do this, we need to keep track of the maximum intensity we are comfortable with each workout so that we can be mindful of what we do each time we work out. Finally, from time to time, we must also try to break through the maximum intensity that we can accept, just like a perturbation. Each breakthrough, even if we can only do one or two reps, will be our goal for the next consolidation phase.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Paper Reading: Asynchronous Functional Reactive Programming for GUIs (The Elm Paper)","slug":"Paper-Reading-Asynchronous-Functional-Reactive-Programming-for-GUIs","date":"2024-01-24T05:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2024/01/24/Paper-Reading-Asynchronous-Functional-Reactive-Programming-for-GUIs/","link":"","permalink":"https://jifengwu2k.github.io/2024/01/24/Paper-Reading-Asynchronous-Functional-Reactive-Programming-for-GUIs/","excerpt":"","text":"NOTE: This is a Paper Reading for the PL Reading Group. The original paper can be found here. Introduction Functional reactive programming (FRP) integrates pure functional programming with time-varying values (signals), useful for GUIs. FRP allows modeling of complex, time-dependent relationships in a declarative style. Previous FRP languages faced inefficiencies, including unnecessary recomputation and global delays. Most FRP languages treat signals as continuously changing, leading to excessive sampling and recomputation. Elm, an FRP language, treats all signals as discrete, reducing unnecessary recomputation by detecting unchanged signals. In Elm, signals change only with discrete events (like user inputs), necessitating program recomputation. Traditional FRP systems process events synchronously, causing delays if event processing is time-consuming. Synchronous processing in GUIs can lead to unresponsiveness during long computations. Elm introduces an abstraction for specifying asynchronous computations within FRP. This feature in Elm allows concurrent execution of long-running computations and other events, maintaining GUI responsiveness. Elm's approach to asynchronous computation in FRP is novel and is formalized in its language semantics. Elm restricts signal use for efficient implementation, similar to previous efficient FRP systems. Core Language The core language of Elm, termed \"FElm\" (Featherweight Elm), is introduced, outlining Elm's key abstractions. FElm is a simply-typed functional language with a set of reactive primitives. It differentiates between simple types (like unit and int, and functions from simple types to simple types) and signal types (like Signal[T] and functions producing signal types). Signals are conceptualized as streams of values, and input signals are required to have a default value. Signal transformations and combinations are achieved using lift_n primitives, which apply functions to the current values of signals. The foldp primitive performs computations on both current and previous signal values, acting like a fold operation on a signal. An example of foldp is counting key presses using a signal that indicates the latest key pressed. FElm's type system prohibits signals of signals to avoid potential computational inefficiencies and inconsistencies. FElm programs evaluate in two stages: functional constructs are first evaluated, resulting in an intermediate term showing signal connections; then signals are evaluated in a push-based manner as new input values arrive. Signal terms are represented as directed acyclic graphs, where nodes represent source nodes, liftn terms, and foldp terms. An event occurs when a source node produces a new value, with a global event dispatcher ensuring total order and non-simultaneity of events. Whenever an event occurs, all source nodes are notified by the global event dispatcher: the one source node relevant to the event produces the new value, and all other source nodes generate a special value noChange v, where v is the current (unchanged) value of the signal. Nodes perform computations on signal values, with synchronous conceptual computation but pipelined execution for efficiency. The async primitive allows for specifying asynchronous computations, enabling separation of long-running computations and maintaining GUI responsiveness. An async node creates a new source node. When an async node produces a value, it is treated like an event from the external environment associated from that new source node. async effectively divides the synchronous graph into a primary subgraph and multiple secondary subgraphs, maintaining event order within each subgraph but not globally, enhancing responsiveness without strict global event ordering. Building GUIs with Elm Elm encourages separation between reactive code and GUI layout code, using a purely functional and declarative approach for graphical layout. Elm supports various base values, including strings, numbers, time, tuples, and graphical values like Elements and Forms. Libraries in Elm offer data structures like options, lists, sets, and dictionaries. Elm provides input support from devices like mouse, keyboard, touch, and also handles window attributes and network communication via HTTP. It supports JSON, Markdown for text creation, let-polymorphism, recursive simple types, type inference, extensible records, and a module system. Elm has two major graphical primitives: elements and forms. Elements are rectangles that can contain text, images, or videos. Forms allow for non-rectangular shapes and text, including arbitrary 2D shapes with texture and color enhancements. Reactive GUIs in Elm interact with user input and environmental events using primitive signals. Elm's signal identifiers include Mouse.position, Mouse.clicks, Window.dimensions, Time.every, Time.fps, Touch.touches, Touch.taps, Keyboard.keysDown, Keyboard.arrows, and Keyboard.shift. Input components like text boxes, buttons, and sliders are represented as pairs of signals: an element for the graphical component and a value for the input. The Input.text function in Elm allows for the creation of text input components, returning a pair of signals for the graphical input field and the current user input. Other Links Understanding the Automaton Time-Travel Debugging (when Elm was still based on FRP) Interactive Programming Elm’s Time Traveling Debugger https://web.archive.org/web/20160504183927/http://elm-lang.org/ Bret Victor style reactive debugging ‒ Laszlo Pandy Bret Victor - Inventing on Principle Reason for a farewell to FRP: learning curve as Elm went mainstream","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"PL Reading Group","slug":"Paper-Reading/PL-Reading-Group","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/PL-Reading-Group/"}],"tags":[]},{"title":"Functional Array Programming","slug":"Functional-Array-Programming","date":"2024-01-04T05:00:00.000Z","updated":"2025-08-13T04:31:51.274Z","comments":true,"path":"2024/01/04/Functional-Array-Programming/","link":"","permalink":"https://jifengwu2k.github.io/2024/01/04/Functional-Array-Programming/","excerpt":"","text":"Functional Programming in R (focus on usability, contains detailed description of removing for loops) http://modern-rstats.eu/functional-programming.html https://appsilon.com/functional-programming-in-r-part-1/ http://adv-r.had.co.nz/Functional-programming.html https://www.stat.umn.edu/geyer/8054/notes/functional.html https://www.reddit.com/r/Rlanguage/comments/vxsf4p/is_r_a_functional_programming_language/ Functional Programming in Python (focus on usability, contains detailed description of removing for loops, broadcasting, persistent ndarrays (JAX) and representing multimodal data using records and trees) https://realpython.com/numpy-array-programming/ https://jax.readthedocs.io/en/latest/jax-101/07-state.html https://data-apis.org/ https://github.com/docarray/docarray https://jax.readthedocs.io/en/latest/pytrees.html Scientific Computing in OCaml (focus on OCaml, comprehensive, not necessarily pure) https://link.springer.com/book/10.1007/978-3-030-97645-3 Functional Array Programming Per Se (focus on theory - rank polymorphism and performance) https://github.com/f5devcentral/shapeRank https://prl.khoury.northeastern.edu/blog/2017/05/04/rank-polymorphism/ https://futhark-lang.org/publications.html Related Topics Arrays vs. Linked Lists in Functional Programming https://www.reddit.com/r/haskell/comments/hvxqzz/is_it_unfunctional_to_use_direct_access_arrays/ Data Access Patterns http://www.nic.uoregon.edu/~khuck/ts/acumem-report/manual_html/ch05s02.html","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"}],"tags":[]},{"title":"Strategies, Tactics, and Mindset Learned from \"The Ph.D. Grind\"","slug":"Strategies-Tactics-and-Mindset-Learned-from-The-Ph-D-Grind","date":"2023-12-31T05:00:00.000Z","updated":"2025-08-13T04:31:00.460Z","comments":true,"path":"2023/12/31/Strategies-Tactics-and-Mindset-Learned-from-The-Ph-D-Grind/","link":"","permalink":"https://jifengwu2k.github.io/2023/12/31/Strategies-Tactics-and-Mindset-Learned-from-The-Ph-D-Grind/","excerpt":"","text":"Note: This is a Paper Reading for Philip Guo's famous book \"The Ph.D. Grind: A Ph.D. Student Memoir.\" Main Strategies Be careful when choosing advisors and collaborators. Consider the background and incentives of the advisors and collaborators, as evidenced by recent papers, grant applications, and future aspirations. Think about what you want to do, what that work is like, and how that aligns with mutual interests. A match in research philosophy does not imply feeling comfortable working together. Be extremely careful working with people with a grind mindset. I found a master's thesis advisor, and like any ambitious student, I began proposing my own partially developed research project ideas to him. My advisor patiently entertained my ideas but ultimately convinced me to focus on more conventional research topics that aligned with both his academic interests and, more importantly, the conditions of his grant funding. Since my master's program tuition was partially covered by a research grant my advisor had obtained from the U.S. government, I was obligated to work on projects within the scope of that grant. Therefore, I followed his recommendations and dedicated two and a half years to developing prototype tools for analyzing the runtime behavior of computer programs written in the C and C++ languages. When I arrived on campus, Dawson was a recently-tenured professor who had been at Stanford for the past eight years; professors usually earn tenure (a lifetime employment guarantee) if they have published enough notable papers in their first seven years on the job. Dawson's main research interest was in building innovative tools that could automatically find bugs (errors in software code) in complex pieces of real-world software. Over the past decade, Dawson and his students built several tools that were able to find far more bugs than any of their competitors. Their research techniques were so effective that they created a successful startup company to sell software bug-finding services based on those techniques. Although I somewhat liked Dawson's projects, what appealed more to me was that his research philosophy matched my own: He was an ardent pragmatist who cared more about achieving compelling results than demonstrating theoretical \"interestingness\" for the sake of appearing scholarly. During my first meeting with Dawson, he seemed vaguely interested in my broader goals of making computer usage and programming more productive. However, he made it very clear that he wanted to recruit new students to work on an automatic bug-finding tool called Klee that his grant money was currently funding. (The tool has had several names, but I will call it \"Klee\" for simplicity.) From talking with other professors and senior Ph.D. students in my department, I realized it was the norm for new students to join an existing grant-funded research project rather than to try creating their own original project right away. I convinced myself that automatically finding software bugs was an indirect way to make programmers more productive, so I decided to join the Klee project. Even though none of my particular ideas managed to persuade Scott, he was still interested in collaborating with me to develop a project aligned with my broader interests. During that period, Scott held the position of an assistant professor, aiming to secure tenure at Stanford, and had been there for just three years. Consequently, he was eager to publish more papers as part of his tenure quest. As I was funded by a fellowship, Scott didn't need to allocate funds from his grants to support me, which made the collaboration appealing to him without any significant downsides. In hindsight, I can now see why this project was likely to face challenges due to misaligned incentives, but at the time, I lacked the wisdom to anticipate such issues. I had decided to become a Klee assistant for Cristi and Dawson because I wanted to join an experienced older Ph.D. student and a professor who had a track record of publishing papers in their specific subfield. This approach had worked exceptionally well the previous year when I collaborated with Joel, an older Ph.D. student, and Scott, a professor, on their HCI project, which resulted in a top-tier, award-nominated paper. So, what was different in this case? In short, neither Cristi nor Dawson had a strong urge to publish. They had already authored several Klee papers together, and a cross-checking paper co-authored with me would have been a \"nice-to-have\" but not an obligatory follow-up publication. Cristi was in the final year of his Ph.D. and didn't require further papers to graduate, while Dawson had already secured tenure and wasn't in a hurry to publish more. In contrast, Joel was a mid-stage Ph.D. student eager to publish the first paper of his dissertation, and Scott was an assistant professor who needed to publish prolifically to earn tenure. These two contrasting experiences taught me the crucial importance of thoroughly understanding the motivations and incentives of potential collaborators before embarking on a project with them. I believe that Dawson expected Peter and me to achieve publishable results at a faster pace, which may have led him to perceive us as either incompetent or not fully committed to our work. It's a harsh reality that, as a professor at a top-tier university, Dawson's students are likely less accomplished than he was during his own Ph.D. years. The explanation is quite straightforward: Only about 1 out of every 75 Ph.D. students from a top-tier university typically possesses the qualities necessary to become a professor at an institution like Stanford (or perhaps 1 out of every 200 Ph.D. students from an average university). Predictably, neither Peter nor I met those exceptional standards. If Dawson had partnered with a younger version of himself, progress may have been different. Two years after Peter and I departed from the Klee project, Dawson eventually found a new Ph.D. student who could successfully bring his Klee-UC vision to fruition. In 2011, Dawson and his new student published a significant paper that incorporated both Klee-UC and cross-checking ideas. Ultimately, it took three attempts involving four different Ph.D. students over five years before Dawson's original Klee-UC concept materialized into a published paper. Of those four students, only one persevered— I left the Klee project, and two others decided to exit the Ph.D. program altogether. From an individual student's standpoint, the chances of success appeared rather low. From a professor's perspective, however, Klee-UC represented a resounding success. Since Dawson held tenure, his job was never at risk. In fact, one of the purposes of tenure is to enable professors to take risks by pursuing more ambitious project ideas. However, the downside of this privilege is that professors often assign students to work on these risky projects, which may have lower success rates. Students often find it challenging to decline such assignments, especially if they are financially supported by their advisors' grants. Fortunately, as I was funded by fellowships, it was considerably easier for me to discontinue my involvement in the Klee project. Tom's extensive experience in publishing and reviewing numerous empirical software measurement papers made him an invaluable \"insider\" who understood what types of results and paper writing were well-received by reviewers in that specific subfield. When it came time to submit our paper at the end of that summer, Tom skillfully positioned our contributions within the context of related work, presented compelling arguments for the novelty and significance of our findings, and meticulously polished our paper. Three months later, I was thrilled to receive the news that our paper, which focused on studying the causes of bug fixes, had been accepted at a top-tier conference. This was particularly impressive given that only 14 percent of all papers submitted that year were accepted. However, Tom's dedication didn't stop there. As a newly-hired researcher at MSR, he was motivated to build his reputation by publishing additional papers. In the following years, we leveraged the results from my summer 2009 internship to write two more top-tier conference papers. One of these papers explored bug report reassignments, while the other delved into bug report reopenings and even earned a Best Paper Award. During my first month in the new phase of my academic journey, I primarily spent my time reconnecting with old college friends, as my alma mater, MIT, was conveniently located near Harvard. Additionally, I had several meetings with Margo to explore potential research ideas. Margo was open to the idea of me pursuing my own project under her loose supervision, granting me a considerable degree of intellectual freedom. However, I approached my brainstorming process pragmatically because I aimed to generate a project that would genuinely excite her and secure her strong support for its inclusion in my dissertation. To achieve this, I delved into her recent papers and grant applications to gain insight into her research philosophy. I tailored my ideas to align with her preferences, recognizing the importance of harmonizing with the subjective inclinations of senior collaborators, as well as the expectations of paper reviewers, even within fields that are considered technically objective. At that time, reading a grant proposal was an entirely novel experience for me, and it appeared foreign and unfamiliar. However, with time and practice, I have since become accustomed to writing grants, and it has become a routine part of my academic life. Apply to fellowships before starting the Ph.D. for better academic freedom and advisor-advisee relationship. I was also fortunate to receive two prestigious fellowships, the NSF and NDSEG graduate research fellowships. These fellowships were granted to only about five percent of all applicants. They covered the full expenses for five out of the six years of my Ph.D. studies and relieved me from the obligations of working on specific grant-funded projects. This was a significant advantage over students who had to work on such projects throughout their college years. Applying to Ph.D. programs and fellowships during my master's year gave me a huge advantage over students who applied during senior year of college, since I had an extra year of research experience. In other fields, such as the humanities and social sciences, students typically do not receive direct funding from their advisors. This distinction significantly changes the dynamics of the advisor-advisee relationship, turning the Ph.D. experience into more of a solitary journey and less of an employer-employee arrangement. However, I soon came to the realization that I wasn't obligated to remain tethered to Klee in any way, given that my funding came from the NDSEG fellowship rather than Dawson's grants. In contrast, all of Dawson's other students had no option but to persist with their work on Klee, as they were supported by his Klee-related grants. Therefore, I retained Dawson as my advisor but departed from the Klee project, embarking on the journey to create my own research project entirely from scratch. From a professor's perspective, however, Klee-UC represented a resounding success. Since Dawson held tenure, his job was never at risk. In fact, one of the purposes of tenure is to enable professors to take risks by pursuing more ambitious project ideas. However, the downside of this privilege is that professors often assign students to work on these risky projects, which may have lower success rates. Students often find it challenging to decline such assignments, especially if they are financially supported by their advisors' grants. Fortunately, as I was funded by fellowships, it was considerably easier for me to discontinue my involvement in the Klee project. Question each step in the decision-making process. Have an outline or draft of the research paper, especially the evaluation section, before starting a research project. This brings the obvious benefit of making research contributions. It also forces everyone not to push you around, helping you to jump out of the \"pecking order\" slowly and surely. Dawson believed that Klee could uncover new bugs that no automated tool or human being had previously discovered within the code of thousands of Linux device drivers. I recall thinking that while finding new Linux device driver bugs could be interesting to present in a paper, it wasn't entirely clear to me how these results constituted a substantial research contribution. To my understanding, my role was to use Klee to uncover new bugs, essentially applying existing research, rather than significantly enhancing Klee in an innovative manner. Moreover, I couldn't envision how my project would seamlessly integrate with the projects of the other five students for a coherent paper submission in March. Nevertheless, I had faith in Dawson's high-level paper writing strategy. Since I had recently joined the project, I didn't want to immediately question these decisions typically made by professors. I was assigned a specific task, and I was determined to carry it out to the best of my abilities. My rational understanding acknowledged that experimental research in science and engineering fields often demands an extensive amount of unglamorous and labor-intensive work to produce tangible results. Ph.D. students, particularly those in their first and second years, are typically the ones tasked with undertaking the most tedious tasks—this is essentially what we are compensated for. In a typical research group, the professor and senior Ph.D. students formulate the high-level project plans and then delegate the responsibility of making all the intricate details function in practice to the junior students. First- and second-year students usually have minimal influence on the overall direction of the group's project. Although I fully embraced my position as the lowest-ranking member of the team, my emotional state still suffered significantly during those initial months because the work was exceptionally challenging and lacked immediate rewards. I met with Dawson to express my frustration regarding the overwhelming task I was currently tackling. It felt ludicrous to spend several days configuring Klee for each new device driver. Not only was it physically exhausting, but it also didn't seem like genuine research. What could we possibly write in our paper? That I had devoted nearly 1,000 hours to manual labor in getting Klee to function with device drivers without gaining any meaningful insights? It didn't feel like a valuable research contribution; it seemed rather futile Additionally, panic set in as there were only five weeks left until the paper submission deadline, and Dawson had yet to discuss our group's paper writing strategy. Typically, writing a respectable paper submission takes a minimum of four weeks, especially when coordinating efforts among six students involved in the project. However, a significant problem arose. When we finally achieved those favorable results, there were only three days left until the paper submission deadline, and not a single word of the paper had been written yet. In such a short timeframe, it was physically impossible to write, edit, and refine a paper submission that had any chance of being accepted at a top-tier computer science conference. Nevertheless, we decided to give it our best shot. During the final 72 hours leading up to the deadline, Dawson and five of us students (one had dropped out of the project by this point) practically lived in the office, pulling two consecutive all-nighters to wrap up the experiments and draft the paper. Deep down, all of us students realized that there was virtually no chance that this paper would be accepted, but we followed Dawson's lead and pressed on. The result was a submission that can only be described as a disorganized mess – it contained numerous typos, nonsensical sentence fragments, graphics lacking explanations, and lacked concluding paragraphs. It was a dismal sight. At that moment, I couldn't fathom how I would ever complete a Ph.D. if it meant working in such a chaotic and haphazard manner. As anticipated, three months later, the reviews for our paper were overwhelmingly negative, filled with harsh comments like, \"The program committee believes that this paper is far too sloppily prepared to warrant acceptance; please refrain from submitting papers that are clearly unready for review.\" My friend Greg, who was one of Rob's Ph.D. students, emphasized the significance of the third point: thinking about experiments when suggesting research project ideas. Professors are often driven by the desire to have their names associated with published papers, and in the field of computer science, conference papers typically require robust experiments to secure acceptance for publication. Therefore, it's essential to consider experiment design right from the outset when formulating project proposals. Target fellow researchers with similar incentives when conducting HCI studies in academia. Without exceptionally strong resources, find a novel, meaningful niche for research instead of an overcrowded and highly competitive domain. Do not attempt to do in academia what should be done in industry. In hindsight, I'm not astonished that my efforts to shadow professionals in their workplaces were unsuccessful. I had nothing to contribute to these seasoned programmers; my presence would have likely disrupted their workday. Fortunately, a few years later, I had the opportunity to observe a different group of programmers—fellow graduate students engaged in programming for scientific research. They were open to my occasional inquiries and more than willing to discuss their working environments. These interviews would ultimately serve as a direct source of inspiration for my dissertation work. Dawson and I encountered significant challenges in getting our research results published. Over the course of a year, we submitted two papers that were both rejected. It would take another full year before our work was finally published as a shorter-length paper in a second-tier conference, which held minimal prestige and did not count as a contribution to my dissertation. However, by that point, I had already moved on to other projects. The primary reason behind our struggles with publication was that we were not considered \"insiders\" in the empirical software measurement subfield (sometimes referred to as empirical software engineering), to which our project belonged. When Dawson and I embarked on this work, numerous research teams from various universities and corporate research labs were already engaged in similar endeavors. We were clearly outmatched by the competition, which included professors and research scientists specializing in empirical software measurement, guiding armies of Ph.D. students through the extensive data analysis. These individuals were eager to publish a multitude of papers, especially young professors aspiring to attain tenure. They possessed expertise in statistical methodologies, framing related work, and crafting persuasive narratives needed to secure acceptance for such papers. Most significantly, they frequently served on program committees and acted as external reviewers for relevant conferences, which provided them with in-depth knowledge of the requisites for producing publishable papers in this subfield. One significant advantage of being an intern at MSR was access to a wealth of internal data sets containing information about Microsoft's software bugs and personnel files. These confidential data sets would have been inaccessible to me as an external researcher. The richness of these Microsoft data sets provided MSR researchers like Tom with a distinct advantage, making it easier to obtain groundbreaking and publishable results compared to competitors who lacked access to such data. In contrast, when I worked with Dawson, the Linux data sets I had access to were smaller and of lower quality. Open-source software projects typically do not maintain records as meticulously as one of the world's largest software companies. This limitation is something that all university researchers face unless they establish partnerships with companies that can provide them with access to relevant data. Upon returning to Stanford in the fall of 2009, inspired by my previous HCI work with Scott and Joel during my second year, I embarked on a project to interview colleagues who used Python for data analysis in their research. The objective was to identify the programming-related inefficiencies they faced and explore how IncPy could address and eliminate these inefficiencies. I also leveraged my connections to give presentations about IncPy, even though it was still a half-baked idea at that stage, at various lab group meetings. These early efforts helped generate fresh ideas and refine the project's \"marketing pitch.\" I'm deeply thankful for the friends who supported me in kickstarting my project when I had little more than a few rudimentary PowerPoint slides. As I continued my interviews and refined my design plans, I grew increasingly optimistic. I discovered that researchers in various computation-based fields, including machine learning, pharmacology, bioengineering, bioinformatics, neuroscience, and ocean engineering, all faced similar challenges in their data analysis workflows, making them potential beneficiaries of IncPy. After a few weeks of interviews and subsequent adjustments to my project's direction, I felt confident that I could convincingly pitch the idea in a future paper submission. The core argument I aimed to convey was that many computational researchers across diverse fields grappled with common inefficiencies in their daily programming tasks, and IncPy presented a novel, fully automated solution to these inefficiencies that had not been previously implemented. This initial pitch would ultimately become the central theme of my entire dissertation. Climb the shoulders of giants as much as possible and pick the low-hanging fruit from there before you become a giant. Following the creation of Klee and related projects between 2005 and 2008, a new subfield emerged. This development led to numerous assistant professors and young research scientists eagerly producing a plethora of papers, each detailing incremental improvements in their quest to secure tenure or job promotions. It was akin to an academic gold rush, spurred by the early insights of Cristi, Dawson, and a select few pioneers. Since Dawson already possessed tenure and had gained fame for his contributions, he was above the fray and lacked the desire to publish solely for the purpose of bolstering his academic resume. In practice, Ph.D. students collaborating with these young researchers found it comparatively easier to publish their work and complete their graduate programs, while Dawson's students faced considerably more challenges. Over the three years since I departed from the Klee project, research groups worldwide have collectively published hundreds of papers grounded in Klee-like concepts. Remarkably, fifteen of these papers detailed enhancements to Klee itself, as our laboratory released it as open-source software to encourage further research. In the meantime, five of Dawson's Ph.D. students have made serious efforts to work on Klee; however, only one has managed to publish a single paper on Klee-UC. Actively expand your network and seek collaboration opportunities. Just before commencing my second year of the Ph.D. program in September 2007, I took a one-week vacation to Boston to visit friends from college. While in the area, I reached out to a few MIT professors I knew from my undergraduate years, seeking their guidance. During our meetings, they all conveyed a similar message: Take the initiative to engage with professors, explore research topics of mutual interest, and above all, avoid isolation. This straightforward advice, consistently applied over the next five years, ultimately paved the way for a successful completion of my Ph.D. journey. I wasted no time in taking this advice to heart while still in Boston. I sent a cold email to an MIT computer science professor named Rob, politely requesting a meeting with him. In this initial email, I briefly introduced myself as a recent MIT graduate and a current Stanford Ph.D. student with a keen interest in developing tools to enhance the productivity of computer programmers. Given that I knew Rob shared an interest in this research area, I hoped my email would pique his interest rather than end up in his spam folder. Fortunately, Rob generously agreed to meet with me for an hour in his office, during which I presented a few project proposals and sought his feedback. He appeared to find merit in my ideas, which bolstered my confidence that they held promise in the eyes of a professor working in this research domain. Regrettably, I couldn't collaborate with Rob as I was no longer an MIT student. Nonetheless, at the conclusion of our meeting, Rob suggested that I approach a Stanford computer science professor named Scott to see if I could garner his interest in my ideas. The lasting impact of an MSR (Microsoft Research) internship often extends beyond research achievements to the friendships forged during the experience. During that particular summer, I had the privilege of forming connections with some of the brightest and most inspiring young computer science researchers of my generation. For example, one of my three office mates was on the verge of beginning her Ph.D. journey at MIT and had already published more top-tier papers during her undergraduate research than most Ph.D. students could ever aspire to. Another office mate was a UC Berkeley Ph.D. student who dedicated his nights and weekends to a separate research project with collaborators from across the country, all while diligently working on his internship project during workdays. These peers are likely to evolve into award-winning professors, research leaders, and high-tech entrepreneurs, and I am genuinely humbled to have had the opportunity to share a summer with them. This is how we can evaluate productivity claims. From the very beginning of my IncPy project, I recognized the challenge of presenting a compelling evaluation. The core premise, that IncPy could enhance the productivity of computational researchers, was inherently subjective. To address this, after studying similar papers, I developed a two-pronged evaluation approach: Case Studies: I planned to gather a variety of Python programs from computational researchers and simulate the productivity gains they might have achieved using IncPy instead of standard Python. Deployment: The goal was to encourage researchers to incorporate IncPy into their regular work, allowing them to directly experience and report on its impact on their productivity. In pursuit of this, I adopted the roles of both salesman and beggar, persistently seeking Python programs from colleagues for my case studies and encouraging them to use IncPy in their research. Despite mostly receiving negative responses, I continued asking for referrals and volunteered to speak at various lab meetings to generate interest in IncPy. After months of effort, I managed to acquire Python programs from researchers across various fields, sufficient for starting my case studies. As I concluded my fourth Ph.D. year in September 2010, I submitted my IncPy paper to a top-tier conference. The paper included case studies and a few deployment anecdotes. Aware of the low acceptance rate and the unconventional nature of IncPy within academic fields, I was prepared for potential rejection but still aimed high, knowing the value of a top-tier publication for my graduation prospects. The runtime is a vital (yet often overlooked) aspect to consider in programming language research. On July 29, 2010, a year after the initial concept of IncPy was born, I was struck by another idea, this time addressing a common issue in computational research. I noticed that researchers often write their computer programs in an ad-hoc, somewhat careless manner, leading to frequent crashes for trivial reasons. These crashes not only prevent the production of any results but also cause considerable frustration. My realization was that by modifying the runtime environment of the Python programming language, specifically the interpreter, I could address this issue. The idea was to adapt the Python interpreter in a way that would allow these less rigorously written programs to still run and produce partial results, rather than failing completely and producing none. I decided to name this modified version of the Python interpreter \"SlopPy,\" a playful blend of 'Sloppy' and 'Python', emphasizing its tolerance for less meticulous coding practices. This concept aimed to make the process of data analysis more forgiving and efficient for researchers who may not always adhere to stringent coding standards. Focus on acknowledging shortcomings and deriving their insights if experimental evaluation results are suboptimal. While I was interning at Google during the summer of 2011, I received the joyful news that our ProWrangler paper had been accepted with outstanding reviews. The main factor contributing to our success was Jeff's exceptional work in crafting both the introduction of our paper and the interpretation of our evaluation results. Initially, our user testing had not demonstrated the productivity improvements we had hoped for, which made me concerned that our paper might face rejection. However, Jeff's skill in technical writing and framing our arguments skillfully transformed what seemed like impending failure into a surprising victory. The reviewers appreciated our candid acknowledgment of the shortcomings in our evaluation and the valuable insights we extracted from them. Undoubtedly, our paper would not have gained acceptance without Jeff's rhetorical expertise. He had accumulated substantial experience in this area, having published 19 papers during his Ph.D. studies, mostly in top-tier conferences, which is five to ten times more than what is typically expected from computer science Ph.D. students. This level of dedication and productivity is often necessary to secure a faculty position at a prestigious university like Stanford. In fiercely competitive domains, do not self-consciously try to network and seek opportunities, and never schmooze. Instead, rely on bypasses and side roads. Many things grow in the garden that were never sown there（有心栽花花不开，无心插柳柳成荫）. When I made the decision to leave academia, one of the immediate impacts was that I no longer felt the need to engage in networking activities at the three academic conferences I attended that summer. These conferences included talks I gave on IncPy, SlopPy, and CDE. Academic conferences are typically filled with senior Ph.D. students, postdocs, and pre-tenure professors who are actively networking to impress their more senior colleagues. For these junior researchers, professional networking at conferences is a crucial and time-consuming task as it greatly influences their budding careers and academic reputations. However, since I had decided to step away from the academic world, I found myself enjoying the conferences without the usual nervousness or strategic calculations. At one of these conferences, I had a casual conversation with John, a professor from the University of Utah who was the keynote speaker. Later on, he wrote a generous blog post that significantly increased the popularity of \"The Ph.D. Grind\" among professors. He also provided me with practical advice regarding the possibility of returning to academia myself. This unexpected turn of events occurred because I was no longer driven by a desire to stay in academia and simply chatted with people I found interesting at the conference without a specific networking agenda. It was a reminder of how unpredictable life can be. During a break between sessions at another conference, I noticed Margo sitting alone and working on her laptop. I had previously met Margo during my fourth year at a San Jose workshop where I presented my original IncPy paper. Although I had some reservations about approaching her and reintroducing myself, fearing that she might not remember me or that our conversation might lack substance, my impending departure from academia meant I had no real networking agenda to uphold. I decided to take the chance and greeted her. I reminded her of our previous encounter, and she appeared to recall me. We had a brief five-minute conversation about my new CDE project before I had to rush off to give my talk. After returning home, I sent her a courteous follow-up email with a link to the CDE project webpage, in case her students were interested in using it for their research. This was my standard polite approach when introducing CDE to professional colleagues, and I didn't have high expectations for follow-up. As it turned out, Margo would later play a pivotal role in helping me secure a professorship. Ironically, if I had initially desired a professorship, I might have been too self-conscious and hesitant to approach her in the first place, which could have diminished my chances of eventually obtaining the position. In the end, it was because I didn't actively seek a professorship at the time that I ultimately ended up getting one. Life has its own unique way of working things out! The culmination of my graduate school journey wouldn't have been possible if I hadn't actively seized the opportunities that I was fortunate enough to receive. If Robert hadn't informed me about the San Jose workshop two years ago, if I hadn't submitted and presented my IncPy paper there, if Margo hadn't taken an interest in my paper and introduced me to Elaine, if I hadn't maintained contact with Elaine, if I hadn't spontaneously approached Margo again at last summer's conference where I presented CDE, if she hadn't sent me a gracious follow-up email, and if I hadn't taken a risk with my unconventional counterproposal to her, then I would have still been at Stanford struggling to find one last project and thesis committee member. It's important to acknowledge that achieving this outcome required me to try many different approaches like this, and most of them did not yield the desired results. Success often involves numerous attempts and failures before finding the right path. Supportive Tactics Optimize the process of note-taking for research. My daily routine primarily revolved around the development of computer programs designed to extract, clean, reformat, and analyze data from the Linux revision control history and the 2,000 bug reports. In my pursuit of gaining insights, I independently acquired a foundational understanding of quantitative data analysis, statistics, and data visualization techniques. Throughout my work, I meticulously recorded my experimental progress in a research lab notebook, carefully documenting which trials were successful and which were not. Every week or so, I would meet with Dawson to present my findings. Typically, these meetings involved me presenting him with printouts of graphs or data tables generated through my analyses, followed by him offering high-level suggestions, such as, \"This part of the graph appears unusual; can you explain why? Try breaking down the data in this manner and delve deeper.\" It was only years later that I discovered this working style was relatively common among computational researchers in various academic disciplines. For my dissertation, I went on to develop tools aimed at streamlining the typical inefficiencies in this prevalent workflow. However, at that time, I had no such long-term vision; my primary goal was to make intriguing discoveries and have them published. Raise encountered problems and frustrations, no matter how minor, at meetings and complain. Actively contact people for feedback, motivation, and emotional support. I found myself navigating unfamiliar territory, making it significantly more challenging to seek assistance compared to my undergraduate years when solutions were more straightforward. As the sole person working with Klee on device driver code, my colleagues couldn't offer much guidance. While Dawson occasionally provided high-level strategic advice, as is customary for tenured professors, his role didn't involve being directly involved in the day-to-day challenges we faced. It fell upon us, the students, to decipher all the intricate details necessary to yield results—my task being to uncover new bugs in Linux device drivers that had not been previously identified. Professors often reiterate the mantra, \"If it's already been done before, then it wouldn't be research!\" For the first time, I truly grasped the essence of those words. For the following ten weeks, I found myself daydreaming about my own research concepts in complete isolation, without engaging in any conversations with others. Given my negative initial experience working in a research group over the past few months, I craved solitude to think independently. Dawson was supportive of my absence since he wasn't financially supporting me through his grants. I lived in complete seclusion, mentally drained but still attempting to make gradual progress. Each day, I dedicated myself to reading numerous computer science research papers and taking notes in the hope of finding inspiration for my own creative ideas. However, lacking proper guidance or context, I often ended up squandering a lot of time without gaining meaningful insights from my readings. I also roamed aimlessly on my bicycle through the neighborhoods around campus, hoping to spark new research ideas to no avail. Most notably, I procrastinated more than I ever had in my life up to that point: I watched countless TV shows, took numerous naps, and wasted countless hours online. Unlike my friends with conventional nine-to-five jobs, there was no supervisor monitoring my daily activities, so I allowed my mind to wander without any structure in my life. During those ten solitary weeks, I scarcely spoke to anyone, not even my friends or family. Complaining seemed futile because it felt like nobody could truly grasp what I was going through at the time. My friends who were not pursuing Ph.D. programs believed I was simply \"in school\" and taking classes like a typical student. Meanwhile, the few friends I had made in my department were grappling with their own first-year Ph.D. struggles, primarily the shock of diving headfirst into complex, open-ended research problems without the ability to influence the overarching direction of their assigned projects. We, as young computer scientists, willingly engaged in tasks that were both exceptionally challenging and seemingly devoid of purpose, all while earning a quarter of the salary of our friends in the corporate world. It was so disheartening that it became almost comically tragic. However, I didn't believe that group complaining would be productive, so I chose to remain silent. I avoided the Computer Science Department building, fearing encounters with colleagues who might inevitably inquire about my work, and I had no respectable response to offer. Instead, I preferred secluding myself in libraries and coffee shops. In hindsight, going solo so early in my graduate school journey was a regrettable decision. Contrary to romanticized notions of a solitary scholar sitting outdoors, sipping a latte, and scribbling on blank notebook pages, real research is never conducted in isolation. It necessitates a solid foundation in intellectual, historical, and sometimes even physical aspects (such as laboratory equipment) to develop innovative ideas. A wiser approach during those weeks would have been to communicate with Dawson more frequently and actively seek collaborations with other professors or senior students. However, at that time, I was so burnt out and frustrated with the traditional hierarchy of group-based research – which often involved new Ph.D. students performing the most unglamorous tasks – that I retreated and embarked on my own path. By the middle of my third year, many of my fellow students and I found ourselves in a state of \"limbo.\" It became increasingly challenging to muster the motivation to come into the office day in and day out. We also grappled with feelings of isolation and loneliness, as we spent our days and nights immersed in tackling obscure, highly specialized problems that few people in our immediate surroundings comprehended or showed interest in. While our advisors and senior colleagues occasionally offered high-level guidance, they seldom sat down with us to work through the intricate details of our research endeavors. If there is work-life balance in a job and stress levels are not too high, i.e., one is not drained or overburned, would it be a viable opportunity to pursue self-improvement and open doors for future pursuits? Of course, it would be foolish to pursue a Ph.D. solely out of irrational childhood fears. To get a preview of corporate working life, I did internships at engineering companies every summer during college. Since I happened to work in offices where I was the only intern, I was given the full responsibilities of a junior engineer, which was a rare privilege. Although I learned a lot of technical skills, I found the day-to-day work to be mind-numbingly dull. My coworkers were also unenthusiastic about their jobs, and there were few appealing prospects for career advancement. Of course, I'm not claiming that all engineering jobs are mind-numbingly dull; it just happened that the companies I worked for were not first-rate. Many of my college friends who interned at first-rate companies such as Microsoft had great experiences. Ironically, my first full-time job after finishing my Ph.D. was at Google. Google loved their experiences and signed on to work at those companies full-time after graduation. Research software tends to be rough prototypes. Keep them simple, stupid. Similar to other sophisticated software tools, Klee featured numerous configurable options. However, since it was a research prototype assembled by students, most of these options lacked clear documentation regarding their behaviors. Consequently, I spent a significant amount of time grappling with misunderstandings about the subtle interactions between these options as I adjusted them. My research lab notebook became filled with frustrations, including entries like: \"OH SH*T, I believe my mistake was failing to realize that there are specific options meant to be passed into Klee (e.g., -emit-all-errors), and others that should be passed into the target program to set up the model environment (e.g., --sym-args). When these get confused, bizarre outcomes occur because Klee ends up executing the target program with argc and argv values that differ from what you'd expect.\" From a research perspective, my goal was achieved: I successfully developed an initial prototype of CDE and demonstrated its functionality in a practical use case. In many applied engineering fields, it's widely accepted that research prototypes like CDE primarily serve to prove the feasibility of innovative concepts. The role of a researcher involves creating prototypes, conducting experimental assessments to measure their effectiveness, publishing research papers, and then moving on to the next idea. It would be unrealistic for a researcher to expect people to use their prototypes as if they were fully-fledged products. Instead, if the ideas are promising, professional engineers may incorporate them into their company's future products. At best, some other research groups might utilize your prototypes as a foundation to develop their own prototypes and subsequently reference your work in their papers (for example, more than a dozen university research groups have expanded upon the Klee tool and published papers detailing their enhancements). However, it is exceedingly rare for individuals outside of the research community to employ research prototypes in their daily tasks. In essence, the objective of academic research is to generate validated ideas, not refined products. Make full use of workshops to disseminate preliminary results and collect feedback rapidly, even if it involves spending money yourself. Typically, I wouldn't have paid much attention to such an announcement for two reasons. Firstly, Robert's research area, data provenance, had no direct relevance to IncPy, so his paper submissions didn't affect me. Secondly, in our department, workshop papers don't usually count as substantial contributions toward a dissertation. Workshops are primarily meant for sharing early-stage ideas and tend to have much higher acceptance rates (60 to 80 percent) compared to conferences (8 to 30 percent). Many professors prefer their students to focus on publishing in conferences since workshops require them to cover travel, hotel, and registration costs, similar to conferences, but without the same level of prestige. Consequently, top-tier computer science professors often encourage their students to prioritize conference papers over workshop submissions. While presenting my IncPy workshop paper was beneficial for feedback and networking, particularly with Margo, it didn't qualify as an official publication for my dissertation. I was aware that publishing this work at a recognized conference in my department was necessary. The main difference between a workshop and a conference paper lies in the requirement for a conference paper to have a robust experimental evaluation, demonstrating the effectiveness of the proposed tool or technique. The evaluation part of a paper can vary, from measuring runtime performance to conducting user behavior studies in a controlled environment. Given the commonality of similar research ideas, reviewers pay close attention to the implementation and experimental analysis of these ideas when deciding on the acceptance or rejection of papers. Mindset Find a meaning for your research and build a reputation. After two months of persistent effort, I began to achieve some modest victories. I managed to get Klee to function well enough to identify my first few bugs in the smallest device drivers. To ascertain whether these bugs were genuine (as opposed to false positives resulting from Klee's limitations), I sent emails outlining each potential bug to the Linux developers responsible for those drivers. Several driver creators verified that I had indeed discovered real bugs in their code. These email confirmations brought me great excitement, as they represented my initial glimpses of external validation. The skill of crafting concise and impactful professional emails has proven to be highly beneficial for my career. Do not bootlick the \"establishment\" or the \"mainstream.\" Be wise and brave, identify emergent trends, conquer virgin land, and blaze unheard-of new paths. The second and more significant reason why pursuing a postdoc didn't make sense for me was the nature of the research topics I was truly passionate about. These topics didn't align well with the likelihood of winning grant funding because they weren't widely accepted by the current academic establishment. Without grants, it would be impossible to fund students to work on these topics, and without motivated students to tackle the challenging manual work involved, it would be difficult to produce reputable publications. Furthermore, without a substantial number of publications each year, achieving tenure would be out of reach. Even if I did manage to secure tenure, I would still require new grants to support new students in implementing my ideas, perpetuating an ongoing funding cycle. Given my research interests, I wasn't emotionally prepared to engage in the uphill battles necessary to have my proposals taken seriously by grant funding agencies. Convincing peer reviewers to accept my papers had already been a challenge, and grant reviewers would likely be even less sympathetic, as they control the distribution of significant financial resources and would prefer to allocate funding to colleagues conducting more mainstream computer science research. When I began my faculty career in 2014, this was my greatest fear. Consequently, I deliberately transitioned to a new research area that had greater potential for securing funding. Out of the 26 Ph.D. graduates in the Stanford Computer Science Department from my year, I considered myself relatively average from an academic perspective, as most of my papers were second-tier and not well-received by the academic community. My dissertation work straddled multiple computer science subfields, including Programming Languages, Human-Computer Interaction, and Operating Systems, which made it challenging to gain recognition from top experts in any one specific subfield. Given that my dissertation topic was far from mainstream, any junior professor or scientist attempting to build their academic career around its concepts would face difficulties gaining the approval of grant funding agencies, crucial for launching new projects, and the support of senior colleagues, essential for publication and tenure. While I am more than willing to support anyone willing to take on this commendable challenge, I wasn't courageous enough to risk my own career on it. Instead, I have chosen to pursue an entirely different professional passion, which may become the subject of a future book. Interestingly, these ideas eventually became more fundable in the year after I graduated, thanks to the emergence of the Big Data and data science movements. However, by that time, I had already shifted my focus to other interests.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"}],"tags":[]},{"title":"Linear Regression, Ridge Regression, Lasso Regression, and Kernel Ridge Regression","slug":"Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression","date":"2023-12-24T05:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2023/12/24/Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression/","link":"","permalink":"https://jifengwu2k.github.io/2023/12/24/Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression/","excerpt":"","text":"Linear Regression Linear Regression Linear regression is a fundamental statistical model used in statistics and supervised machine learning. It establishes a linear relationship between a scalar response and one or more explanatory variables. The simplicity and well-established properties of linear regression make it a cornerstone algorithm in machine learning. Historically, linear regression was developed by Legendre (1805) and Gauss (1809) for astronomical predictions and later popularized in the social sciences by Quetelet. Linear regression is widely used for two primary purposes: For predictive modeling, it fits a model to observed data sets, allowing for future predictions when new explanatory variables are available without their corresponding response values. For analysis, it helps quantify the relationship between response and explanatory variables, assessing the strength of this relationship and identifying variables with no linear relationship or redundant information. In its most general case, a linear regression model can be written in matrix notation as \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\] where \\(\\mathbf{y} = {\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}}\\) is a vector of \\(n\\) observed values of the response variable. \\(\\mathbf{X} = {\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\\\ \\mathbf{x}_{2}^{\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\end{bmatrix}} = {\\begin{bmatrix} 1 &amp; x_{1, 1} &amp; \\cdots &amp; x_{1, p} \\\\ 1 &amp; x_{2, 1} &amp; \\cdots &amp; x_{2, p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n, 1} &amp; \\cdots &amp; x_{n, p} \\end{bmatrix}}\\) is a matrix of \\(n\\) observed \\((p + 1)\\)-dimensional row-vectors of the explanatory variables. \\(\\boldsymbol{\\beta} = {\\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\beta_{2} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix}}\\) is a \\((p + 1)\\)-dimensional parameter vector, whose elements, multiplied with each dimension of the explanatory variables, are known as effects or regression coefficients. \\(\\boldsymbol{\\varepsilon}={\\begin{bmatrix}\\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon _{n} \\end{bmatrix}}\\) is a vector of \\(n\\) error terms. It captures all other factors that influence \\(\\mathbf{y}\\) other than \\(\\mathbf{X}\\). Note that the first dimension of the explanatory variables is the constant 1. This is designed such that the corresponding first element of \\(\\boldsymbol{\\beta}\\), \\(\\beta_{0}\\), would be the intercept after matrix multiplication. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero. Fitting a linear model to a given data set usually requires estimating \\(\\boldsymbol{\\beta}\\) such that \\(\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\) is minimized. For example, it is common to use the sum of squared errors (known as ordinary least squares) \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) as a loss function for minimization. This minimization problem has a unique solution, \\({\\hat{\\boldsymbol{\\beta}}} = (\\mathbf{X}^{\\operatorname{T}} \\mathbf{X})^{-1}\\mathbf{X}^{\\operatorname{T}} \\mathbf{y}\\). References: https://en.wikipedia.org/wiki/Linear_regression https://en.wikipedia.org/wiki/Ordinary_least_squares References: https://en.wikipedia.org/wiki/Linear_regression https://en.wikipedia.org/wiki/Ordinary_least_squares Ridge Regression However, when linear regression models have some multicollinear (highly correlated) dimensions of the explanatory variables, which commonly occurs in models with high-dimensional explanatory variables, \\(\\mathbf{X}^{\\operatorname{T}} \\mathbf{X}\\) approaches a singular matrix and calculating \\(\\left(\\mathbf{X}^{\\operatorname{T}} \\mathbf{X} \\right)^{-1}\\) becomes numerically unstable (note how the magnitude of np.linalg.inv(X.T @ X) changes as the columns of X become more and more correlated below): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; for x_21 in [2.1, 2.01, 2.001, 2.0001, 2.00001]:... X = np.array([... [1., 2.],... [1., x_21]... ])... ... print(&#x27;X:&#x27;)... print(X)... ... print(&#x27;X.T @ X:&#x27;)... print(X.T @ X)... ... print(&#x27;np.linalg.inv(X.T @ X):&#x27;)... print(np.linalg.inv(X.T @ X))... X:[[1. 2. ] [1. 2.1]]X.T @ X:[[2. 4.1 ] [4.1 8.41]]np.linalg.inv(X.T @ X):[[ 841. -410.] [-410. 200.]]X:[[1. 2. ] [1. 2.01]]X.T @ X:[[2. 4.01 ] [4.01 8.0401]]np.linalg.inv(X.T @ X):[[ 80401.00000048 -40100.00000024] [-40100.00000024 20000.00000012]]X:[[1. 2. ] [1. 2.001]]X.T @ X:[[2. 4.001 ] [4.001 8.004001]]np.linalg.inv(X.T @ X):[[ 8004000.98507102 -4000999.99253738] [-4000999.99253738 1999999.99626962]]X:[[1. 2. ] [1. 2.0001]]X.T @ X:[[2. 4.0001 ] [4.0001 8.00040001]]np.linalg.inv(X.T @ X):[[ 8.00039556e+08 -4.00009777e+08] [-4.00009777e+08 1.99999889e+08]]X:[[1. 2. ] [1. 2.00001]]X.T @ X:[[2. 4.00001] [4.00001 8.00004]]np.linalg.inv(X.T @ X):[[ 7.99973381e+10 -3.99985690e+10] [-3.99985690e+10 1.99992345e+10]] This problem can be alleviated by adding positive elements to the diagonals. 123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.array([... [1., 2.],... [1., 2.00001]... ])&gt;&gt;&gt; for _lambda in [1, 0.1, 0.01, 0.001, 0.0001]:... print(f&#x27;np.linalg.inv(X.T @ X + &#123;_lambda&#125; * np.eye(len(X))):&#x27;)... print(np.linalg.inv(X.T @ X + _lambda * np.eye(len(X))))... np.linalg.inv(X.T @ X + 1 * np.eye(len(X))):[[ 0.81818248 -0.36363595] [-0.36363595 0.27272628]]np.linalg.inv(X.T @ X + 0.1 * np.eye(len(X))):[[ 8.01980982 -3.96039026] [-3.96039026 2.07919969]]np.linalg.inv(X.T @ X + 0.01 * np.eye(len(X))):[[ 80.02005978 -39.95998014] [-39.95998014 20.07983982]]np.linalg.inv(X.T @ X + 0.001 * np.eye(len(X))):[[ 800.02078984 -399.95940022] [-399.95940022 200.07918976]]np.linalg.inv(X.T @ X + 0.0001 * np.eye(len(X))):[[ 8000.02719959 -3999.95360059] [-3999.95360059 2000.07179896]] By replacing \\((\\mathbf{X}^{\\operatorname{T}} \\mathbf{X})^{-1}\\) with \\((\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\) in \\({\\hat{\\boldsymbol{\\beta}}} = (\\mathbf{X}^{\\operatorname{T}} \\mathbf{X})^{-1}\\mathbf{X}^{\\operatorname{T}} \\mathbf{y}\\), we derive the solution to ridge regression, \\({\\hat {\\beta }}_{R}=(\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\). Ridge regression (linear regression with L2 regularization), is linear regression using \\({\\mathcal{L}}(\\boldsymbol{\\beta}, \\lambda) = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2} + \\lambda (\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} - C)\\) as the loss function to minimize. This is a Lagrangian function expressing the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) subject to the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} \\le C\\) for some \\(C &gt; 0\\). Note that by calculating \\({\\hat {\\beta }}_{R}=(\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\) with a given \\(\\lambda\\) value, instead of simultaneously solving for \\(\\boldsymbol{\\beta}\\) and lambda through \\(\\nabla{\\mathcal{L}}(\\boldsymbol{\\beta}, \\lambda) = 0\\) (the usual practice of using Lagrangian functions for constrained optimization), we do not necessary obtain a \\(\\boldsymbol{\\beta}\\) that satisfies for a given value of C. However, increasing the given \\(\\lambda\\) value monotonically decreases the value of \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2}\\), thus making the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} \\le C\\) be satisfied for smaller values of \\(C\\). Image from https://math.stackexchange.com/questions/1723201/solution-for-arg-min-xt-x-1-xt-a-x-ct-x-quadratic We can also demonstrate this with an example: 1234567891011121314151617181920212223242526&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; &gt;&gt;&gt; def beta_squared(l, X, y):... beta = np.linalg.inv(X.T @ X + l * np.eye(len(X))) @ X.T @ y... return beta.T @ beta... &gt;&gt;&gt; np.random.seed(0)&gt;&gt;&gt; &gt;&gt;&gt; X = np.random.rand(2, 2)&gt;&gt;&gt; &gt;&gt;&gt; y = np.random.rand(2, 1)&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(0.01, X, y)array([[1.33717503]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(0.1, X, y)array([[0.37141735]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(1.0, X, y)array([[0.13504294]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(10.0, X, y)array([[0.0062103]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(100.0, X, y)array([[7.92298438e-05]]) Furthermore, as \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} = \\beta_{0}^{2} + \\beta_{1}^{2} + \\cdots + \\beta_{p}^{2}\\), increasing the given \\(\\lambda\\) value helps to constrain the magnitude of the effects or regression coefficients corresponding to dimensions which are redundant in high-dimensional explanatory variables. This is visualized in the right diagram, where the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} \\le C\\) in the Lagrangian function (the green circle) tangentially touches a contour of the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) at a point where one of the effects (or regression coefficients) is close to 0. Modified from the plot used in \"The Elements of Statistical Learning\" by Saptashwa Bhattacharyya To further strengthen this effect and completely \"zero out\" certain effects or regression coefficients, lasso regression (linear regression with L1 regularization) can be used in lieu of ridge recursion. In this case, the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) subject to the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{1} = |\\beta_{0}| + |\\beta_{1}| + \\cdots + |\\beta_{p}| \\le C\\) for some \\(C &gt; 0\\), as depicted in the left diagram, where the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{1} \\le C\\) in the Lagrangian function (the cyan square) tangentially touches a contour of the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) at a point where one of the effects (or regression coefficients) is 0. However, we cannot derive an analytical solution for \\(\\boldsymbol{\\beta}\\) given the Lagrangian function for lasso regression (a.k.a. the loss function to minimize), \\({\\mathcal{L}}(\\boldsymbol{\\beta}, \\lambda) = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2} + \\lambda (\\|{\\boldsymbol{\\beta}}\\|_{1} - C)\\). We can only iteratively solve for \\(\\boldsymbol{\\beta}\\) in this case. References: https://en.wikipedia.org/wiki/Lagrange_multiplier https://stats.stackexchange.com/questions/401212/showing-the-equivalence-between-the-l-2-norm-regularized-regression-and https://math.stackexchange.com/questions/1723201/solution-for-arg-min-xt-x-1-xt-a-x-ct-x-quadratic https://arxiv.org/pdf/1509.09169.pdf https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b https://en.wikipedia.org/wiki/Ridge_regression https://online.stat.psu.edu/stat857/node/155/ https://allmodelsarewrong.github.io/ridge.html Kernel Ridge Regression Given the solution to ridge recursion above, \\({\\hat {\\beta }}_{R}=(\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\), we can predict the value of the response variable \\(y_{n + 1}(\\mathbf{x}_{n + 1})\\), given an out-of-dataset vector of explanatory variables \\(\\mathbf{x}_{n + 1} = {\\begin{bmatrix} 1 \\\\ x_{n + 1, 1} \\\\ \\vdots \\\\ x_{n + 1, p} \\end{bmatrix}}\\): \\[y_{n + 1}(\\mathbf{x}_{n + 1}) = \\mathbf{x}_{n + 1}^{\\mathsf{T}} {\\hat {\\beta }}_{R} = \\mathbf{x}_{n + 1}^{\\mathsf{T}} (\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\] We can make some changes to \\(\\mathbf{x}_{n + 1}^{\\mathsf{T}} (\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\). Push-Through Identity Given two matrices \\(\\mathbf{P}, \\mathbf{Q}\\), based on \\(\\mathbf{P} (I + \\mathbf{Q} \\mathbf{P}) = (I + \\mathbf{P} \\mathbf{Q}) \\mathbf{P}\\), we can derive \\({(I + \\mathbf{P} \\mathbf{Q})}^{-1} \\mathbf{P} = \\mathbf{P} {(I + \\mathbf{Q} \\mathbf{P})}^{-1}\\). This is known as the push-through identity, one of the matrix inversion identities used to derive the Woodbury matrix identity, which allows cheap computation of inverses and solutions to linear equations. References: http://www0.cs.ucl.ac.uk/staff/g.ridgway/mil/mil.pdf https://en.wikipedia.org/wiki/Woodbury_bmatrix_identity Based on the push through identity, \\(\\mathbf{x}_{n + 1}^{\\mathsf{T}} (\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} + \\lambda \\mathbf{I} )^{-1} \\mathbf{X}^{\\mathsf{T}} \\mathbf{y} = \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{X}^{\\mathsf{T}} {(\\mathbf{X} \\mathbf{X}^{\\mathsf{T}} + \\lambda \\mathbf{I})}^{-1} \\mathbf{y}\\). As \\(\\mathbf{X} = {\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\end{bmatrix}}\\), \\(\\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n} \\end{bmatrix}}\\), we have: \\(\\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}}\\) \\(\\mathbf{X} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}}\\) Thus: \\[y_{n + 1}(\\mathbf{x}_{n + 1}) = {\\begin{bmatrix} \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}} {({\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}} + \\lambda \\mathbf{I})}^{-1} \\mathbf{y}\\] This means that we can calculate \\(y_{n + 1}(\\mathbf{x}_{n + 1})\\) directly from the dot products among \\(\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{n}\\) and the dot products between \\(\\mathbf{x}_{n + 1}\\) and \\(\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{n}\\), without having to explicitly know the values of \\(\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{n}\\) and \\(\\mathbf{x}_{n + 1}\\). Moreover, the dot product between two vectors of explanatory variables here can be generalized to any symmetric similarity function between two vectors of explanatory variables known as kernel functions. Using \\(k(\\mathbf{x}_{i}, \\mathbf{x}_{j})\\) to denote the similarity between \\(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\) under the kernel function \\(k\\), let: \\(\\mathbf{K} = \\mathbf{X} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} k(\\mathbf{x}_{1}, \\mathbf{x}_{1}) &amp; \\cdots &amp; k(\\mathbf{x}_{1}, \\mathbf{x}_{n}) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ k(\\mathbf{x}_{n}, \\mathbf{x}_{1}) &amp; \\cdots &amp; k(\\mathbf{x}_{n}, \\mathbf{x}_{n}) \\end{bmatrix}}\\) \\(\\mathbf{k}(\\mathbf{x}_{n + 1}) = \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} k(\\mathbf{x}_{n + 1}, \\mathbf{x}_{1}) &amp; \\cdots &amp; k(\\mathbf{x}_{n + 1}, \\mathbf{x}_{n}) \\end{bmatrix}}\\) We have: \\[y_{n + 1}(\\mathbf{x}_{n + 1}) = \\mathbf{k}(\\mathbf{x}_{n + 1}) {(\\mathbf{K} + \\lambda \\mathbf{I})}^{-1} \\mathbf{y}\\] There are two benefits of kernel ridge regression. It allows implicitly performing nonlinear transformations on the vector representations of explanatory variables within similarity calculation, allowing nonlinearity to be introduced. A prominent example is the widespread radial basis function kernel, first used in mining engineering (\"kriging\"). It allows regressions on explanatory variables that do not have explicit vector representations but have similarity functions. There are \"string kernels,\" \"image kernels,\" \"graph kernels,\" and so on. Kriging (from UBC CPSC 340 slides) Kriging Kernel Trick for Non-Vector Data (from UBC CPSC 340 slides) Kernel Trick for Non-Vector Data (from UBC CPSC 340 slides)","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://jifengwu2k.github.io/categories/Mathematics/"}],"tags":[]},{"title":"Sarah Chasins' Works on PL and HCI","slug":"Sarah-Chasins-Works-on-PL-and-HCI","date":"2023-11-05T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2023/11/05/Sarah-Chasins-Works-on-PL-and-HCI/","link":"","permalink":"https://jifengwu2k.github.io/2023/11/05/Sarah-Chasins-Works-on-PL-and-HCI/","excerpt":"","text":"Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain Investigative journalists and public defenders are crucial in scrutinizing and litigating significant matters concerning police violence and misconduct. However, they often need help navigating through vast, unordered heaps of data, which adds strain to their resource-constrained teams. In partnership with U.S. public defenders and investigative journalists, we developed an AI-enhanced tool through a joint design effort to aid in working with such data. This process offered us valuable insights into the requirements of resource-constrained teams dealing with large data sets, including how some experts became self-taught programmers to streamline their workflows. We pinpointed three primary data needs throughout our collaborative design journey and established five design objectives. Three Primary Data Needs Data Cleaning, particularly the process of de-duplication. That involves identifying identical (images of pages are pixel-for-pixel copies of each other) or nearly identical data (images are not pixel-for-pixel identical but capture the same physical document) within a dataset. Data Extraction. The professionals also struggled in extracting relevant information such as names, dates, locations, and case numbers from case files due to their disparate formats and layouts, necessitating extensive, hands-on work. Data Organization. There was a need to systematically organize PDF documents by specific cases, complicated by the fact that cases may be spread across numerous documents and folders, or conversely, several cases might be compiled into one extensive PDF. Five Fundamental Design Principles Human Control and Intervention. The design must prioritize aiding users over complete automation of the process. Non-Interference with Existing Practices. The design should integrate seamlessly with existing workflows and practices. Adaptability to Data Diversity. High-level Abstractions. General-purpose languages like Python or R demand extensive technical expertise. Pre-built software, on the other hand, offers limited flexibility. Cost-Sensitive Solutions. Results Participants in our sessions became adept in all three programming paradigms (visual, PBE, and text-based interfaces). This contradicts the common misconception that non-technical experts need formal coding training to handle text-based programming; if the tools are appropriately supportive, they can. Rather than creating new code, participants preferred to modify what was already there. Particularly with text-based coding, almost all chose to adapt sample code instead of originating their own, aligning with previous research on the blank-page syndrome. A Need-Finding Study with Users of Geospatial Data Current geospatial analysis and visualization tools present significant learning curves and usability challenges. Finding and transforming geospatial data to specific spatiotemporal constraints. Grasping the behavior of geospatial operators. Tracking the provenance of geospatial data, including cross-system provenance. Exploring the cartographic design space. Grasping the behavior of geospatial operators Users had to run operators and manually check outputs to understand operator semantics. Live programming, which offers users immediate visual feedback on program behavior using concrete inputs, could align with users' existing debugging patterns of using small collections of geographic features or pixels as test cases to infer operator behavior. Tracking the provenance of geospatial data, including cross-system provenance The GIS tools used by participants did not track the steps leading to final outputs, complicating the replication of previous analyses. Modifying maps or adapting them to new datasets often meant laboriously reverse engineering the initial analysis steps. Creating repeatable and communicable geospatial workflows was a struggle for GIS users. Limitations in current history features made it difficult to recover information on the current analysis state or revisit past analysis decisions. The problem of tracking provenance across different systems was also prominent. Users often kept informal records of the steps taken in data acquisition, cleaning, analysis, and visualization, which spanned several applications. For instance, one user used macOS Notes to detail a process involving data transfer between Sentinel Hub, QGIS, Illustrator, and Photoshop, documenting everything from selecting a Sentinel-2 image to reassembling raster segments in Illustrator. This kind of multi-tool orchestration was typical among our subjects, yet none had automated systems to log data lineage across these platforms. Exploring the cartographic design space Many participants used direct manipulation tools for geospatial data visualization, which discarded all geographical metadata, posing challenges to revising the analysis after starting the visualization. This uncovers a potential for development in tools that (1) unify geospatial analysis with cartographic design and (2) preserve the geospatial data aspects of visual elements while supporting direct manipulation. Existing research suggests that combining scripting with direct manipulation for visually oriented tasks is feasible. The Sketch-n-sketch application is a testament to the successful merger of these methods for SVG graphics. Such a combined approach could also remedy the fundamental issue participants faced when using direct manipulation tools for cartography: the need to recreate map designs in code after finishing a design. How Statically-Typed Functional Programmers Write Code A deeper comprehension of the coding methods of statically-typed functional programmers could lead to the creation of more practical tools, more user-friendly programming languages, and better gateways into programming communities. These programmers utilize their compilers for more than just producing an executable; they also use compilers as corrective and directive aids. Compilers as corrective tools. Compiler error messages were useful not just to fix their programs but also to correct their mental models of the problem domain. Compilers as directive tools. Many developers treat compiler errors as to-do lists, guiding their subsequent coding actions. A typical process includes beginning a program change with a minor alteration and compiling to receive error-driven sub-tasks - essentially turning error messages into a step-by-step guide for coding. It's not uncommon for programmers to compile their code with the expectation of errors, using the compiler to validate the direction of their development. Statically-typed functional programmers often seek feedback from automated tools even when their code isn't yet operational, suggesting that such tools should strive to extract as much information as possible from non-compilable code. When comparing pattern matching with combinators, statically-typed functional programmers report less cognitive and time pressure with the former. This could be due to pattern matching's explicit textual representation of tasks, explicit handling of recursion, or consistent interface across various data structures. Nonetheless, some programmers prefer to rewrite their code using combinators eventually. Ideally, a tool would assist in this process, starting with a data type and guiding the programmer through case completion, subsequently offering a series of combinators as a refined alternative. It is beneficial to recognize which language constructs allow for low-workload or opportunistic construction and how these constructs are valued within the programming community. There's a demand for tools that minimize the difficulty of altering types during development. Furthermore, these tools should facilitate the natural cyclic changes of a developer's focus between modifying types and modifying expressions, possibly by employing program repair techniques to predict how changes in one will affect the other. Program sketches provide a wealth of information about undefined functions, like inferred types and potential uses. Since statically-typed functional programmers regularly employ this method of drafting and refining code, there's a clear opportunity for tools that could enhance or even automate parts of this practice.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Research Programming","slug":"Paper-Reading/Research-Programming","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Research-Programming/"}],"tags":[]},{"title":"Conversation with Prof. Robert Xiao","slug":"Conversation-with-Prof-Robert-Xiao","date":"2023-10-23T04:00:00.000Z","updated":"2025-08-13T04:31:00.446Z","comments":true,"path":"2023/10/23/Conversation-with-Prof-Robert-Xiao/","link":"","permalink":"https://jifengwu2k.github.io/2023/10/23/Conversation-with-Prof-Robert-Xiao/","excerpt":"","text":"Abstract The following is a polished version of a conversation with Prof. Robert Xiao on the confluence of Programming Languages, Software Engineering, and Human-Computer Interaction (HCI) for research programming. The main points mentioned by Prof. Robert Xiao are as follows. Determining which aspects of research programming are amenable to systematization is a challenge. Advancements in code tracing, like PyTorch 2's bytecode analysis, which, while not addressing verifiability directly, allows for in-depth program behavior analysis. Understanding the influence of input data on outputs is central to the challenge of explainable AI (XAI). However, pinpointing specific model features or layers leading to an output is perhaps more accessible and useful for debugging. The concept of 'radioactively' tagging data to trace its influence through a model is an intriguing one, akin to tracking the uptake of a tagged substance in a biological system. There are many scenarios with extensive object interactions in programming. Game programming provides a more structured context to study these complexities, with ample open-source resources for research. The difficulty in game programming arises from the myriad interactions between diverse object systems - like physics, collision, and interaction logic. Coding these interactions is a lot of work. It would be enlightening to study how game developers handle this complexity and whether there are ways to simplify it. Game studios, being the behemoths they are, would undoubtedly embrace methods to alleviate the strenuous nature of their programming efforts. Polished Transcript Robert Xiao: [00:00] Could you share the focus of your research and how it's pertinent to this project? Also, what do you aim to achieve with it? There seem to be several components you've touched upon, such as visualization, pipeline development, and programming processes. These represent different approaches you could potentially adopt or consider integrating into a comprehensive pipeline. I believe the ultimate goal here is to aid research programmers in accelerating system development while minimizing errors, correct? Jifeng Wu: [00:43] Yes, precisely. Robert Xiao: [00:45] Let's delve into your research focus. How does your current work align with this? Jifeng Wu: [00:52] My ongoing research isn't directly related, as this is a path I'm contemplating for a future Ph.D. project, which I still need to commit to. I'm currently working on my master's thesis titled 'Type Inference for Python.' It aims to infer types in Python code, which often lacks annotations. This lack can lead to IDEs providing less accurate suggestions. With type information, predictions become more reliable, enhancing the coding and code interaction experience. Robert Xiao: [01:54] So, to clarify, your project is about developing a system for automatic type inference that assists IDEs, not just creating a type annotation database. Existing tools do offer preliminary type extraction, but I'm interested in the novel contribution your research makes. Jifeng Wu: [02:31] Exactly. I'm not just extracting types; I'm inferring them in unannotated code bases to enhance IDE functionality. Robert Xiao: [02:45] Understood. There are incremental typing tools available, but we can explore that later. For now, it's great that you're well-versed in Python, especially since it's prevalent in LLM research. An interesting aspect of your direction could be mitigating bugs, which often derail projects. Implementing automated checks could be invaluable. However, the challenge lies in determining which aspects of research programming are amenable to systematization. Jifeng Wu: [06:17] My vision is to support researchers engaged in data analysis or custom model design in an environment akin to Jupyter notebooks. And touching on debugging, I see a potential to harness functional programming due to its purity and ease of debugging. Robert Xiao: [07:22] The question, however, is the application of functional programming to research code, which often depends on pre-existing libraries. While functional design has its merits, the practicality of integrating it into the current ecosystem is worth discussing. Jifeng Wu: [07:55] I concede the point; many codebases are indeed messy. I'm contemplating a clean slate design, potentially developing a new language or library to demonstrate the concept. Robert Xiao: [08:21] It's noteworthy that there have been advancements in code tracing, like PyTorch 2's bytecode analysis, which, while not addressing verifiability directly, allows for in-depth program behavior analysis. Jifeng Wu: [11:09] Certainly. There are facets of current notebook technologies that pique my interest, primarily due to their inadequate support, with debugging being a prime example. Debugging encompasses two key aspects: the logic of the program, as previously mentioned, and data provenance. Sometimes, despite the sound logic, I need to delve into the origins of an unexpected output data point by tracing the implicit calculations that led to it. [11:58] This necessity for data provenance tracking is something I find critically important in my daily research, and I understand it's known as the data provenance problem. Robert Xiao: [12:09] Indeed, if you've ever discussed this with Margo, you're likely well-versed in the topic, given her research focuses precisely on provenance. Many of her colleagues are exploring this area, which is complex, particularly in the context of outputs from extensive machine learning models. While it would be beneficial to trace data points back to their origins, integrating such a mechanism into a model is a formidable challenge. [12:55] As a developer, I'm keen on understanding the influence of input data on outputs, which is central to the challenge of explainable AI (XAI). Resolving this would mark a significant milestone. However, pinpointing specific model features or layers leading to an output is perhaps more accessible and useful for debugging. [13:53] For instance, identifying a misconfigured layer responsible for input-related issues would be invaluable. Although considering the interconnected nature of model layers, this remains a complex task. [14:57] The concept of 'radioactively' tagging data to trace its influence through a model is an intriguing one, akin to tracking the uptake of a tagged substance in a biological system. Yet, translating this to a machine learning environment presents a unique set of challenges. [16:57] While I'm not deeply familiar with the latest advancements in this field, it's clear that XAI could significantly benefit HCI applications. The goal is to incrementally address these challenges by developing models that acknowledge tagged inputs throughout the data processing pipeline. [17:58] These are some thoughts on the subject. I'd like to know which aspects you find most relevant or valuable for your future endeavors. Jifeng Wu: [18:15] The examples and pointers you've provided are insightful. As someone with a software engineering background, I believe that adapting certain constructs, like functional programming and traditional program analysis, could offer potential solutions. These are directions I'm considering for my Ph.D. research. Robert Xiao: [18:58] Exploring program tracing for optimization could prove fruitful, given the untapped potential in that area. The philosophy I subscribe to favors solutions that minimize user effort, exemplified by the tracing compiler feature in PyTorch 2.0. Unlike TensorFlow model, which requires upfront operation declarations, PyTorch's immediate mode operation presents a more straightforward approach for users, facilitating a clearer understanding of variable flow during execution. Jifeng Wu: [ 24:12 ] Incidentally, as a researcher in HCI, have you ever engaged in work that marries aspects of software engineering, specifically functional programming, with HCI? Robert Xiao: [ 24:28 ] My experience with functional programming in a research capacity is virtually nonexistent. My computer science education covered the basics of functional programming - I dabbled in Scheme and Racket - but in terms of research, functional programming hasn't been part of my repertoire. Our work typically involves Python, C#, and various visual programming tools, none of which adhere to a functional programming paradigm. [ 24:56 ] Game programming, which encompasses many of our VR/AR programming, is about as far removed from functional programming as possible. It's heavily state-driven, with an ever-changing state environment. A functional approach could be applied to VR/AR development. It could offer advantages over current methods. I'm aware of actor model programming being used for VR/AR experiences, though it's not functional programming per se, and I have yet to adopt it in my work personally. Conversely, when it comes to software engineering, we do integrate its methodologies into our software creation process. This includes best practices like code structuring for reusability, modularization, refactoring, and especially source control, which I find is grossly underutilized in research programming, among other things. Jifeng Wu: [ 26:33 ] Understood, yes. My vision, when I speak of integrating functional programming, isn't confined to conventional languages like Scheme or Racket that you mentioned earlier. My thoughts were more aligned with programming paradigms like the actor model you described, as well as visual programming. I'm interested in approaches that are more formalized, easier to reason about, and offer a clearer path to verifying properties and facilitating debugging. Robert Xiao: [ 27:18 ] With general-purpose imperative coding, the debugging process is, frankly, a nightmare. Although my personal experience in developing large-scale games is limited, our research typically involves creating specific VR/AR experiences. These are smaller in scale, utilizing existing libraries to build a finite number of interactive elements within a controlled environment. This contrasts with the vast complexity of full-scale game development, where the difficulty arises from the myriad interactions between diverse object systems - like physics, collision, and interaction logic. [ 28:55 ] Take collision logic as an instance; the multitude of possible outcomes from a single collision event can be incredibly intricate to code. If we consider bullet dynamics in games, the behavior of these projectiles upon impact with walls, enemies, or objects varies dramatically, leading to a cascade of different effects. Coding these interactions is a lot of work. It would be enlightening to study how game developers handle this complexity and whether there are ways to simplify the process. The Entity Component System (ECS) attempts to mitigate this by adopting an actor-like model, but even then, complexity escalates rapidly as interactions increase. [ 30:56 ] Despite efforts to manage these interactions, there comes a point where local decisions require some form of higher-level orchestration. When a bullet is fired, for instance, numerous actions must be coordinated, from ammo count adjustments to triggering animations. All this complexity makes game programming something I would not want to revisit except in the context of my research. However, there lies a vast potential for impactful research in understanding and easing the complexities of game development. Game studios, being the behemoths they are, would undoubtedly embrace methods to alleviate the strenuous nature of their programming efforts. Yes, indeed. Jifeng Wu: [32:00] Indeed, that aligns with my central interests. My master's thesis on Python type inference encountered similar challenges to those you've highlighted. In analyzing types, one must grapple with substantial propagation throughout the program. This is precisely the issue at hand, and it serves as a prime motivator in my quest to develop formalizations that simplify the process for programmers, particularly in areas like game development, where complex interactions are commonplace. Robert Xiao: [32:45] You've sparked a thought here - this may be a digression - but I'm curious. Has there been research into the amount of typing necessary for untyped code to converge? You made an excellent point about the recursive search required in untyped code to determine types, which resembles an intricate graph search. However, it's more than that because the connections in the 'type graph' are not always apparent. This raises an intriguing theoretical question: At what point in a code base's typing does the cost shift from an exponential to a linear time complexity? It's a highly theoretical question, indeed, and one that surely must have been examined in terms of computational complexity. [34:58] It's fascinating because most research focuses on strong typing systems and type inference within defined parameters. But the dynamics change with untyped constructs. Apologies for the tangent - it's just a curious question. Jifeng Wu: [35:32] Your point is very interesting, and it is pertinent to my future research endeavors. Robert Xiao: [35:39] There ought to be studies on this - how computationally complex is a type system? The PL community has likely delved into this. However, the issue becomes significantly more compelling when considering untyped languages. Sorry for the tangent, but it's a topic worth exploring. Jifeng Wu: [36:15] Returning to our discussion, my work in type inference for Python resonates with your mention of interaction-heavy domains like game programming. Robert Xiao: [36:27] Certainly. Game programming exemplifies a scenario with extensive object interactions, which is atypical in most systems where such interactions are minimized. For instance, hiring a student triggers a cascade of bureaucratic actions, illustrating how a single decision can activate multiple layers of complexity. Managing these interactions presents a formidable challenge in software engineering - taming the complexity is a substantial part of the job. However, game programming provides a more structured context to study these complexities, with ample open-source resources for research. Jifeng Wu: [38:55] Your insights on game programming are quite valuable. [38:59] I hadn't realized the prominence of such problems in that field. Your points offer an excellent foundation for my research into these issues. Robert Xiao: [39:16] There are indeed intriguing research opportunities at the intersection of AI, software, programming languages, and HCI. Although my HCI pursuits are broad, one key HCI interest is explainable AI. As language models advance, the ability to explain AI operations lags, posing a risk of increasing reliance on inscrutable systems. Advancing explainable AI methodologies will be critical in HCI, particularly in the coming years. Jifeng Wu: [42:10] Yes, indeed. What captivates my interest more than the empirical approach to software engineering - like automated bug detection and performance optimization - is the human-computer interaction aspect, particularly making developers' lives easier. This is crucial, especially for rapid prototyping. Robert Xiao: [42:42] Right, your IDE example aligns perfectly with that. It's a tool that enhances usability for people - expanding how they interact with typing. It could very well be something for publication. So, you're drawing connections here. When do you expect to graduate? Jifeng Wu: [43:05] If all goes according to my advisor's plan, I should graduate in May 2024. Robert Xiao: [43:13] Okay. And your advisor is? Jifeng Wu: [43:16] Caroline Lemieux from the Software Practices Lab. Robert Xiao: [43:20] Oh, yes, she's a recent addition. So, she's guiding you toward a May graduation. And regarding your Ph.D., are you considering continuing in the Software Practices Lab or exploring other areas? Jifeng Wu: [43:38] Well, our lab's focus is split between user studies and the technical side, like bug detection or software fuzzing, and, on the other hand, theoretical topics like formal semantics. There isn't much overlap with HCI and usability, so I'm uncertain about where I'll pursue a Ph.D., if at all, but likely not within our Software Practices Lab. Robert Xiao: [44:21] That's something to ponder, especially as you approach graduation in May. Are you already applying to Ph.D. programs? Jifeng Wu: [44:34] I am still deciding. I'm still considering my options. Robert Xiao: [44:38] Sure, it's a big decision. My focus is on VR/AR-related projects, where my funding comes from. It's trickier to shift to developing support tools for developers without an established portfolio. However, I'm open to discussing co-advisory opportunities or committee collaborations if you pursue a Ph.D. here at UBC. Jifeng Wu: [45:44] That's encouraging to hear. Robert Xiao: [45:45] To clarify, I'm not sure I could supervise a Ph.D., but I'm open to discussing future possibilities. Jifeng Wu: [45:59] That aligns with my thoughts as well. Robert Xiao: [46:01] Engaging in research discussions can help refine your interests, which is beneficial for crafting a strong research statement for Ph.D. applications. These conversations can guide your initial research direction, even though your focus may evolve. Jifeng Wu: [47:10] Thank you for the insights and advice today. They've been very helpful, and I'll reflect on them further. Robert Xiao: [47:24] I'm glad to assist. You should have my email address. Jifeng Wu: [47:30] Is it listed on your website? Robert Xiao: [47:32] It should be - unless my website isn't updated with my current email, which would be a blunder. Let me check. Jifeng Wu: [47:36] I'll look it up. Robert Xiao: [47:37] If it's on there, then it's correct. Jifeng Wu: [47:45] Yes, it's on your site. Robert Xiao: [47:47] Great, feel free to reach out anytime. Robert Xiao: [47:56] I'm impressed by your work and would be happy to attend your thesis presentation when the time comes. Jifeng Wu: [48:15] I appreciate that. Our conversation today has been very enjoyable. Robert Xiao: [48:21] It was an enlightening chat, indeed. If you have further questions or topics, don't hesitate to email me. Jifeng Wu: [48:34] For now, that's all I have. If something else comes up, I'll send you an email. Thank you.","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"Nico Ritschel's Ph.D. Defense Summary","slug":"Nico-Ritschel-s-Ph-D-Defense-Summary","date":"2023-10-13T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2023/10/13/Nico-Ritschel-s-Ph-D-Defense-Summary/","link":"","permalink":"https://jifengwu2k.github.io/2023/10/13/Nico-Ritschel-s-Ph-D-Defense-Summary/","excerpt":"","text":"Nico Ritschel's research focuses on refining block-based programming by integrating elements from visual programming to make it more accessible and effective for end-users, especially in the robotics domain. Problem Statement Block-based programming is mainly used for computer science education. Can they target other tasks, such as end-user programming? The challenge: end-users often need to write larger, real-world programs, contrasting with the simple toy examples students typically handle. Traditional block-based programming struggles with scalability, especially in terms of readability. While visual end-user programming tools like Excel and Simulink support bigger programs through domain-specific visual abstractions, creating new visual languages is difficult and costly. Solution Approach: Merge design features from visual programming into block-based programming languages. Target Domain: Robotics Current Scenario: Professional tools exist, but they're challenging to use. There needs to be more effective block-based tools in the domain. Robot Arms for Factory Floors: Task: Coordinate and synchronize two robot arms. Issues: Current block-based languages require complex solutions like nontrivial mutexes. Solution &amp; Studies: Proposed two design ideas: Represent programs for each arm vertically and side-by-side. Synchronized actions appear as shared nodes between the arms. A left-to-right flow resembling video editing. The 'side-by-side' design was selected. A study found that end-users using this design outperformed those using a commercial, text-based tool. Mobile Robots for Warehouses &amp; Labs: Task: Handle large tasks across multiple workstations. Issues: Difficulty decomposing long programs and locating where to make changes. Solutions &amp; Features: Introduced block-based language that supports functional decomposition. Provided two separate canvases: one for task composition/movement and the other for low-level task definitions. Included triggers as dataflow graphs to improve the visibility of nested expressions and enhance user freedom in structuring programs. Questions Addressed During the Practice Session Why focus on the two robotics scenarios? They are important and relevant in the robotics domain. These scenarios present challenges for end-users learning to program. They represent a complex form of programming that's worth refining. Would functional programming principles enhance end-user visual programming, given the imperative nature of block-based programming? The inherent complexity in robotics means many elements can't be simplified. Introducing functional programming might not necessarily boost user productivity. What was the environment for user studies? Engaged actual end-users for genuine feedback. Also recruited students from non-computer science departments for a broader perspective. Questions Asked During the Ph.D. Defense How were the visions and observations formulated? Separate users into traditional versus new environments and then compare. Gain knowledge of their needs and patterns. Test on a small pool of users to refine the design. How do you account for the spectrum of end-users regarding programming experience, domain-specific task time, and tool experience? Which results were the most and least robust? What factors made the tool easy to learn? The \"blocks\" concept is already well-known. The tool matches the users' previous domain-specific knowledge (e.g., separate columns for two arms). How realistic is the decomposition at scale? Any evidence from related work? More of a \"lower bound,\" limited by the time of the user study. Why was the comparison made between block-based methods and graph-based methods? Graph-based methods are already used in end-user programming, such as game programming. What is the importance and implication of the determined p-value? We have a null hypothesis - there is no difference between the performance of the two groups. What improvements (e.g., 5%) are worthwhile? What are the advantages of block-based approaches over dataflow, and how can this be further investigated? Different aspects, e.g., reading vs writing Different domains, e.g., robotics vs game Different styles of programs Different representations of graphs How are potential accessibility challenges addressed? Already addressed to a degree in the normal block-based domain. Domain-specific challenges are directions for future work. How does the new tool compare with LLMs? Can work together. Have advantages in evolution and understanding vs. writing something that would work the first time. Debugging. Reliability. No training required. What follow-up studies are anticipated for real-world usage? How do you anticipate the tool's usability in practical scenarios? Follow-up studies based on real-world usage in the wild may encounter unanticipated, really specific problems. Is your tool something someone wants to use in practice? Would featuring a table of reactive values a la Excel be beneficial? How do different domains within computer science influence the tool's design and analysis? What interdisciplinary expertise would be beneficial? Information visualization. Designing design drafts with an expert in visualization would be beneficial. What about your tool's applicability to expert programmers instead of end users? Different design goals.","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"Conversation with Prof. Margo Seltzer","slug":"Conversation-with-Prof-Margo-Seltzer","date":"2023-10-10T04:00:00.000Z","updated":"2025-08-13T04:31:00.445Z","comments":true,"path":"2023/10/10/Conversation-with-Prof-Margo-Seltzer/","link":"","permalink":"https://jifengwu2k.github.io/2023/10/10/Conversation-with-Prof-Margo-Seltzer/","excerpt":"","text":"Introduction to Prof. Margo Seltzer (quoted from Wikipedia) \"Margo Ilene Seltzer is a professor and researcher in computer systems. She is currently the Canada 150 Research Chair in Computer Systems and the Cheriton Family Chair in Computer Science at the University of British Columbia. Previously, Seltzer was the Herchel Smith Professor of Computer Science at Harvard University's John A. Paulson School of Engineering and Applied Sciences and director at the Center for Research on Computation and Society.\" Question: How did you conduct research across a variety of domains, from operating systems to machine learning systems? Prof. Seltzer: I've always been intellectually curious and I find almost all research problems fascinating. Engaging in discussions with diverse people has also fueled my passion. When I was a junior faculty member, I focused on tenure and focused on core systems research, but that was miserable. However, I still explored different areas. My deep interest lies in software architecture, even though my Ph.D. was in storage. I was fortunate when another lab decided to support my research. This shift allowed me to progress from storage to core systems. I also got interested into data provenance, especially realizing that we could do a lot more at the systems level. Transitioning to machine learning was a natural progression, driven mainly by collaborations with graduate students and other partners. Question: Why did you pursue a Ph.D. in storage if you were more interested in core systems? Prof. Seltzer: Before pursuing my Ph.D., I was primarily involved with databases. However, as I delved deeper, my curiosity veered towards system issues. Question: How do you manage evolving interests during a Ph.D.? Prof. Seltzer: It's uncommon for Ph.D. students to plot a lifetime research agenda. Instead, it's about producing one miracle per paper and developing the skills to do research for your whole life. The key is to focus on accomplishing your first piece of independent research during your Ph.D. Choosing a supervisor you get along well with is most important. It's essential to be involved in an interesting area and join a lab that aligns with your interests. However, a perfect match isn't always necessary. Looking at co-supervised students can give insights into potential co-supervision opportunities. As a Ph.D. student, your primary goal should be to define your research problem. Although you shouldn't jump between entirely different areas, it's crucial to select a project that genuinely interests you in the first year. Other interests can be pursued as side projects. To maintain engagement, pick a broad domain that offers a plethora of projects you find captivating. Before starting a Ph.D., actively seek out research papers that intrigue you and identify the labs behind them. Question: What's your vision for the future of Computer Systems? Prof. Seltzer: A pressing concern is that people are not very good at writing software that works. We need to develop strategies to create software with minimal bugs from the ground up. Embracing modularity can be a solution, and the solution is about software architecture. Researchers focus on verifying existing software products because the publication cycle is way too short. Moreover, we lack good metrics for evaluating software architecture, and there is no equivalent of a debugger for software architecture. Software architecture, in its current state, remains an art more than a well-defined discipline. Often, professionals in the field rely heavily on mentors, and they don't see the growth of a new generation of software architects. Personal Comments and Recommendations: I'm pleased to note your inspiration derived from challenges in the 'Type Inference for Python' project, including the importance of formalizations and specifications both for the design goal and for implementation, the tedious and fault-prone task of setting up an evaluation pipeline, etc. Deep learning thrives in domains with a clear ground truth. In other scenarios, basic probabilistic methods might offer better results. Your task of combining AST traversal with introspection of live objects reminds me of the work I did in \"StarFlow: A Script-Centric Data Analysis Environment\" and Arpan Gujarati's tracing infrastructure efforts in Python. Should you wish to delve deeper into software architecture for data science and machine learning, I recommend focusing on constructing intricate software like operating systems instead of shorter data wrangling scripts. Or you can explore the challenges in experimental frameworks. For insights on this, consider discussing with Joe Wonsil. Additionally, Philip Guo at UCSD has an intriguing Ph.D. thesis about tools for research programmers that might be of interest to you.","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"Pre-MICCAI Workshop@UBC Observations and Gained Insights","slug":"Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights","date":"2023-10-08T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/","link":"","permalink":"https://jifengwu2k.github.io/2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/","excerpt":"","text":"From the Pre-MICCAI Workshop@UBC website: The Pre-MICCAI Workshop is a dynamic and innovative platform that unites machine learning and medical computer vision. As a prelude to the prestigious MICCAI (Medical Image Computing and Computer-Assisted Intervention) conference, this workshop serves as a vital nexus where experts, researchers, and enthusiasts converge to explore cutting-edge advancements, exchange knowledge, and foster collaborative partnerships in the field of medical image analysis. Shaoting Zhang (Shanghai AI Lab) - Keynote Talk 2 - Foundation Models in Medicine: Generalist vs Specialist Advantages of Large Models: Emergent abilities. Long-tail problems (only a small amount of fine-tuning is required for downstream tasks and does not require a tremendous amount of data collection and labeling). Model sharing strengthens data security. Shanghai AI Lab presents OpenMEDLab (open-source medical image and language foundation models). Utilizing a single model with varied prompts for diverse tasks. Large language model training encompasses: Self-supervised pre-training. Instruction tuning. RLHF. Plugins for accessing updated information without retraining. Computer vision researchers lean towards generalist models due to the technical challenges. Clinicians prefer specialist models to solve day-to-day work. Question: Will medical foundation models support more modalities in the future besides vision and language? Answer: People will still focus on one modality for one model with high accuracy to address practical business demands. Multiple models can be used on demand to handle multimodal data. Briefings Sana Ayromlou - Continual Class-Specific Impression for Data-free Class Incremental Learning Focuses on training models over newly introduced classes, termed incremental learning. Challenges include the loss of old data, resulting in catastrophic forgetting. Proposed Solution: Generate synthetic medical data from prior classes using model inversion (extracting training data from the model) and employing cosine-normalized cross-entropy loss. Hooman Vaseli - ProtoASNet Emphasizes the importance of interpretability in AI solutions, especially in healthcare. Core Technology: Prototypical neural networks, which \"learn a metric space in which classification can be performed by computing distances to prototype representations of each class.\" Ruogu Fang (University of Florida) - Keynote Talk 4 - A Tale of Two Frontiers: When Brain Meets AI Research Vision: Integrate domain knowledge over mere data-driven approaches. Harness neuroscience principles for next-gen AI designs. Leverage AI in testing neural science hypotheses and promoting brain health. Deep Evolutionary Networks with Expedited Genetic Algorithms for Medical Image Denoising Auto feature extraction and hyperparameter search are major pain points in deep learning research (compared with traditional machine learning research) faced by deep learning researchers. Fine gene transfer learning to optimize on a larger dataset - c.f. portfolio balance in finance Question: Is it possible to combine the genetic algorithm that maintains a gene pool of neural networks with ensemble learning? Answer: Different objective. Emergence of Emotion Selectivity in A Deep Neural Network Trained to Recognize Visual Objects Simple, interpretable neural network architecture based on biology. Representation similarity between the DNN model and brain amygdala. $1M NSF funding. Modular machine learning for Alzheimer's disease classification from retinal vasculature Retina data is easy to collect. A lot of information (gender, body mass index) can be seen from the retina. The results are interpretable. Hervé Lombaert (ETS Montreal) - Keynote Talk 3 - Geometric Deep Learning - Examples on Brain Surfaces Research directions: Geometry and Machine Learning. Correspondences and variability existent in the brain. Motivation: Traditional algorithms frequently rely on an image grid (pixels). However, in neuroimaging, data is often on 3D surfaces. Two neighboring points may be neighbors but may lie very far away on such a surface. How to learn on such surfaces? How do we transfer convolution and pooling on images to such surfaces? Solution: Represent surfaces as graphs. Project problem into spectral space (spectral shape analysis). An object's vibration pattern is governed by shape - spectral space captures a unique intrinsic shape signature. Extract spectral signature via spectral decomposition and exploit to find correspondences. Enables transforming convolutions on surfaces to convolutions on spectral embeddings, enabling classical architectures on brain surfaces. Ongoing work: Active learning to reduce annotation effort - focus on sample-level uncertainty and find the most uncertain images. Goals: Informative and diverse samples. Works: TAAL: Test-time augmentation for active learning in medical image segmentation Active learning for medical image segmentation with stochastic batches Ali Bashashati/Ruogu Fang/Shaoting Zhang/Hervé Lombaer/Jun Ma - Panel Discussion The influence of Large Language Models is growing significantly. What changes do you think LLMs will bring about in medical imaging (from both positive and negative sides)? Language contributes to improved performance. Still need a diversity of models to investigate different modalities and tasks. Large language models help in day-to-day routine tasks. They are a copilot which facilitates the processing of huge amounts of information in pathology and brain research. Reduces cost and boosts accessibility for patients. Multimodal data integration. LLMs face data privacy and trustworthiness. When to use LLMs and when to use human abilities requires careful thinking. What other recent medical image analysis advancements excite you the most? Classic problems like segmentations and how to capture geometry remain unsolved. More comprehensive and dynamic brain-inspired, biologically-inspired AI. Understanding the biology behind the data will help you design more applicable models. Those models can better make a difference Prior knowledge is important in addition to big data. Foundational models will explore all non-synthetic data in the next few years; no new data will exist. Montreal is a major hub for neuroscience and AI. For the many students here, what technical skills and knowledge should the next generation of medical image analysis researchers prepare for? Know the neglected basics, e.g., solid mathematical background and proficiency in programming Understand the data Ability to explain the results and ask the question of why and how Visualization is very important for both exploratory data analysis and publishing Learning from mistakes - find out why a model doesn't work instead of throwing in different models Ask yourself: Who will care about an increase in accuracy? Is it significant? Will it have tradeoffs in robustness, explainability, etc.? Quickly take up new skills (mathematics, programming, etc.) Research paradigms have changed in the foundation model era - how to leverage foundation models for your field to stand on the shoulders of giants? Low-level implementation details such as preprocessing, multiprocessing in coding for large-scale data, model development, multi-node distributed training, efficient fine-tuning, and model deployment on constrained environments are also critical skills. Work and have fun at the same time. Perseverance in the face of failure is one of the most essential qualities for Ph.D. students. Question: The future of models for specific tasks (e.g., segmentation) vs end-to-end models. New models for specific tasks make lovely reads. Methodology will change, but specific tasks will stay there. However, improving specific tasks will gradually shift towards industry. Universities will focus on publishing the first paper in a domain, while industry will focus on publishing the last paper in a domain. In the end, we care about helping patients.","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"Understanding the Name, Structure, and Loss Function of the Variational Autoencoder","slug":"Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder","date":"2023-09-30T04:00:00.000Z","updated":"2025-08-13T04:31:00.461Z","comments":true,"path":"2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/","link":"","permalink":"https://jifengwu2k.github.io/2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/","excerpt":"","text":"Despite the intuitive appeal of variational autoencoders (VAEs), their underlying principles can be elusive. After extensive research across papers and online resources, I will summarize the core insights behind the VAE's name, structure, and loss function and try to explain how the mathematical formulas used to describe the VAE came into being from first principles, as opposed to simply providing interpretations for them. Basics of VAEs VAEs are probabilistic generative models, when trained on a dataset \\(X\\), allow us to sample from a latent variable \\(Z\\) and generate output resembling samples in \\(X\\) through a trained neural network \\(f: Z \\rightarrow X\\). This can be formulated as making the probability of generating \\(X = x\\) as close as possible to the actual \\(P(X = x)\\) (known quality) under the entire generative process. Ideal Training Goal In the ideal situation, based on the marginal distribution formula we have \\(P(X = x) = \\int{P(X = x | Z = z) P(Z = z) dz}\\). Thus, the training goal of variational autoencoders is to make the actual \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) as close to \\(P(X = x)\\) as possible. Latent Variable Distribution VAEs select a multivariate normal distribution for the latent variable \\(Z\\) based on the principle that any distribution in \\(d\\) dimensions can be generated by mapping normally distributed variables through a sufficiently complicated function, which could be approximated using the neural network \\(f: Z \\rightarrow X\\) we train. Approximation Challenge Having reasonably decided \\(Z \\sim N(0, I)\\), we may calculate the actual \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\). This is straightforward to approximate: we can randomly sample a large number of \\(Z\\) values \\(\\{z_1, \\dots, z_n\\}\\), and approximate \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) as \\(\\sum_{j}^{n}{P(X = x | Z = z_j)}\\). However, for most \\(Z\\) values, \\(P(X = x | Z)\\) will be nearly zero, contributing almost nothing to our calculation. This is especially the case in high dimensional spaces, for which an extremely large number of samples of \\(Z\\) may be required. To address the problem, we can attempt to sample values of \\(Z\\) that are likely to have produced \\(X = x\\) and compute \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) just from those. The \"Variational\" Aspect: To do so, we can fit another parametrized function \\(Q(Z | X = x)\\), which can give us a distribution over \\(Z\\) values that are likely to produce \\(X = x\\) through \\(f: Z \\rightarrow X\\) given \\(X = x\\). This is an example of a variational Bayesian method, which involves finding an \"optimal\" function (a task known as variational calculus) and is the source of the word \"variational\" in variational autoencoders. Minimizing Divergence Theoretically, the values of \\(Z\\) that are likely to have produced \\(X = x\\) follow the conditional distribution \\(P(Z | X = x)\\). Thus, our original goal of making the actual \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) as close to \\(P(X = x)\\) as possible can be transformed to minimizing the Kullback-Leibler divergence between \\(P(Z | X = x)\\) and \\(Q(Z | X = x)\\): \\[KL(Q(Z | X = x) || P(Z | X = x)) = \\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x)}{P(Z = z | X = x)}} dz}\\] According to Bayes' Law, \\[P(Z = z | X = x) = \\frac{P(X = x | Z = z) P(Z = z)}{P(X = x)}\\] Thus, we have: \\[\\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x) P(X = x)}{P(X = x | Z = z) P(Z = z)}} dz}\\] \\[= \\int{Q(Z = z | X = x) (\\log{\\frac{Q(Z = z | X = x)}{P(Z = z)}} + \\log{P(X = x)} - \\log{P(X = x | Z = z)}) dz}\\] \\[= \\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} + \\int{Q(Z = z | X = x) \\log{P(X = x)} dz} - \\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\] Note that: \\[\\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} = KL(Q(Z | X = x) || P(Z))\\] \\[\\int{Q(Z = z | X = x) \\log{P(X = x)} dz} = \\log{P(X = x)} \\int{Q(Z = z | X = x)} dz = \\log{P(X = x)}\\] Thus, we have: \\[KL(Q(Z | X = x) || P(Z | X = x)) = KL(Q(Z | X = x) || P(Z)) + \\log{P(X = x)} - \\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\] As \\(\\log{P(X = x)}\\) is constant, if we were to minimize \\(KL(Q(Z | X = x) || P(Z | X = x))\\), we should minimize: \\[KL(Q(Z | X = x) || P(Z)) - \\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\] To further transfer that into a calculatable function, we need to be more specific about the form that \\(Q(Z | X)\\) will take. The usual choice is to say that \\(Q(Z | X = x) = N(Z | \\mu(X = x), \\Sigma(X = x))\\), i.e., \\(Q(Z | X = x)\\) follows a Gaussian distribution where the mean and covariance matrix are calculated by parameterized functions (trained neural networks) given \\(X = x\\). In this case, fitting \\(Q(Z | X = x)\\) involves training these neural networks. \\(Q(Z | X = x) = N(Z | \\mu(X = x), \\Sigma(X = x))\\) The advantages of this choice are computational, as \\(KL(Q(Z | X = x) || P(Z)) + \\log{P(X = x)}\\) is now a KL-divergence between two multivariate Gaussian distributions, which can be computed in closed form. As for \\(\\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\), it depicts the expected log-likelihood of generating \\(X = x\\) as the VAE's output through \\(f(Z)\\) when sampling from \\(Q(Z = z | X = x)\\) given \\(X = x\\). Thus, it can be treated as the \"reconstruction loss\" of the VAE, and different closed-form indices, such as mean square error, may be used as proxies of it depending on the project domain. Why \"Autoencoders\"? Despite the mathematical basis of VAEs being quite different from classical autoencoders, they are named \"autoencoders\" due to their final training objective involving an encoder (the neural networks \\(\\mu\\) and \\(\\Sigma\\) determining mean and covariance) and a decoder (the neural network \\(f\\)), which resembles a traditional autoencoder in structure. References https://arxiv.org/abs/1606.05908 https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/ https://arxiv.org/abs/1312.6114 https://arxiv.org/abs/1907.08956 https://stats.stackexchange.com/questions/485488/should-reconstruction-loss-be-computed-as-sum-or-average-over-input-for-variatio https://stats.stackexchange.com/questions/540092/how-do-we-get-to-the-mse-in-the-loss-function-for-a-variational-autoencoder https://stats.stackexchange.com/questions/464875/mean-square-error-as-reconstruction-loss-in-vae https://stats.stackexchange.com/questions/323568/help-understanding-reconstruction-loss-in-variational-autoencoder","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://jifengwu2k.github.io/categories/Mathematics/"}],"tags":[]},{"title":"My Software Engineering Philosophy","slug":"My-Software-Engineering-Philosophy","date":"2023-09-24T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2023/09/24/My-Software-Engineering-Philosophy/","link":"","permalink":"https://jifengwu2k.github.io/2023/09/24/My-Software-Engineering-Philosophy/","excerpt":"","text":"If you deprive yourself of outsourcing and your competitors do not, you're putting yourself out of business. Lee Kuan Yew Do the high-level, high-value requirements, analysis, and design work in an incremental fashion while not sacrificing rigor. Maximize the utilization of tools that make coding, testing, and operations as cheap, trivial, straightforward, and error-free as possible, minimizing technical debt, including: Generative AI tools like ChatGPT. Functional Programming. A critique on both the waterfall model and the agile model.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"The Cornell, Maryland, Max Planck Pre-doctoral Research School 2023 Observations and Gained Insights","slug":"The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights","date":"2023-09-08T04:00:00.000Z","updated":"2025-08-13T04:31:00.460Z","comments":true,"path":"2023/09/08/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights/","link":"","permalink":"https://jifengwu2k.github.io/2023/09/08/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights/","excerpt":"","text":"Group Photo Panel Session 2: \"Research in industry vs. academia\" Problem Focus &amp; Recognition Industry tends to focus on concrete problems. In academia, broader issues are often addressed. Authorship and credit in academia is complex. It's not a zero-sum game. It's not just about who is first or second author; giving credit to students doesn't mean professors won't get any. Publication &amp; Quality The emphasis is on publishing fewer papers but ensuring they are of high quality. It's not about the quantity but the impact and quality of the papers. The first, last, or best paper on a topic are the most influential. Career Path Before securing a tenure professor position, many go through multiple postdocs and even stints as industrial research scientists. Only about 10-20% of PhDs eventually become faculty. Some research scientists find academic-like environments within the right industry groups. Factors Differentiating Academia and Industry: Industrial research must eventually have some commercial value. In academia, there are constraints like obtaining funding, student recruitment, and equipment acquisition. Academics have better job security and can rebound from mistakes. Industry doesn't need to chase grants or funding in the same way academia does. Skills &amp; Transitions Transferring skills between departments or companies is straightforward. Transitioning between academia and industry is often a one-way street. It's challenging to return to academia from industry unless one maintains a consistent publishing record and works on research-valued projects. Geographical and Topic Mobility Researchers are encouraged to be flexible, moving across countries and topics. Work-Life Balance Systems vary across locations. Enforce personal boundaries and learn to say no. A balance doesn't mean absence of stress. In the industry, even if the work-life balance is okay, stress may arise from working on undesired projects or facing peer pressure. Find people who become friends with you. Two-Body Problem It's more of an issue in academia than in industry since it's easier to change companies than academic institutions. Personality and Approach Industry caters to hackers and those interested in tooling. Academics focus on research and higher purposes and see coding as a tool. Effective communication, including selling your idea in proposals and talks, is vital. Startups vs. PhD Journey Both require a significant commitment, typically around 6-8 years to IPO. Startups demand full devotion, often with little to no work-life balance. Funding &amp; Tenure If a grant from a company fails, there will be no direct legal consequences, but the likelihood of getting another might be reduced. Tenure provides a basic salary and job security, but researchers still need to raise funds for their research. Doing a job aligned research can be beneficial for dissertation and future career opportunities. Laxman Dhulipala (2nd Lecture) Graphs are ubiquitous structures. Implementing high-performance graph algorithms speeds up scientific discovery. I don't work on dense graphs. Real-world graphs are sparse, and I haven't seen a dense graph in practice in 10 years. I focus on shared-memory algorithms and don't recommend programming supercomputers until you have to. Recommended reading: Scalability! But at what COST? Should batch updates to dynamic graphs More parallelism Reduces the cost of each update Representing adjacency information using purely functional trees are safe for concurrency. Guest Lecture: Yiting Xia There are different available connections at different time slices. Precomputing routes and handling link failure is still work in progress. Group-Mentoring Session Peter Druschel and Bobby Bhattacharjee Key Skills and Knowledge Emphasized the importance of academic aptitude and the ability to work in unstructured environments. Problem-solving Approach Seek problems that are significant, solvable, and align with your skill set. Recognize that one may not always approach the right problem from the best angle. Handling setbacks is crucial. Time spent on tackling a problem is never lost. Resilience, dedication, and discipline are essential traits for success. Read many things that are loosely related to solve a problem, as they might offer insights. Application Strategy Apply to a minimum of 5-10 institutions. Do the homework for providing a strong application, especially given low acceptance rates, like 10%. Interests and Graduate Programs Have a broad range of interests when considering a graduate program. Opt for programs that offer a wide variety of choices. Expressing diverse interests in applications can improve acceptance chances. It's advisable not to close one's doors apriori. Monitoring Progress in Grad Programs A competent group advisor is crucial, as they will guide and look out for students challenges like selecting an excessively challenging problem, lacking motivation, or poor time management. Set achievable milestones that lead to publications, helping to build a solid publication record. ### Mariya Toneva Changing Discipline during Ph.D. Evaluate if the institution has the necessary resources to support this transition. Traits of an Ideal Ph.D. Student Effective communication skills. Strong critical thinking abilities. A robust computational background. Prior research experience. Linguistics Noted a resurgence in the domain of linguistics as opposed to pure data-driven techniques. MPI-SWS MPI-SWS is highly recommended for programming languages, especially when collaborating with diverse groups of people. Diving into NLP (Natural Language Processing) - Hop On Now? When considering venturing into NLP, focus on: Experts who have a distinct vision in a less-saturated niche. Those with substantial experience in related fields, such as the intersection of NLP and robotics. Distinguishing Yourself in Applications To stand out: Foster qualities like initiative, drive, and ambition. Accumulate experiences that align with and support your academic and research interests. Obtain references that can vouch for your character and work ethic. It's also essential to explore and consider multiple options or paths. Lorenzo Alvisi Cultivating an Academic Sense To nurture an academic mindset, one should assess how an individual performs when faced with a problem. He mentioned the \"Dijkstra club\" at UT Austin as an example. Emphasized the significance of \"beautiful work\" and that it's crucial for individuals to produce work of beauty and quality. Observing and learning from the endeavors of others is beneficial. Life's Blueprint Life does not come with a set map but rather a compass for direction. Professor Alvisi never limited his imagination about his capabilities. Guiding principles in life: Seeking personal happiness. Maintaining healthy relationships. Pursuing a fulfilling job that combines happiness with challenges. Acceptance of uncertain outcomes: One might not always know if they will succeed or fail. The importance of personal growth: Find joy in self-improvement. Shared personal experience of pursuing two Ph.D. degrees, the first of which was at an institution he didn't particularly favor. Highlighted that struggles are often hidden from view. Career Perspectives One's career doesn't necessarily peak at a fixed point; there's always potential for growth, including entering academia. Career choices are not always black and white; it depends on personal preferences and aspirations, such as seeking excellent opportunities close to home. Consider the duration of your investments in particular career choices. Not every commitment needs to be long-term. Balancing Hobbies and Work Prof. Alvisi shared advice from his mentor's mentor about integrating hobbies into professional life. While he had diverse interests, he made sacrifices to focus on computer science due to his intellectual capacities. Some hobbies were too time-consuming. Emphasized the importance of hobbies as they provide a necessary balance and maintain mental well-being. Addressing the Two-Body Problem Universities recognize the challenge when both partners in a relationship are professionals. If partners excel in different domains, there's potential for both to be hired with attractive incentives. Solutions include proactive planning, alternating priorities between partners over the years, and considering remote work opportunities. Other Insights Mentioned the Sloan Fellowship as a notable achievement before tenure. Advised young professionals to delay specialization as long as possible. Explore various options. Encouraged students to seek advice from multiple professors to gain a diverse range of opinions and insights. Tapomayukh Bhattacharjee (2nd Lecture) There are six activities of daily living (ADLs) defined in literature: personal hygiene or grooming, dressing, toileting, transferring or ambulating, and eating Anomaly detection is used in processing sensor data. A* is widely used in motion planning due to its efficiency and optimality (it never overestimates the cost). Motion planning time = search time + collision checking time (~90%). Therefore, the author proposed lazy A* (which finds an optimal path in an unconstrained situation, goes over collision checking while on the path, and re-searches a path if a collision is encountered). Collect a dataset before embarking on research. To understand how to manipulate different kinds of foods, the author created a food manipulation taxonomy. Choose hardware components for real-world deployability. Use deformation of points on a gel coupled with computer vision algorithms to measure shear force Add structure to machine learning algorithms to overcome a lack of data. If integrating multimodal data sources, think of where to integrate as the size or magnitude of different data may be inconsistent. A \"bandit\" algorithm is an RL algorithm where we utilize partial feedback of one step in the decision-making process, unlike conventional RL algorithms with \"episodes\" spanning multiple steps. Audience question: How to stay up-to-date with the state-of-the-art (especially in the fast-changing landscape of machine learning)? One of the main tasks of faculty life Look at titles and abstracts of publications in all well-known conferences. Organize reading groups and reading sessions. Interact with known other research groups. Derek Dreyer: How to write papers and give talks that people can follow Many papers suffer from the TMI (too much information) problem. Aim at giving constructive principles that are easy to check and fix. A paper is different from a textbook - people aren't as committed to reading a paper as they are to reading a textbook. A good but not interesting paper tends to get a \"B\" or a \"weak accept.\" Putting the Related Work section at the front (as opposed to in the back before the Conclusion) may hinder unfamiliar authors from understanding your work. Most people don't listen to talks to determine whether they should read a paper. Instead, they listen to talks to discuss with others. The main goal of a talk is to give people positive feelings about your work. A talk should only cover the intro and key ideas sections of the corresponding paper. The key ideas should be the high point in your talk before presenting the takeaway messages. Add visual elements to emphasize one point per slide. Use smooth animations to help the listener follow.","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"From the Fourier Series to the Fourier Transform to the Discrete-time Fourier Transform: Demystifying the Formulas","slug":"From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas","date":"2023-09-04T04:00:00.000Z","updated":"2025-08-13T04:31:00.453Z","comments":true,"path":"2023/09/04/From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas/","link":"","permalink":"https://jifengwu2k.github.io/2023/09/04/From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas/","excerpt":"","text":"In realms as broad as electrical engineering, acoustics, optics, signal processing, quantum mechanics, and econometrics, the Fourier Series, Fourier Transform, and Discrete-time Fourier Transform play a pivotal role in analyzing signals by allowing us to decompose them into simpler components. Many articles present their formulas or dive into their intuition and applications. However, what seems to be missing is a blog post that explains the derivation of their formulas in a way that is both clear and accessible, requiring no more than a rudimentary understanding of calculus. Fourier Series Standard Form of the Fourier Series Our journey begins with the Fourier Series - a method to represent periodic functions as a sum of sine and cosine waves. Let \\(x(t)\\) be a periodic function with period \\(T\\). The standard form of the Fourier series for \\(x(t)\\) is given by: \\[x(t) = \\frac{a_0}{2} + a_1 \\cos{\\frac{2\\pi}{T} t} + b_1 \\sin{\\frac{2\\pi}{T} t} + a_2 \\cos{\\frac{4\\pi}{T} t} + b_2 \\sin{\\frac{4\\pi}{T} t} + \\dots \\] To solve for \\(a_0, a_1, b_1, \\dots\\), we first observe the . Thus, we can multiply both sides of the equation by \\(cos{\\frac{2k\\pi}{T} t}\\) or \\(\\sin{\\frac{2k\\pi}{T} t}\\) \\((k \\in \\{0, 1, 2, \\dots, n\\})\\), and then integrate over one period \\([-\\frac{T}{2}, \\frac{T}{2})\\) to obtain: \\[a_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) \\cos{\\frac{2k\\pi}{T} t} dt}\\] \\[b_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) \\sin{\\frac{2k\\pi}{T} t} dt}\\] Exponential Form of the Fourier Series Using Euler's formula \\(e^{ix} = \\cos{x} + i \\sin{x}\\), we can derive: \\[\\cos{x} = \\frac{e^{ix} + e^{-ix}}{2}\\] \\[\\sin{x} = -i \\frac{e^{ix} - e^{-ix}}{2}\\] Substituting these representations of \\(\\cos{x}\\) and \\(\\sin{x}\\) into \\(a_k\\) and \\(b_k\\), we get: \\[a_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) \\frac{e^{i \\frac{2k\\pi}{T} t} + e^{-i \\frac{2k\\pi}{T} t}}{2} dt}\\] \\[b_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{-i x(t) \\frac{e^{i \\frac{2k\\pi}{T} t} - e^{-i \\frac{2k\\pi}{T} t}}{2} dt}\\] And: \\[a_k \\cos{\\frac{2k\\pi}{T} t} + b_k \\sin{\\frac{2k\\pi}{T} t} = a_k \\frac{e^{i \\frac{2k\\pi}{T} t} + e^{-i \\frac{2k\\pi}{T} t}}{2} - i b_k \\frac{e^{i \\frac{2k\\pi}{T} t} - e^{-i \\frac{2k\\pi}{T} t}}{2} = \\frac{a_k - i b_k}{2} e^{i \\frac{2k\\pi}{T} t} + \\frac{a_k + i b_k}{2} e^{-i \\frac{2k\\pi}{T} t}\\] And: \\[\\frac{a_k - i b_k}{2} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] \\[\\frac{a_k + i b_k}{2} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{i \\frac{2k\\pi}{T} t} dt}\\] Furthermore, if we let: \\[c_k = \\frac{a_k - i b_k}{2} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] Substituting \\(k \\leftarrow -k\\) into the expression for \\(c_k\\), we will obtain: \\[c_{-k} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{i \\frac{2k\\pi}{T} t} dt} = \\frac{a_k + i b_k}{2}\\] Thus: \\[a_k \\cos{\\frac{2k\\pi}{T} t} + b_k \\sin{\\frac{2k\\pi}{T} t} = \\frac{a_k - i b_k}{2} e^{i \\frac{2k\\pi}{T} t} + \\frac{a_k + i b_k}{2} e^{-i \\frac{2k\\pi}{T} t} = c_k e^{i \\frac{2k\\pi}{T} t} + c_{-k} e^{-i \\frac{2k\\pi}{T} t}\\] And by substituting \\(k \\leftarrow 0\\) into the expression for \\(c_k\\), we get: \\[c_0 = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) dt} = \\frac{a_0}{2}\\] Therefore: \\[x(t) = \\frac{a_0}{2} + \\sum_{k=1}^{n}{(a_k \\cos{\\frac{2k\\pi}{T} t} + b_k \\sin{\\frac{2k\\pi}{T} t})} = c_0 + \\sum_{k=1}^{n}{(c_k e^{i \\frac{2k\\pi}{T} t} + c_{-k} e^{-i \\frac{2k\\pi}{T} t})} = \\sum_{k=-n}^{n}{c_k e^{i \\frac{2k\\pi}{T} t}}\\] Where: \\[c_k= \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] This is the exponential form of the Fourier series. It is more concise than the standard form of the Fourier series and is used more often in practice. Fourier Transform The Fourier transform is a generalization of the Fourier series, which can analyze the effect of a frequency in any function (which may not necessarily be a periodic function). In this section, we will present how it can be derived from the exponential form of the Fourier series. Given a periodic function \\(x(t)\\) with period \\(T\\), the exponential form of the Fourier series of \\(x(t)\\) is as follows: \\[x(t) = \\sum_{k=-n}^{n}{c_k e^{i \\frac{2k\\pi}{T} t}}\\] Where: \\[c_k= \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] Let's say that the period \\(T\\) is associated with a frequency known as the fundamental frequency \\(f_0 = \\frac{1}{T}\\). Given \\(f_0\\), we can rewrite the previous Fourier series as: \\[x(t) = \\sum_{k=-n}^{n}{c_k e^{i 2\\pi k f_0 t}}\\] Where: \\[c_k= \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i 2\\pi k f_0 t} dt}\\] For a non-periodic function, we can consider it as a periodic function with \\(T \\rightarrow +\\infty\\). In this case, the fundamental frequency \\(f_0\\) is an infinitesimal quantity; therefore, we can consider that any frequency \\(f\\) can be expressed as an integer multiple of the fundamental frequency, and the difference between two neighboring frequencies is the fundamental frequency \\(f_0\\). In this case, the fundamental frequency \\(f_0\\) can be expressed as a differential of the frequency \\(f\\), i.e., \\(df\\). In this case, for a possibly non-periodic function \\(x(t)\\): \\[x(t) = \\sum_{k=-\\infty}^{\\infty}{c_k e^{i 2\\pi k (df)t}}\\] \\[c_k= (df) \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi k (df) t} dt}\\] Thus, \\(x(t)\\) can be represented as: \\[x(t) = \\sum_{k=-\\infty}^{\\infty}{[(df) \\cdot \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi k (df) t} dt} \\cdot e^{i 2\\pi k (df) t}]}\\] By considering \\(f \\leftarrow k (df)\\), we can transform the summation into a definite integral: \\[x(t)= \\int_{-\\infty}^{\\infty}{ [(\\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt}) e^{i 2\\pi f t}] df}\\] Let: \\[X(f) = \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt}\\] Then \\(x(t)\\) can be represented as: \\[x(t) = \\int_{-\\infty}^{\\infty}{ X(f) e^{i 2\\pi f t} df}\\] These two equations are very important. If we know \\(x(t)\\) (i.e., the value of \\(x(t)\\) at any time \\(t\\)), through \\(X(f) = \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt}\\), we can compute the relative magnitude of any frequency \\(f\\) over the whole time period. At the same time, if we know \\(X(f)\\) (i.e., the relative magnitude of any frequency \\(f\\) over the whole time period), by means of \\(x(t) = \\int_{-\\infty}^{\\infty}{ X(f) e^{i 2\\pi f t} df}\\), we can calculate the value of \\(x(t)\\) at any time \\(t\\). We refer to \\(X(f)\\) as the Fourier transform of \\(x(t)\\), also known as the spectrum of \\(x(t)\\), and to \\(x(t)\\) as the inverse Fourier transform of \\(X(f)\\). Discrete-time Fourier Transform When we process signals with computers, as computers cannot store a continuous infinite function, we usually take \\(N\\) samples of the original signal \\(x(t)\\) at a certain time interval \\(\\Delta t\\), obtaining an array \\(x[0:N-1]\\). Using \\(x[0:N-1]\\) to estimate the Fourier transform \\(X(f)\\) of the sampled function \\(x(t)\\), we get: \\[X(f) = \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt} \\approx \\int_{0}^{N \\Delta t}{x(t) e^{-i 2\\pi f t} dt} \\approx \\sum_{m=0}^{N - 1}{x(m \\Delta t) e^{-i 2\\pi f m \\Delta t}}\\] If we assume these samples have spanned a period of the original signal, e.g. \\(T = N \\Delta t\\), and that we only consider frequencies satisfying \\(f = k \\frac{1}{N \\Delta t} (k \\in \\{0, 1, \\dots, N - 1\\})\\), we get: \\[X(k \\frac{1}{N \\Delta t}) \\approx \\sum_{n=0}^{N - 1}{x(n \\Delta t) e^{-i 2\\pi k \\frac{1}{N \\Delta t} n \\Delta t}} = \\sum_{n=0}^{N - 1}{x(n \\Delta t) e^{-i 2\\pi \\frac{k}{N} n}} = \\sum_{n=0}^{N - 1}{x[n] e^{-i 2\\pi \\frac{k}{N} n}}\\] Let: \\[X[k] = \\sum_{n=0}^{N - 1}{x[n] e^{-i 2\\pi \\frac{k}{N} n}} (k \\in \\{0, 1, \\dots, N - 1\\})\\] We call such an array of \\(N\\) discrete numbers \\(X[0:N-1]\\) the discrete-time Fourier transform of \\(x[0:N-1]\\), which is a discrete frequency domain representation of \\(x[0:N-1]\\). Using \\(X[0:N-1]\\), we can restore \\(x[0:N-1]\\): \\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N - 1}{X[k] e^{i 2\\pi \\frac{k}{N} n}} (n \\in \\{0, 1, \\dots, N - 1\\})\\] We call \\(x[0:N-1]\\) the inverse discrete-time Fourier transform of \\(X[0:N-1]\\). This is analogous to \\(X(f)\\) being the Fourier transform of \\(x(t)\\) and \\(x(t)\\) being the inverse Fourier transform of \\(X(f)\\) in the continuous case.","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://jifengwu2k.github.io/categories/Mathematics/"}],"tags":[]},{"title":"On Convolutional Neural Networks and Photographic Lenses","slug":"On-Convolutional-Neural-Networks-and-Photographic-Lenses","date":"2023-08-24T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2023/08/24/On-Convolutional-Neural-Networks-and-Photographic-Lenses/","link":"","permalink":"https://jifengwu2k.github.io/2023/08/24/On-Convolutional-Neural-Networks-and-Photographic-Lenses/","excerpt":"","text":"Convolutional neural networks are camera lenses to a computer. A convolutional neural network A camera lens The analogy does not stop at the point that both compress visual information: The evolution of convolutional neural network architectures resembles the evolution of camera lenses. The P-R curve showing the performance of a convolutional neural network is strikingly similar to the MTF curve evaluating lens performance. A P-R curve An MTF curve","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Our Motivation for Maintaining Our Blog","slug":"Our-Motivation-for-Maintaining-Our-Blog","date":"2023-08-16T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2023/08/16/Our-Motivation-for-Maintaining-Our-Blog/","link":"","permalink":"https://jifengwu2k.github.io/2023/08/16/Our-Motivation-for-Maintaining-Our-Blog/","excerpt":"","text":"今之博客，乃昔（初高中）之错题本之翻版也。昔之背景知识、解题思路、高效算法，今之认识、洞见，皆为辛苦求索所得，故笔录之，以期日积月累，唯“应试”“科研”之直接目的异也。 This blog is a replica of our previous \"problem books\" used for junior and senior high school. In the past, we would record background knowledge, problem solving ideas, and efficient algorithms. Today, we would note down understandings and insights. These are all the result of the hard work of searching and exploring, and we record them down in order to gradually accumulate our knowledge and understanding. Only the direct purpose has been changed from \"preparing for a test\" to \"doing scientific research\".","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"}],"tags":[]},{"title":"Timetable of Well-known Conferences in Different Subdomains of Computer Science","slug":"Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science","date":"2023-08-16T04:00:00.000Z","updated":"2025-08-13T04:31:00.460Z","comments":true,"path":"2023/08/16/Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science/","link":"","permalink":"https://jifengwu2k.github.io/2023/08/16/Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science/","excerpt":"","text":"We have compiled a timetable of well-known conferences in different subdomains of computer science based on the Class A and Class B conferences in \"Directory of International Academic Conferences and Journals Recommended by the Chinese Computer Society\". Although the precise start dates of each conference vary year by year, the provided start dates provide a general guideline on the relative order of the conferences throughout each year. Name Start Date Subdomain CIDR 01/08/23 Databases/Data Mining/Information Retrieval GROUP 01/08/23 Human Computer Interaction and Ubiquitous Computing POPL 01/15/23 Software Engineering/Systems Software/Programming Languages VMCAI 01/15/23 Software Engineering/Systems Software/Programming Languages HiPEAC 01/16/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems SODA 01/22/23 Theoretical Computer Science PPoPP 02/05/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems AAAI 02/07/23 Artificial Intelligence FPGA 02/12/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems FAST 02/21/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems HPCA 02/25/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CGO 02/25/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems NDSS 02/27/23 Security WSDM 02/27/23 Databases/Data Mining/Information Retrieval FM 03/07/23 Software Engineering/Systems Software/Programming Languages PERCOM 03/13/23 Human Computer Interaction and Ubiquitous Computing FSE 03/20/23 Security SANER 03/21/23 Software Engineering/Systems Software/Programming Languages DCC 03/21/23 Computer Graphics/Multimedia ASPLOS 03/25/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems VR 03/25/23 Computer Graphics/Multimedia IUI 03/27/23 Human Computer Interaction and Ubiquitous Computing ICDT 03/28/23 Databases/Data Mining/Information Retrieval EDBT 03/28/23 Databases/Data Mining/Information Retrieval ICDE 04/03/23 Databases/Data Mining/Information Retrieval RECOMB 04/16/23 Interdisciplinary/Emerging DATE 04/17/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems NSDI 04/17/23 Computer Networks DASFAA 04/17/23 Databases/Data Mining/Information Retrieval ETAPS 04/22/23 Software Engineering/Systems Software/Programming Languages EUROCRYPT 04/23/23 Security CHI 04/23/23 Human Computer Interaction and Ubiquitous Computing SDM 04/27/23 Databases/Data Mining/Information Retrieval WWW 04/30/23 Interdisciplinary/Emerging I3D 05/03/23 Computer Graphics/Multimedia PKC 05/07/23 Security EG 05/08/23 Computer Graphics/Multimedia RTAS 05/09/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems IPSN 05/09/23 Computer Networks HSCC 05/09/23 Theoretical Computer Science EuroSys 05/09/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ICSE 05/14/23 Software Engineering/Systems Software/Programming Languages IPDPS 05/15/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ICPC 05/15/23 Software Engineering/Systems Software/Programming Languages INFOCOM 05/17/23 Computer Networks MSST 05/22/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems S&amp;P 05/22/23 Security ICRA 05/29/23 Artificial Intelligence AAMAS 05/29/23 Artificial Intelligence ICASSP 06/04/23 Computer Graphics/Multimedia ECSCW 06/05/23 Human Computer Interaction and Ubiquitous Computing NOSSDAV 06/10/23 Computer Networks CAiSE 06/12/23 Software Engineering/Systems Software/Programming Languages SoCG 06/12/23 Theoretical Computer Science ICMR 06/12/23 Computer Graphics/Multimedia EuroVis 06/12/23 Computer Graphics/Multimedia SPAA 06/16/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ISCA 06/17/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems PLDI 06/17/23 Software Engineering/Systems Software/Programming Languages LCTES 06/17/23 Software Engineering/Systems Software/Programming Languages MobiSys 06/18/23 Computer Networks SIGMOD 06/18/23 Databases/Data Mining/Information Retrieval PODS 06/18/23 Databases/Data Mining/Information Retrieval CVPR 06/18/23 Artificial Intelligence SIGMETRICS 06/19/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems IWQoS 06/19/23 Computer Networks PODC 06/19/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems HPDC 06/20/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems STOC 06/20/23 Theoretical Computer Science ICS 06/21/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems HotOS 06/22/23 Software Engineering/Systems Software/Programming Languages LICS 06/26/23 Theoretical Computer Science DSN 06/27/23 Security EGSR 06/28/23 Computer Graphics/Multimedia CADE/IJCAR 07/01/23 Theoretical Computer Science ICWS 07/02/23 Software Engineering/Systems Software/Programming Languages SGP 07/03/23 Computer Graphics/Multimedia SAT 07/04/23 Theoretical Computer Science SPM 07/05/23 Computer Graphics/Multimedia ICAPS 07/08/23 Artificial Intelligence DAC 07/09/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CSFW 07/09/23 Security ACL 07/09/23 Artificial Intelligence USENIX ATC 07/10/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems OSDI 07/10/23 Software Engineering/Systems Software/Programming Languages ICALP 07/10/23 Theoretical Computer Science ICME 07/10/23 Computer Graphics/Multimedia COLT 07/12/23 Artificial Intelligence ISSTA 07/17/23 Software Engineering/Systems Software/Programming Languages ECOOP 07/17/23 Software Engineering/Systems Software/Programming Languages CAV 07/17/23 Theoretical Computer Science CCC 07/17/23 Theoretical Computer Science ICCBR 07/17/23 Artificial Intelligence ICDCS 07/18/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems SIGIR 07/23/23 Databases/Data Mining/Information Retrieval ICML 07/23/23 Artificial Intelligence ISMB 07/23/23 Interdisciplinary/Emerging CogSci 07/26/23 Interdisciplinary/Emerging UAI 07/31/23 Artificial Intelligence SCA 08/04/23 Computer Graphics/Multimedia SIGKDD 08/06/23 Databases/Data Mining/Information Retrieval SIGGRAPH 08/06/23 Computer Graphics/Multimedia ICPP 08/07/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems USENIX Security 08/09/23 Security CRYPTO 08/19/23 Security IJCAI 08/19/23 Artificial Intelligence HOT CHIPS 08/27/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CP 08/27/23 Software Engineering/Systems Software/Programming Languages VLDB 08/28/23 Databases/Data Mining/Information Retrieval KR 09/02/23 Artificial Intelligence RE 09/04/23 Software Engineering/Systems Software/Programming Languages ICFP 09/04/23 Software Engineering/Systems Software/Programming Languages ESA 09/04/23 Theoretical Computer Science SIGCOMM 09/10/23 Computer Networks CHES 09/10/23 Security SECON 09/11/23 Computer Networks ASE 09/11/23 Software Engineering/Systems Software/Programming Languages CODES+ISSS 09/17/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems EMSOFT 09/17/23 Interdisciplinary/Emerging ECML-PKDD 09/18/23 Databases/Data Mining/Information Retrieval CONCUR 09/19/23 Theoretical Computer Science ESORICS 09/25/23 Security SRDS 09/25/23 Security MobileHCI 09/26/23 Human Computer Interaction and Ubiquitous Computing ECAI 09/30/23 Artificial Intelligence MoDELS 10/01/23 Software Engineering/Systems Software/Programming Languages ICSME 10/01/23 Software Engineering/Systems Software/Programming Languages MobiCom 10/02/23 Computer Networks ICCV 10/02/23 Artificial Intelligence ECCV 10/02/23 Artificial Intelligence ITC 10/08/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems UbiComp 10/08/23 Human Computer Interaction and Ubiquitous Computing ESEM 10/09/23 Software Engineering/Systems Software/Programming Languages ISSRE 10/09/23 Software Engineering/Systems Software/Programming Languages ICNP 10/10/23 Computer Networks PG 10/10/23 Computer Graphics/Multimedia CSCW 10/14/23 Human Computer Interaction and Ubiquitous Computing RAID 10/16/23 Security ISMAR 10/16/23 Computer Graphics/Multimedia PACT 10/21/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CIKM 10/21/23 Databases/Data Mining/Information Retrieval OOPSLA 10/22/23 Software Engineering/Systems Software/Programming Languages SAS 10/22/23 Software Engineering/Systems Software/Programming Languages IEEE VIS 10/22/23 Computer Graphics/Multimedia MobiHoc 10/23/23 Computer Networks SOSP 10/23/23 Software Engineering/Systems Software/Programming Languages IMC 10/24/23 Computer Networks MICRO 10/28/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ICCAD 10/29/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ACM MM 10/29/23 Computer Graphics/Multimedia UIST 10/29/23 Human Computer Interaction and Ubiquitous Computing SoCC 10/30/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CLUSTER 10/31/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ISS 11/05/23 Human Computer Interaction and Ubiquitous Computing ICCD 11/06/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ISWC 11/06/23 Databases/Data Mining/Information Retrieval FOCS 11/06/23 Theoretical Computer Science SC 11/12/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems SenSys 11/12/23 Computer Networks Performance 11/14/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CCS 11/26/23 Security ICSOC 11/28/23 Software Engineering/Systems Software/Programming Languages TCC 11/29/23 Security ICDM 12/01/23 Databases/Data Mining/Information Retrieval FSE/ESEC 12/03/23 Software Engineering/Systems Software/Programming Languages ACSAC 12/04/23 Security ASIACRYPT 12/04/23 Security CoNEXT 12/05/23 Computer Networks RTSS 12/05/23 Interdisciplinary/Emerging BIBM 12/05/23 Interdisciplinary/Emerging EMNLP 12/06/23 Artificial Intelligence NeurIPS 12/10/23 Artificial Intelligence Middleware 12/11/23 Software Engineering/Systems Software/Programming Languages","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"ISSTA/ECOOP 2023 Observations and Gained Insights","slug":"ISSTA-ECOOP-2023-Observations-and-Gained-Insights","date":"2023-07-21T04:00:00.000Z","updated":"2025-08-13T04:31:00.453Z","comments":true,"path":"2023/07/21/ISSTA-ECOOP-2023-Observations-and-Gained-Insights/","link":"","permalink":"https://jifengwu2k.github.io/2023/07/21/ISSTA-ECOOP-2023-Observations-and-Gained-Insights/","excerpt":"","text":"Mon 17 Jul Session 1 FUZZING at Amazon Auditorium (Gates G20) Welcome and Introductions The following reviewing criteria for workshop papers can serve as a guide for us in writing papers: Is the problem that is addressed significant for research or practice? Are the contributions (technique, hypothesis, or evaluation) over existing work sufficient? Is the methodology (experimental setup or protocol) specified to validate the claims or hypotheses reasonable? Can an independent research group reproduce the results, given the proposed methodology (experimental setup) Establish significance, novelty, and soundness, even if results do not show a large performance gain. Inspect unexpected results, such as why results are negative. Three Colours of Fuzzing: Reflections and Open Challenges - Cristian Cadar Why does fuzzing keep finding bugs in production software? LOTS of code is added or modified without being tested. (Covrig: A framework for the analysis of code, test, and coverage evolution in real software) Fuzzing is not automated enough. Fuzz targets (test drivers) need to be manually specified. There is much work on improving fuzzing heuristics, but more work is required for test driver generation. An ideal test case should benefit quality assurance, debugging aid, and documentation. They should target human users, and be small, fast, readable, and well-documented. However, automatically generated test suites, such as those generated by fuzzers, need to be improved in these aspects. They achieve high code coverage, excel at finding generic/crash bugs in general software that may not be very realistic (assertion faults, crashes, undefined behavior) but do not achieve high feature coverage and are poor at detecting logical bugs in software for specific domains. On the other hand, such fuzzing makes it appropriate for use cases outside of security and software testing that require a novel search to find diverse failing inputs, corner cases, and loopholes, such as ML models and even investigating legal documents (Rohan). Developers tend to be afraid of using fuzzers as they don't understand them or think of them as security tools, in contrast to a standard testing tool. Allowing fuzzing to operate at a higher declarative level and combining fuzzing with domain-specific specification languages would be beneficial. Sound fuzzer evaluation is challenging. Well-designed experiment methodology. Huge variance due to randomness, demanding substantial computation resources (e.g., repeat 20x, 24 hours, X fuzzers, Y programs) Thu 20 Jul Keynotes at Amazon Auditorium (Gates G20) Paper Readinging Statistics 44/97 papers accepted Round 1: 40 submitted, 17 accepted, 9 rejected, 14 resubmit Round 2: 57 submitted (11 resubmissions), 27 accepted, 18 rejected, 12 resubmit Dahl-Nygaard Senior Prize: Safe Journeys into the Unknown - Object Capabilities - Sophia Drossopoulou Think of an exciting question, such as various language features, and look into it as a research question (An Abstract Model of Java Dynamic Linking and Loading, A Flexible Model for Dynamic Linking in Java and C#). The key for program verification is to develop formal models for a (subset) of a language, make it small and simple, and gradually expand (Java is type safe -- probably). Actively start collaborations (Ownership, encapsulation and the disjointness of type and effect). ISSTA 10: Test OptimizationsISSTA Technical Papers at Smith Classroom (Gates G10) June: A Type Testability Transformation for Improved ATG Performance Automatically generating unit tests is a powerful approach to exercising complex software. However, existing methods frequently fail to deliver appropriate input values, like strings, capable of bypassing domain-specific sanity checks. For instance, Randoop commonly uses \"hi!\" as a value. (Saying 'Hi!' is not enough: Mining inputs for effective test generation) Pattern-Based Peephole Optimizations with Java JIT Tests To demonstrate the advantage of JOG over hand-written peephole optimizations in terms of ease of writing, existing hand-written peephole optimizations are compared, and number of characters and number of lines are used as metrics. GPUHarbor: Testing GPU Memory Consistency at Large (Experience Paper) The tool has been implemented as a Web app using WebGPU to access the GPU, allowing the audience to try it out during the talk. Keynote - ISSTA'24 Preview - ClosingKeynotes at Amazon Auditorium (Gates G20) Machine Learning for Software Engineering What underlies the success of machine learning for software engineering? The naturalness of code. i++ is predictable given for (i = 0; i &lt; 10;, and backward() is predictable given loss. The bimodality of code, or code contains natural language. Q. How do I get a platform-dependent new line character? A. public static String getPlatformLineSeparator() { return System.getProperty(\"line.separator\"); } Code has predictable properties. Given ... = x.weight * x.height, what is the ??? in ... = y.weight * ???? Large amount of data (GitHub repos with code, version history, and commit logs, StackOverflow questions and answers, internal corpora in companies, etc.)","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"Batch Killing Processes Looked up Through ps -aux | grep <process_name>","slug":"Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name","date":"2023-06-10T04:00:00.000Z","updated":"2025-08-13T04:31:00.444Z","comments":true,"path":"2023/06/10/Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name/","link":"","permalink":"https://jifengwu2k.github.io/2023/06/10/Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name/","excerpt":"","text":"Sometimes we acidentally spawn a series of processes, and we want to kill them. We can look up their pid's through ps -aux | grep &lt;process_name&gt; (as shown below) and manually run the kill command to kill each process by providing its pid, but how can we automate this tedious task? 12345678910$ ps aux | grep pipreqsjifengwu 58180 0.0 0.1 46440 27332 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/spectria/tildes/tildes --mode no-pinjifengwu 58205 0.1 0.1 48140 29392 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/PnX-SI/GeoNature --mode no-pinjifengwu 58224 5.7 0.2 51856 33108 pts/0 T 13:38 0:23 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/fabiandevia/home --mode no-pinjifengwu 58267 4.4 0.2 57880 38204 pts/0 T 13:39 0:17 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/377312117/gitproject --mode no-pinjifengwu 58272 2.3 0.2 53756 34252 pts/0 T 13:39 0:08 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/crazyfish1111/home --mode no-pinjifengwu 58282 0.1 0.1 47840 28132 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/Piratenpartei/ekklesia-portal --mode no-pinjifengwu 58295 0.1 0.1 48220 28492 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/jauhararifin/ugrade/server --mode no-pinjifengwu 58659 0.3 0.1 48608 29324 pts/0 T 13:41 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/klen/pylama --mode no-pinjifengwu 59564 0.0 0.0 19612 2516 pts/2 S+ 13:45 0:00 grep --color=auto pipreqs First, we can add grep -v grep to the pipe to hide the grep processes from the output: 123456789$ ps aux | grep pipreqs | grep -v grepjifengwu 58180 0.0 0.1 46440 27332 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/spectria/tildes/tildes --mode no-pinjifengwu 58205 0.1 0.1 48140 29392 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/PnX-SI/GeoNature --mode no-pinjifengwu 58224 5.6 0.2 51856 33108 pts/0 T 13:38 0:23 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/fabiandevia/home --mode no-pinjifengwu 58267 4.4 0.2 57880 38204 pts/0 T 13:39 0:17 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/377312117/gitproject --mode no-pinjifengwu 58272 2.2 0.2 53756 34252 pts/0 T 13:39 0:08 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/crazyfish1111/home --mode no-pinjifengwu 58282 0.1 0.1 47840 28132 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/Piratenpartei/ekklesia-portal --mode no-pinjifengwu 58295 0.1 0.1 48220 28492 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/jauhararifin/ugrade/server --mode no-pinjifengwu 58659 0.3 0.1 48608 29324 pts/0 T 13:41 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/klen/pylama --mode no-pin Then, we can add awk '&#123;print $2&#125;' to the pipe to invoke awk to trim the second space-delimited component (which in this case is the pid). Now we have a list of the pid's of the processes we want to kill: 123456789$ ps aux | grep pipreqs | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;5818058205582245826758272582825829558659 Finally, we can iterate over the pid's in a for-loop to kill them. 1234$ for pid in $(ps aux | grep pipreqs | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;)&gt; do&gt; kill -15 $pid&gt; done References https://www.baeldung.com/linux/grep-exclude-ps-results https://stackoverflow.com/questions/46008880/how-to-always-cut-the-pid-from-ps-aux-command","categories":[{"name":"System Administration","slug":"System-Administration","permalink":"https://jifengwu2k.github.io/categories/System-Administration/"}],"tags":[]},{"title":"Parsing Command-line Options in Shell Scripts Using `getopts`","slug":"Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts","date":"2023-06-09T04:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2023/06/09/Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts/","link":"","permalink":"https://jifengwu2k.github.io/2023/06/09/Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts/","excerpt":"","text":"What is getopts getopts is a built-in Unix shell command for parsing command-line options. It is a wrapper around getopt, a POSIX C library function used to parse command-line options of the Unix/POSIX style. Specifically: Options are single-character alphanumerics preceded by a - (hyphen-minus) character, i.e. -a. -b, -c. Options can take an argument or none. Multiple options can be chained together, as long as the non-last ones are not argument-taking. If -a and -b take no arguments while -c takes an argument, -abc foo is the same as -a -c -e foo, but -bca is not the same as -b -c a due to the preceding rule. When an option takes an argument, this can be in the same token or in the next one. In other words, if -c takes an argument, -cfoo is the same as -c foo. optstring's Both getopt and getopts specifies specify options using a optstring. Specifically: Begin an optstring with :. To specify an option that does not take an argument, append its name to the optstring. To specify an option that takes an argument, append its name and : to the optstring. For example, the optstring that specifies two options -a, -b that do not take arguments and two options -c, -d that take arguments is :abc:d:. Using getopts in a Shell Script In Shell scripts, getopts invoked with an optstring is used with a while-loop to parse command-line options. Say that our Shell script test_getopts.sh accepts two options -a, -b that do not take arguments and two options -c, -d that take arguments. Our Shell script can look like this: 12345678910111213141516171819202122232425#!/bin/shwhile getopts &#x27;:abc:d:&#x27; namedo case $name in a) echo &quot;You provided option -a&quot; ;; b) echo &quot;You provided option -b&quot; ;; c) echo &quot;You provided option -c with argument $OPTARG&quot; ;; d) echo &quot;You provided option -d with argument $OPTARG&quot; ;; :) echo &quot;Option -$OPTARG requires an argument&quot; ;; ?) echo &quot;You provided an invalid option -$OPTARG&quot; ;; esacdone Here, getopts is invoked with the optstring for specifying our options, :abc:d:. In each iteration of the while-loop, the next option is parsed and the Shell variables name and OPTARG are set to different values based on different conditions we may encounter. If a valid option is detected and that option does not take an argument, the Shell variable name is set to the name of the option. If a valid option is detected and that option takes an argument: If we have provided an argument, the Shell variable name is set to the name of the option, and the Shell variable OPTARG is set to the value of the argument. If we haven't provided an argument, the Shell variable name is set to :, and the Shell variable OPTARG is set to the name of the argument. If an invalid option is detected, the Shell variable name is set to ?, and the Shell variable OPTARG is set to the name of the argument. We can see getopts at work by providing different command-line options when invoking our Shell script. Providing no command-line options: 1$ sh test_getopts.sh Providing option -a that do not take arguments: 12$ sh test_getopts.sh -aYou provided option -a Providing option -a that do not take arguments twice: 123456$ sh test_getopts.sh -a -aYou provided option -aYou provided option -a$ sh test_getopts.sh -aaYou provided option -aYou provided option -a Providing option -c that takes an argument with an argument foo: 12$ sh test_getopts.sh -c fooYou provided option -c with argument foo Providing option -c that takes an argument with an argument foo twice: 123$ sh test_getopts.sh -c foo -c barYou provided option -c with argument fooYou provided option -c with argument bar Providing option -c that takes an argument without an argument: 12$ sh test_getopts.sh -cOption -c requires an argument Providing an invalid argument -e: 12$ sh test_getopts.sh -eYou provided an invalid option -e References https://en.wikipedia.org/wiki/Getopts https://pubs.opengroup.org/onlinepubs/9699919799/utilities/getopts.html https://en.wikipedia.org/wiki/Getopt https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap12.html","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Bash","slug":"Software-Design/Bash","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Bash/"}],"tags":[]},{"title":"PNW PLSE Workshop 2023 Observations and Gained Insights","slug":"PNW-PLSE-Workshop-2023-Observations-and-Gained-Insights","date":"2023-05-09T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2023/05/09/PNW-PLSE-Workshop-2023-Observations-and-Gained-Insights/","link":"","permalink":"https://jifengwu2k.github.io/2023/05/09/PNW-PLSE-Workshop-2023-Observations-and-Gained-Insights/","excerpt":"","text":"10:30 - Talks Linear Types for Systems Verification It is a good idea to embed verification information in type system of a programming language. Verified Program Construction Program verification is hard per se. Proofs are brittle. Existing techniques provide poor support for commonly-used datatypes such as vectors, sets, and maps. Partial verification is important due to the complexity of programs. Program verification for general-purpose programming languages and frameworks for general-purpose programming languages is tedious compared to focusing on a DSL. The direction for program verification should be verified program construction. Be aware of the pain point you are trying to solve and the day-to-day engineering reality in real-world software development. 13:00 - Keynote: Patrick Lam Hot Takes on Machine Learning for Program Analysis A crucial step in Machine Learning for Program Analysis is deciding what things could be used as features based on experience. Generative AI can replace junior developers doing raw coding instead of contextual work. 13:30 - Lightning Talks Lakeroad: Hardware Compilation via Program Synthesis If you don't use DSLs, FPGAs bring crappy performance running programs in general-purpose programming languages. Checked C Retrofitting verification into non-verified languages is an arduous task. 15:15 - Talks An Anti-Capitalist, Multicultural, Accessible Programming Language An event-based language enabling time-traveling to all points in program execution history would greatly benefit debugging.","categories":[{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"}],"tags":[]},{"title":"Paper Reading: Sized Types","slug":"Paper-Reading-Sized-Types","date":"2023-03-25T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2023/03/25/Paper-Reading-Sized-Types/","link":"","permalink":"https://jifengwu2k.github.io/2023/03/25/Paper-Reading-Sized-Types/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary You can check the presentation that I made for this paper in this GitHub repository. Critique Although I liked the idea of Sized Types proposed in the motivation, this paper was difficult for me to grasp, and after spending days reading it, there are still sections which I am confused about. I have summarized my understanding of this paper in the uploaded PDF, and I will discuss my thoughts here. I really like the idea of Sized Types that they can be used to prove data computations terminate and codata computations are productive using the same formalization. Apparently, the requirement that size indexes in Sized Types be natural number size variables, the special index \\(\\omega\\), or linear functions of the size variables facilitates generating constraints in the type checking algorithm that can be solved by a constraint solver (e.g. an SMT solver). Although this may lead to overapproximation in certain scenarions (for example, representing the type of the factorial function), over all, I consider it to be a good balance point between expressiveness and usability. 3.2 Semantics of Expressions, 3.3 The Universe of Types, 3.4 Continuity and Ordinals, 3.5 Semantics of Types, and 3.7 \\(\\omega\\)-Types used a lot of concepts before properly introducing them, and I couldn't understand this part. The example presented to demonstrate the type checking algorithm involves generating constraints. However, only the generated constraints are presented, while how the constraints are generated and what each symbol in the constraints stand for with regards to the aforementioned AST nodes is unknown.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: Refinement Types","slug":"Paper-Reading-Refinement-Types","date":"2023-03-19T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2023/03/19/Paper-Reading-Refinement-Types/","link":"","permalink":"https://jifengwu2k.github.io/2023/03/19/Paper-Reading-Refinement-Types/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary This paper presents a clear and organized guide to refinement type systems by condensing the extensive literature on the topic and demonstrating the implementation of a refinement type checker. It first states the motivation for requirement types, a history of requirement types, and refinement logic, which is a logic system used in the proposed refinement type checker. The rest of the paper shows the implementation of a refinement type checker through a series of programming languages, beginning with simply-typed lambda calculus and incrementally adding additional features. This approach is influenced by the nanopass framework, which is used to teach compilation. Critique Honestly, I have found the section on implementing a refinement type checker through a series of programming languages challenging to understand. Still, I have understood much of the paper before that. Therefore, I will summarize my gained insights and state questions that I have in mind. Insights Refinement Types as Subtypes Type systems are the most commonly employed technique for ensuring the correct behavior of software. However, even well-typed programs can contain various bugs, such as buffer overflows, divisions by zero, logic bugs, and out-of-bounds array accesses. One approach to address this issue is to enhance a language's types with subtypes that limit the range of valid values with predicates, such as 'non-negative integer' from 'integer.' These subtypes are known as 'refinement types.' They enable developers to write precise contracts for valid inputs and outputs of functions and specify the correctness properties. This brings formal verification into mainstream software development. Refinement Logic and How it Maps to SMT Expressions I was partically impressed by refinement logic, the logic system used in the proposed refinement type checker, as it is both expressive and easy to be verified using an SMT solver. Refinement logic consists of two parts: predicates and constraints. Predicates are drawn from the quantifier-free fragment of linear arithmetic and uninterpreted functions (commonly used in SMT solvers), and may include boolean and integer literals, boolean and integer variables, arithmetic operators, boolean operators, comparisons, the 'if-then-else' expression, and uninterpreted functions (resembling those in z3). Predicates are the building block of constraints, which are generated from refinement type checking. A constraint is either a predicate, an implication \\(\\forall t: T \\: p \\Rightarrow c\\) which states that for each term \\(t\\) of type \\(T\\), if the predicate \\(p\\) holds then another constraint \\(c\\) must be true, or a conjunction of two other constraints. Constraints can be verified by checking whether there is no satisfying assignment for the negated constraint. In this process, they can be converted into SMT expressions in a straightforward way. For example, the constraint presented in the paper \\[c = \\forall x: array \\: 0 \\le length(x) \\Rightarrow \\forall n: int \\: n = length(x) \\Rightarrow \\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x)\\] can be negated as follows: \\[\\neg c\\] \\[\\neg (\\forall x: array \\: 0 \\le length(x) \\Rightarrow \\forall n: int \\: n = length(x) \\Rightarrow \\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\neg (\\forall n: int \\: n = length(x) \\Rightarrow \\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\exists n: int \\: n = length(x) \\land \\neg (\\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\exists n: int \\: n = length(x) \\land \\exists i: int \\: i = n - 1 \\land \\neg (0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\exists n: int \\: n = length(x) \\land \\exists i: int \\: i = n - 1 \\land (0 &gt; i \\lor i \\ge length(x))\\] We can verify the negated constraint using an SMT solver: 123456789101112131415161718192021222324252627In [1]: import z3In [2]: L = z3.Int(&#x27;L&#x27;)In [3]: n = z3.Int(&#x27;n&#x27;)In [4]: i = z3.Int(&#x27;i&#x27;)In [5]: solver = z3.Solver()In [6]: solver.add(0 &lt;= L)In [7]: solver.add(n == L)In [8]: solver.add(i == n - 1)In [9]: solver.add(z3.Or(i &lt; 0, i &gt; L))In [10]: check_sat_result = solver.check()In [11]: check_sat_resultOut[11]: satIn [12]: model_ref = solver.model()In [13]: model_refOut[13]: [i = -1, L = 0, n = 0] Note that check_sat_result is sat and we can find a satisfying assignment for \\(\\neg c\\): \\(i = -1, length(x) = 0, n = 0\\). This means that the original constraint \\(c\\) is invalid. Questions Although the concept of refinement types is neat, what is the burden on programmers of writing refinement types that describe legal inputs and outputs of functions? This is a critical aspect to determine whether refinement types can bring formal verification into mainstream software development. Furthermore, constraints in the proposed refinement logic generated by the refinement type checker can be negated and converted into SMT expressions. However, what is the feasibility of doing such checking for large-scale programs? Would it become unscalable?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: How to make ad-hoc polymorphism less ad-hoc","slug":"Paper-Reading-How-to-make-ad-hoc-polymorphism-less-ad-hoc","date":"2023-03-06T05:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2023/03/06/Paper-Reading-How-to-make-ad-hoc-polymorphism-less-ad-hoc/","link":"","permalink":"https://jifengwu2k.github.io/2023/03/06/Paper-Reading-How-to-make-ad-hoc-polymorphism-less-ad-hoc/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary The paper first defines and compares parametric and ad-hoc polymorphism and points out the limitations of existing implementations of ad-hoc polymorphism. It then presents type classes that extend the Hindley/Milner type system to support ad-hoc polymorphism as a remedy to these limitations and explains how to translate a program using type classes into an equivalent program without them at compile-time. Furthermore, it showcases the power of type classes and the translation mechanism using the example of a polymorphic equality operation. Finally, it explores subclassing of type classes. Critique The paper is easy to follow as it is written in a lucid manner and gives an informal introduction to type classes and its translation rules. Furthermore, the motivation for type classes and how it connects to object-oriented programming languages is explicitly stated in the paper. I have further looked up some material following these lines. I will summarize them before presenting some questions and comments. My Takeaways Different Types of Polymorphism See my Paper Reading for \"Types and Programming Languages\" Chapter 15 and Chapter 16. Type Classes and Protocols/Interfaces in Smalltalk/Objective-C/Java/C An interface is an abstract type used to provide a collection of methods compliant classes must implement in the Java (and C#) programming languages. Java is mostly influenced by Objective-C, and Java's interfaces are adaptations of the protocols in Objective-C and Smalltalk, which in turn is based on protocols in networking, notably the ARPANet. Although Type Classes and Interfaces do not share a common lineage, it is straightforward to implement Type Classes with Generic Interfaces whose Generic Parameters should be Classes that comply with the Interface. For instance, the Type Class below specifies the equal (==) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented in Java using the following Generic Interface: 123interface Eq&lt;T&gt; &#123; boolean isEqual(T other);&#125; Type Classes and Concepts in C++ Although Java's syntax resembles C++'s, its semantics of late-binding, single inheritance, class objects, and an extensive runtime system are in the lineage of Smalltalk and Objective-C, far away from that of C++'s. However, in C++'s Template Metaprogramming world, Concepts, added in C++20, resembles Type Classes. Template Metaprogramming in C++ had been untyped, with template parameters being generic type variables substituted at template instantiation. In C++20, a type system has been added to this untyped template language through concepts. They are Boolean predicates on template parameters evaluated at the point of, not after, template instantiation. The compiler will produce a clear error immediately if a programmer tries to use a template parameter that doesn't meet the requirements of a concept. This starkly contrasts the challenging-to-grasp errors reported after an invalid type substitutes a generic type variable emanating from the implementation context rather than the template instantiation itself. For instance, the first two arguments to std::sort must be random-access iterators. If an argument is not a random-access iterator, an error will occur when std::sort attempts to use it as a bidirectional iterator. 12std::list&lt;int&gt; l = &#123;2, 1, 3&#125;;std::sort(l.begin(), l.end()); Without concepts, compilers may produce large amounts of error information, starting with an equation that failed to compile when it tried to subtract two non-random-access iterators: 123In instantiation of &#x27;void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = std::_List_iterator&lt;int&gt;; _Compare = __gnu_cxx::__ops::_Iter_less_iter]&#x27;: error: no match for &#x27;operator-&#x27; (operand types are &#x27;std::_List_iterator&lt;int&gt;&#x27; and &#x27;std::_List_iterator&lt;int&gt;&#x27;) std::__lg(__last - __first) * 2, However, if concepts are used, the problem can be found and reported at template instantiation: 12error: cannot call function &#x27;void std::sort(_RAIter, _RAIter) [with _RAIter = std::_List_iterator&lt;int&gt;]&#x27;note: concept &#x27;RandomAccessIterator()&#x27; was not satisfied It is straightforward to implement Type Classes with concepts. For instance, the Type Class below specifies the equal (==) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented using the following C++ concept: 123456789#include &lt;concepts&gt;// Declaration of the concept &quot;Eq&quot;, which is satisfied by any type &#x27;T&#x27;// such that for values &#x27;t&#x27; of type &#x27;T&#x27;, the expression t == t compiles// and its type satisfies the concept std::same_as&lt;bool&gt;template &lt;typename T&gt; concept Eq = requires (T t) &#123; &#123; t == t &#125; -&gt; std::same_as&lt;bool&gt;;&#125;; Afterwards, such a concept can be specified when template parameters are being introduced in a template definition, to indicate that the corresponding template parameter must satisfy the concept. 123template&lt;Eq T&gt; void f(const T&amp; t) &#123; // ...&#125; References https://stackoverflow.com/questions/6948166/javas-interface-and-haskells-type-class-differences-and-similarities https://cs.gmu.edu/~sean/stuff/java-objc.html https://functionalcpp.wordpress.com/2013/08/16/type-classes/ https://stackoverflow.com/questions/32124627/how-are-c-concepts-different-to-haskell-typeclasses https://wiki.haskell.org/OOP_vs_type_classes https://doi.org/10.1145/1411318.1411324 https://www.foonathan.net/2021/07/concepts-structural-nominal/ https://www.reddit.com/r/haskell/comments/1e9f49/concepts_in_c_template_programming_and_type/ Questions and Comments The translation mechanism (pre-processor) proposed in this paper translates a program using type classes into an equivalent program without them at compile-time so that an existing Hindley/Milner type system can be used afterward instead of having to develop a new, complex type system to support type classes. This is indeed a very clever mechanism. Can this be viewed as an example of desugaring?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Type-Theoretic Constructs in C++","slug":"Type-Theoretic-Constructs-in-CXX","date":"2023-03-01T05:00:00.000Z","updated":"2025-08-13T04:31:00.460Z","comments":true,"path":"2023/03/01/Type-Theoretic-Constructs-in-CXX/","link":"","permalink":"https://jifengwu2k.github.io/2023/03/01/Type-Theoretic-Constructs-in-CXX/","excerpt":"","text":"Fixed-point Combinators, Tying the Recursive Knot, and Recursive Lambda Expressions In Lambda Calculus, we cannot refer to the Lambda Abstraction itself within a Lambda Abstraction. Similarly, C++ does not allow defining a recursive lambda expression. Thus, we cannot straightforwardly implement recursion. A workaround for this is to define a lambda expression that: Add an additional first parameter to the lambda expression. Call that additional first parameter inside the lambda expression at each recursive call site. Such an additional first parameter should have the value of a yet-unknown hypothetical recursive function. Thus, we should use auto to represent its type. Using auto in a lambda expression's parameter list requires C++14 or above. For example, we can define the following lambda expression to calculate the nth Fibonacci Number recursively: 12345678910auto fib = []( auto recursive_fib, unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return recursive_fib(n - 1) + recursive_fib(n - 2); &#125;&#125;; After defining such a lambda expression, we can use a Fixed-point Combinator to tie the recursive knot and return a new recursive function without the additional first parameter. What is a Fixed-Point Combinator? In mathematics, a fixed-point for function \\(f\\) refers to an element \\(x\\) that is mapped to itself by the function, i.e., \\(x = f(x)\\). For example, given \\(f(x)=x^{2}-3x+4\\), then \\(2\\) is a fixed point of \\(f\\), because \\(f(2) = 2\\). A combinator is a function that operates on a function (i.e., a higher-order function). A fixed-point combinator g for function f satisfies g(f)(...) = f(g(f), ...). This is reminiscent of the form \\(x = f(x)\\) for fixed points in mathematics, and g(f) can be seen as a fixed-point of function f. This implies that a fixed-point combinator g, when called on f, returns a new function (the g(f) above), that, when called with parameters ..., is equivalent to directly calling f with both g(f) and .... This means that a fixed-point combinator returns a new recursive function without the additional first parameter of f. This is done by tying the recursive knot of f. Tying the recursive knot refers to, for such a previously defined lambda expression f, passing a function that represents the hypothetical recursive function, which in this case is g(f), to its first parameter. We can implement fixed-point combinators in C++ using the following struct, whose instance represents g(f) and contains an operator() method, in which can use this to self-reference to g(f), allowing us to support f(g(f), ...): 1234567template &lt;typename F, typename R, typename... Args&gt; struct FixedPointCombinator &#123; F f; R operator()(Args... args) const &#123; return f(*this, args...); &#125;&#125;; C++ supports compiler optimizations for this pattern. For example, the following code: 1234567891011121314151617181920212223242526272829303132333435#include &lt;stdio.h&gt;template &lt;typename F, typename R, typename... Args&gt; struct FixedPointCombinator &#123; F f; R operator()(Args... args) const &#123; return f(*this, args...); &#125;&#125;;auto fib = []( auto recursive_fib, unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return recursive_fib(n - 1) + recursive_fib(n - 2); &#125;&#125;;int main() &#123; auto fib_ = FixedPointCombinator&lt;decltype(fib), unsigned long, unsigned int&gt; &#123;fib&#125;; unsigned int input; scanf(&quot;%u&quot;, &amp;input); unsigned long result = fib_(input); printf(&quot;%lu\\n&quot;, result); return 0;&#125; compiles to the following LLVM IR under clang++ -std=c++14 -O2 -S -emit-llvm, in which FixedPointCombinator&lt;decltype(fib), unsigned long, unsigned int&gt; &#123;fib&#125; has been optimized to the recursive function @_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071; ModuleID = &#x27;fib_recursive.cpp&#x27;source_filename = &quot;fib_recursive.cpp&quot;target datalayout = &quot;e-m:e-i64:64-f80:128-n8:16:32:64-S128&quot;target triple = &quot;x86_64-unknown-linux-gnu&quot;@.str = private unnamed_addr constant [3 x i8] c&quot;%u\\00&quot;, align 1@.str.1 = private unnamed_addr constant [5 x i8] c&quot;%lu\\0A\\00&quot;, align 1; Function Attrs: norecurse nounwind uwtabledefine dso_local i32 @main() local_unnamed_addr #0 &#123; %1 = alloca i32, align 4 %2 = bitcast i32* %1 to i8* call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4 %3 = call i32 (i8*, ...) @scanf(i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str, i64 0, i64 0), i32* nonnull %1) %4 = load i32, i32* %1, align 4, !tbaa !2 %5 = call fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32 %4) #4 %6 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.1, i64 0, i64 0), i64 %5) call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #4 ret i32 0&#125;; Function Attrs: argmemonly nounwinddeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: nofree nounwinddeclare dso_local i32 @scanf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: nofree nounwinddeclare dso_local i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: argmemonly nounwinddeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: inlinehint nounwind readnone uwtabledefine internal fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32) unnamed_addr #3 align 2 &#123; switch i32 %0, label %3 [ i32 0, label %9 i32 1, label %2 ]2: ; preds = %1 br label %93: ; preds = %1 %4 = add i32 %0, -1 %5 = tail call fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32 %4) #4 %6 = add i32 %0, -2 %7 = tail call fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32 %6) #4 %8 = add i64 %7, %5 ret i64 %89: ; preds = %1, %2 %10 = phi i64 [ 1, %2 ], [ 0, %1 ] ret i64 %10&#125;attributes #0 = &#123; norecurse nounwind uwtable &quot;correctly-rounded-divide-sqrt-fp-math&quot;=&quot;false&quot; &quot;disable-tail-calls&quot;=&quot;false&quot; &quot;less-precise-fpmad&quot;=&quot;false&quot; &quot;min-legal-vector-width&quot;=&quot;0&quot; &quot;no-frame-pointer-elim&quot;=&quot;false&quot; &quot;no-infs-fp-math&quot;=&quot;false&quot; &quot;no-jump-tables&quot;=&quot;false&quot; &quot;no-nans-fp-math&quot;=&quot;false&quot; &quot;no-signed-zeros-fp-math&quot;=&quot;false&quot; &quot;no-trapping-math&quot;=&quot;false&quot; &quot;stack-protector-buffer-size&quot;=&quot;8&quot; &quot;target-cpu&quot;=&quot;x86-64&quot; &quot;target-features&quot;=&quot;+cx8,+fxsr,+mmx,+sse,+sse2,+x87&quot; &quot;unsafe-fp-math&quot;=&quot;false&quot; &quot;use-soft-float&quot;=&quot;false&quot; &#125;attributes #1 = &#123; argmemonly nounwind &#125;attributes #2 = &#123; nofree nounwind &quot;correctly-rounded-divide-sqrt-fp-math&quot;=&quot;false&quot; &quot;disable-tail-calls&quot;=&quot;false&quot; &quot;less-precise-fpmad&quot;=&quot;false&quot; &quot;no-frame-pointer-elim&quot;=&quot;false&quot; &quot;no-infs-fp-math&quot;=&quot;false&quot; &quot;no-nans-fp-math&quot;=&quot;false&quot; &quot;no-signed-zeros-fp-math&quot;=&quot;false&quot; &quot;no-trapping-math&quot;=&quot;false&quot; &quot;stack-protector-buffer-size&quot;=&quot;8&quot; &quot;target-cpu&quot;=&quot;x86-64&quot; &quot;target-features&quot;=&quot;+cx8,+fxsr,+mmx,+sse,+sse2,+x87&quot; &quot;unsafe-fp-math&quot;=&quot;false&quot; &quot;use-soft-float&quot;=&quot;false&quot; &#125;attributes #3 = &#123; inlinehint nounwind readnone uwtable &quot;correctly-rounded-divide-sqrt-fp-math&quot;=&quot;false&quot; &quot;disable-tail-calls&quot;=&quot;false&quot; &quot;less-precise-fpmad&quot;=&quot;false&quot; &quot;min-legal-vector-width&quot;=&quot;0&quot; &quot;no-frame-pointer-elim&quot;=&quot;false&quot; &quot;no-infs-fp-math&quot;=&quot;false&quot; &quot;no-jump-tables&quot;=&quot;false&quot; &quot;no-nans-fp-math&quot;=&quot;false&quot; &quot;no-signed-zeros-fp-math&quot;=&quot;false&quot; &quot;no-trapping-math&quot;=&quot;false&quot; &quot;stack-protector-buffer-size&quot;=&quot;8&quot; &quot;target-cpu&quot;=&quot;x86-64&quot; &quot;target-features&quot;=&quot;+cx8,+fxsr,+mmx,+sse,+sse2,+x87&quot; &quot;unsafe-fp-math&quot;=&quot;false&quot; &quot;use-soft-float&quot;=&quot;false&quot; &#125;attributes #4 = &#123; nounwind &#125;!llvm.module.flags = !&#123;!0&#125;!llvm.ident = !&#123;!1&#125;!0 = !&#123;i32 1, !&quot;wchar_size&quot;, i32 4&#125;!1 = !&#123;!&quot;clang version 9.0.1 (https://github.com/conda-forge/clangdev-feedstock 2ea3b72da24769de0dfc6dac99251a5d7a46144d)&quot;&#125;!2 = !&#123;!3, !3, i64 0&#125;!3 = !&#123;!&quot;int&quot;, !4, i64 0&#125;!4 = !&#123;!&quot;omnipotent char&quot;, !5, i64 0&#125;!5 = !&#123;!&quot;Simple C++ TBAA&quot;&#125; We can slightly modify the code above to support a fixed-point combinator that also provides memoization. Note that the instance of the fixed-point combinator is now stateful, and we should pass it by reference to fib (i.e., change the first parameter of fib to reference type): 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;map&gt;#include &lt;tuple&gt;template &lt;typename F, typename R, typename... Args&gt; struct FixedPointCombinatorWithMemoization &#123; F f; std::map&lt;std::tuple&lt;Args...&gt;, R&gt; memo; R operator()(Args... args) &#123; std::tuple&lt;Args...&gt; args_tuple(args...); // Check if result is in the memoization map auto found = memo.find(args_tuple); if (found != memo.end()) &#123; return found-&gt;second; // Return the cached result &#125; // Otherwise, compute and store the result R result = f(*this, args...); memo[args_tuple] = result; return result; &#125;&#125;;auto fib = []( auto&amp; recursive_fib, unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return recursive_fib(n - 1) + recursive_fib(n - 2); &#125;&#125;;int main() &#123; auto fib_ = FixedPointCombinatorWithMemoization&lt;decltype(fib), unsigned long, unsigned int&gt; &#123;fib&#125;; unsigned int input; scanf(&quot;%u&quot;, &amp;input); unsigned long result = fib_(input); printf(&quot;%lu\\n&quot;, result); return 0;&#125; Type Abstractions and Template Functions Polymorphic Lambda Calculus (also known as Second Order Lambda Calculus or System F) introduces Type Abstractions and Type Applications. A Type Abstraction, written as λ X . t, represents a Term (often a Lambda Abstraction) t containing a Type Variable X. A Type Application, written as t [T], uses a Concrete Type T to replace all instances of the Type Variable in the Term of the Type Abstraction. This can be used to implement Polymorphic Lambda Abstractions. For example, the following Type Abstraction representing a Polymorphic Identity Function: 1id = λ X . λ x: X . x can be instantiated to yield any concrete identity function that may be required, such as id [Nat]: Nat -&gt; Nat. Such Type Abstractions can be implemented in C++ using template functions: 123template &lt;typename X&gt; auto id(X x) &#123; return x;&#125; while Type Applications correspond to template instantiations: 1id&lt;int&gt; Should the template function be passed a callable, we usually want to use a template typename to support functions, function pointers, functors, and lambda expressions. Alternatively, we can also use auto to represent its type in the template function's parameter list. Note that using auto in a (non-lambda expression) function's parameter list requires C++20 or above. For example, the following Type Abstraction: 1double = λ X . λ f: X -&gt; X . λ a: X . f(f a) can be represented using the following template function: 12345template &lt;typename X, typename F&gt; auto double_(const F f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125; or in C++20 or above: 12345template &lt;typename X&gt; auto double_(const auto f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125; C++ compilers support aggressive inlining optimizations when lambda expressions are used. For example, the call to const auto g = double_&lt;int&gt;([](int x) &#123; return 2 * x; &#125;); in the following source code: 1234567891011121314151617181920#include &lt;stdio.h&gt;template &lt;typename X&gt; auto double_(const auto f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125;int main() &#123; const auto g = double_&lt;int&gt;([](int x) &#123; return 2 * x; &#125;); int input; scanf(&quot;%d&quot;, &amp;input); printf(&quot;%d\\n&quot;, g(input)); return 0;&#125; has been completely inlined to %5 = shl i32 %4, 2 in the LLVM IR generated with clang++ -std=c++20 -O2 -S -emit-llvm: 123456789101112131415161718192021222324252627@.str = private unnamed_addr constant [3 x i8] c&quot;%d\\00&quot;, align 1@.str.1 = private unnamed_addr constant [4 x i8] c&quot;%d\\0A\\00&quot;, align 1; Function Attrs: norecurse nounwind uwtabledefine dso_local i32 @main() local_unnamed_addr #0 &#123; %1 = alloca i32, align 4 %2 = bitcast i32* %1 to i8* call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #3 %3 = call i32 (i8*, ...) @__isoc99_scanf(i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str, i64 0, i64 0), i32* nonnull %1) %4 = load i32, i32* %1, align 4, !tbaa !2 %5 = shl i32 %4, 2 %6 = call i32 (i8*, ...) @printf(i8* nonnull dereferenceable(1) getelementptr inbounds ([4 x i8], [4 x i8]* @.str.1, i64 0, i64 0), i32 %5) call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #3 ret i32 0&#125;; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: nofree nounwinddeclare dso_local i32 @__isoc99_scanf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: nofree nounwinddeclare dso_local i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1 Type Classes and Concepts in C++ Template Metaprogramming in C++ had been untyped, with template parameters being generic type variables substituted at template instantiation. In C++20, a type system has been added to this untyped template language through concepts. They are Boolean predicates on template parameters evaluated at the point of, not after, template instantiation. The compiler will produce a clear error immediately if a programmer tries to use a template parameter that doesn't meet the requirements of a concept. This starkly contrasts the challenging-to-grasp errors reported after an invalid type substitutes a generic type variable emanating from the implementation context rather than the template instantiation itself. For instance, the first two arguments to std::sort must be random-access iterators. If an argument is not a random-access iterator, an error will occur when std::sort attempts to use it as a bidirectional iterator. 12std::list&lt;int&gt; l = &#123;2, 1, 3&#125;;std::sort(l.begin(), l.end()); Without concepts, compilers may produce large amounts of error information, starting with an equation that failed to compile when it tried to subtract two non-random-access iterators: 123In instantiation of &#x27;void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = std::_List_iterator&lt;int&gt;; _Compare = __gnu_cxx::__ops::_Iter_less_iter]&#x27;: error: no match for &#x27;operator-&#x27; (operand types are &#x27;std::_List_iterator&lt;int&gt;&#x27; and &#x27;std::_List_iterator&lt;int&gt;&#x27;) std::__lg(__last - __first) * 2, However, if concepts are used, the problem can be found and reported at template instantiation: 12error: cannot call function &#x27;void std::sort(_RAIter, _RAIter) [with _RAIter = std::_List_iterator&lt;int&gt;]&#x27;note: concept &#x27;RandomAccessIterator()&#x27; was not satisfied It is straightforward to implement Type Classes with concepts. For instance, the Type Class below specifies the equal (==) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented using the following C++ concept: 123456789#include &lt;concepts&gt;// Declaration of the concept &quot;Eq&quot;,// which is satisfied by any type &#x27;T&#x27; such that for values &#x27;t&#x27; of type &#x27;T&#x27;, the expression t == t compiles and its type satisfies the concept std::same_as&lt;bool&gt;// This is represented using a &quot;requires expression&quot; which returns a booltemplate &lt;typename T&gt; concept Eq = requires (T t) &#123; &#123; t == t &#125; -&gt; std::same_as&lt;bool&gt;;&#125;; Afterwards, such a concept can be specified when template parameters are being introduced in a template definition, to indicate that the corresponding template parameter must satisfy the concept. 123template&lt;Eq T&gt; void f(const T&amp; t) &#123; // ...&#125; or (using a \"requires clause\"): 123template&lt;typename T&gt; requires Eq&lt;T&gt; void f(const T&amp; t) &#123; // ...&#125; References https://stackoverflow.com/questions/6948166/javas-interface-and-haskells-type-class-differences-and-similarities https://cs.gmu.edu/~sean/stuff/java-objc.html https://functionalcpp.wordpress.com/2013/08/16/type-classes/ https://stackoverflow.com/questions/32124627/how-are-c-concepts-different-to-haskell-typeclasses https://wiki.haskell.org/OOP_vs_type_classes https://doi.org/10.1145/1411318.1411324 https://www.foonathan.net/2021/07/concepts-structural-nominal/ https://www.reddit.com/r/haskell/comments/1e9f49/concepts_in_c_template_programming_and_type/","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"C++","slug":"Software-Design/C","permalink":"https://jifengwu2k.github.io/categories/Software-Design/C/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 22","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-22","date":"2023-02-26T05:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2023/02/26/Paper-Reading-Types-and-Programming-Languages-Chapter-22/","link":"","permalink":"https://jifengwu2k.github.io/2023/02/26/Paper-Reading-Types-and-Programming-Languages-Chapter-22/","excerpt":"","text":"Summary Chapter 22 of \"Types and Programming Languages\" explores the problem of Type Reconstruction (Type Inference) or deriving Types for Unannotated Arguments of Lambda Abstractions. It first introduces Type Variables and Substitutions before formalizing the Type Reconstruction problem. Then, it points out that Type Reconstruction can be implemented using a Constraint Typing Algorithm or an Algorithm that calculates a Set of Constraints between Types involving Type Variables and records them for later consideration, and proves the Completeness and Soundness of Constraint Typing. Moreover, it introduces a Unification Algorithm to calculate Principle Solutions (most general solutions) to Constraint Sets. Finally, the Chapter presents how the Typing Rules for Let Expressions can be modified to support Let Polymorphism - allowing an Untyped Function to generate different Constraints, thus be able to be Reconstructed to Different Types when applied to Terms of different Types. Critique Overall, Chapter 22 is clearly written, and several sections intrigued me (such as that Parametric Polymorphism and Type Reconstruction can result from two different interpretations of Dependent Types containing Type Variables). Moreover, this Chapter provides essential inspiration for my Class Project, \"Inferring Feasible Types for the Parameters and Return Values of Python Functions.\" However, the Chapter also used some Concepts without introducing them (such as the Unification Problem), and I had to look into them to understand parts of the Chapter. Background Knowledge Completeness and Soundness of a Theory Using \\(TRUE\\) and \\(PROVABLE\\) to represent the Set of Facts that are True and Provable under a Theory, respectively: Completeness: \\(TRUE \\subseteq PROVABLE\\) or every Fact that is True is also Provable (but there may be some Facts that are Provable but are not True). Soundness: \\(PROVABLE \\subseteq TRUE\\) or every Fact that is Provable is also True (but there may be some True Facts that are not Provable). Completeness and Soundness: \\(TRUE = PROVABLE\\). An ideal Theory should be both Complete and Sound. Unification Problem Given two Terms containing some Variables, find a Substitution (an Assignment of Terms to Variables) that makes the two Terms equal. For example, given \\(f(x_1, h(x_1), x_2) = f(g(x_3), x_4, x_3)\\), a valid Substitution is \\(\\sigma = \\{g(x_3): x_1, x_3: x_2, h(g(x_3)): x_4\\}\\). Takeaways From This Paper Parametric Polymorphism and Type Reconstruction Given Dependent Types containing Type Variables (often the result of the Programmer leaving out Type Annotations in Source Code), we can make one of the following assumptions. All Substitution Instances are well-typed. Thus, it is possible for Type Variables to be held abstract during Type Checking and only be Substituted for Concrete Types later on. This is the basis of Parametric Polymorphism. Not all Substitution Instances are well-typed. In this case, we want to look for valid Substitutions. This leads us to the problem of Type Reconstruction. Deriving Constraint Sets and Calculating Solutions to Them To explore valid ways that Concrete Types can substitute Type Variables, we can calculate a Set of Constraints between Types involving Type Variables. This is similar to an ordinary Type Checking Algorithm checking Requirements in the Premise but records these Requirements as Constraints for later consideration instead of checking them immediately. After we have generated a Constraint Set, we can use a Unification Algorithm to calculate Solutions to it. The Unification Algorithm proposed in the Chapter removes a Constraint from the Constraint Set, processes it, and recursively processes the remaining Constraint Set. There is a most general way to instantiate the Type Variables. This is known as a Principle Solution, which contains Principle Types, or the most general types, for Type Variables. Inspirations From This Paper This Paper points out a viable way to implement my Class Project \"Inferring Feasible Types for the Parameters and Return Values of Python Functions.\" Propose Typing Rules for Python Expressions. Implement an Algorithm similar to an ordinary Type Checking Algorithm checking Requirements in the Premise, but which records these Requirements as Constraints for later consideration instead of checking them immediately. Questions What are the specific types of Constraints that are recorded when deriving Constraint Sets? What do the derived Constraint Sets look like? Implementing the Unification Algorithm proposed to calculate Solutions to the Constraint Set seems non-trivial. Are there any implementations of it for more \"real-world\" (imperative, non-ML Family) Programming Languages? What adjustments have to be made to accomplish such an implementation?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Displaying Information for Thrown and Caught Exceptions to the User in Python","slug":"Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python","date":"2023-02-20T05:00:00.000Z","updated":"2025-08-13T04:31:00.453Z","comments":true,"path":"2023/02/20/Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python/","link":"","permalink":"https://jifengwu2k.github.io/2023/02/20/Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python/","excerpt":"","text":"Exception Semantics in Python Exception handling refers to how a program reacts when unexpected events, known as exceptions, occur throughout the program's execution. Exception semantics varies considerably among programming languages. Based on this, we can divide programming languages into two groups: Programming languages that only employ exceptions to address exceptional, unforeseen, or incorrect circumstances, such as C++, Java, and C#. Programming languages that use exceptions as standard flow control structures, such as Ada, ML, OCaml, Python, and Ruby. For example, in Python, when an iterator has exhausted its output, and no more items can be generated, an exception of type StopIteration is thrown. As a result, exceptions are pervasive in Python, and exception catching and handling is a must for writing robust Python code. Displaying Information for Thrown and Caught Exceptions to the User In many situations, it is beneficial to handle the exception and give a user a \"loud and clear\" message of what has happened as feedback. This is also particularly useful in investigating the root cause of the exception and whether it is the tip of the iceberg of a more significant latent bug. This can be simplified by the fact that exceptions thrown by built-in functions, standard library functions, and functions in many well-tested third-party libraries all contain rich semantics in: The class of the exception. Given an exception e, it is accessible via type(e), and type(e).__name__ gives a str representation. The message of the exception. Given an exception e, str(e) generates a representation of the argument(s) to the instance. In command-line programs, we can write both of them to stderr, as shown in the example below: 12345678from sys import stderrtry: # Do some potentially erroneous operationexcept Exception as e: # Write the class of the exception and the message of the exception to stderr print(type(e).__name__, str(e), file=stderr) In GUI programs, we can display them in a message box, with the class of the exception being the title of the message box and the message of the exception being the message of the message box, as shown in the example below: 12345678910111213141516171819202122232425262728from PySide6.QtCore import Slotfrom PySide6.QtWidgets import QDialog, QMessageBoxfrom .ui import Ui_ConnectToServerDialogclass ConnectToServerDialog(QDialog): def __init__(self, parent=None): super().__init__(parent) self.ui=Ui_ConnectToServerDialog() self.ui.setupUi(self) self.ui.connectPushButton.clicked.connect(self.accept) self.server=None @Slot() def accept(self): try: # Do some operation that involves potentially erroneous user input except Exception as e: # Display the exception thrown in a QMessageBox # The type of the exception is the title of the QMessageBox # The message of the exception is the message of the QMessageBox QMessageBox.about(self, type(e).__name__, str(e)) return super().accept() References https://en.wikipedia.org/wiki/Exception_handling#Exception_support_in_programming_languages https://docs.python.org/3/library/exceptions.html#bltin-exceptions","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Python","slug":"Software-Design/Python","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Python/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 15 and Chapter 16","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-15-and-Chapter-16","date":"2023-02-10T05:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2023/02/10/Paper-Reading-Types-and-Programming-Languages-Chapter-15-and-Chapter-16/","link":"","permalink":"https://jifengwu2k.github.io/2023/02/10/Paper-Reading-Types-and-Programming-Languages-Chapter-15-and-Chapter-16/","excerpt":"","text":"Summary Chapter 15, \"Subtyping,\" describes adding Subtyping with Functions and Records into Simply Typed Lambda Calculus. It formalizes the Subtype Relation as a collection of Inference Rules, verifies that verify that the Preservation and Progress Theorems of Simply Typed Lambda Calculus still apply, examines Ascription (or Casting) in the context of Subtyping, and proposes Subtyping Rules for Variants, Lists, References, and Arrays. Finally, it presents alternative Coercion Semantics for Subtyping. Chapter 16, \"Metatheory of Subtyping,\" observes that the Subtyping Rules presented in the previous chapter are not syntax-directed and have overlapping conclusions, which impedes implementing a Typechecking Algorithm, and develops the Algorithmic Subtype Relation and the Algorithmic Typing Relation to address these problems. Critique Acquired Insights I will first summarize the insights that I gained while reading these Chapters. An empty Bottom Type is useful, both as a way of expressing that a Function is not intended to return and telling the Typechecker that the Term can be associated with any Type. Implementing Ascription (Casting) in Subtyping is non-trivial, especially for Downcasting. As blindly following Type Assertions may lead to potentially serious consequences, the Compiler would need to insert a Runtime Type Check, essentially adding the Machinery for Typechecking to the Runtime System. This might incur a significant performance overhead. Different from an Inheritance Based Class Hierarchy, which is a physical relationship between Types, Subtyping generally is more of a logical relationship between Types. For example, in the alternative Coercion Semantics for Subtyping, we can consider that int and float, two Types that do not inherit from one another, have a Subtyping Relation, as they can be converted to one another. In this case, the Subtyping Relation is compiled to Coercions at runtime (instructions physically converting an int to a float, or vice versa), which are much more efficient than virtual function calls frequently seen in an Inheritance Based Class Hierarchy. Background Knowledge There is no doubt that the Chapters are written in great detail. However, I find some of the content, especially the terminology, a little difficult to understand, and I have looked into background knowledge concerning the topic. Below summarizes what I have read. Polymorphism Polymorphism describes that a single Interface can work with Terms of Different Types in Programming Languages. There are different kinds of Polymorphism in the context of Programming Languages, including: Parametric Polymorphism Also known as \"Generic Programming\". Using Abstract Symbols that can substitute for any Type instead of specifying Concrete Types in Interfaces. C++'s Template Metaprogramming comes close to Parametric Polymorphism (except for Template Specializations). Ad Hoc Polymorphism Defining a Common Interface for a Set of Individually Specified Types. Includes Function Overloading, Operator Overloading, and C++'s Template Metaprogramming with Template Specializations. Subtyping It is a form of Polymorphism in which the Terms of a Subtype T, which is related to another Type known as the Supertype T' in some way, can be safely used in any Context where the Terms of T' are used. The Concept of Subtyping has gained visibility with the advent of Object Oriented Programming Languages, where it is frequently the case that an Inheritance Based Class Hierarchy forms the basis of Subtyping, and such Safe Substitution is known as the Liskov Substitution Principle. However, stepping out of this specific and widely known context, there are several different Schemes of Subtyping. They can be broadly classified along two dimensions: Nominal Subtyping vs. Structural Subtyping and Inclusive Implementations vs. Coercive Implementations. Nominal Subtyping requires the Subtyping Relation to be explicitly declared among the two Types. This is the case with the Subtyping based on an Inheritance Based Class Hierarchy frequently encountered in Object Oriented Programming Languages. In contrast, in Structural Subtyping, a Type T is implicitly the Subtype of another Type T' if Terms of T has all the Properties of Terms of T' and can handle all the Messages Terms of T' can handle. This is closely related to Row Polymorphism or the so-called Duck Typing in Dynamically Typed Programming Languages. On another dimension, Implementations of Subtyping can be divided into Inclusive Implementations and Coercive Implementations. In Inclusive Implementations, any Term of a Subtype, left unchanged, is automatically a Term of a Supertype. This is often the case with the Subtyping based on an Inheritance Based Class Hierarchy frequently encountered in Object Oriented Programming Languages. A Term can have multiple Types in this situation. In contrast, Coercive Implementations are defined by Type Conversion Functions from Subtype to Supertype and allow a Term of a Subtype to be converted to a Term of a Supertype, such as the case for int's, float's, and str's. It is also worth noticing that applying the Type Coercion Function from A to B and then from B to C might have a different result from directly applying the Type Coercion Function from A to C. For example, str(float(2)) returns a value different from str(2). Based on the concept of Subtyping, the concept of Variance reference to how the Subtyping Relations between more complex Types relates to the Subtyping Relations between the simpler Types they include. For example, given that Cat is a Subtype of Animal, should a List of Cat's be a Subtype of a List of Animal's? What about a Function that takes a Term of Type Cat as an Arugument and a Function that takes a Term of Type Animal as an Arugument? Different Programming Languages have different implementations, but most Programming Languages respect the following patterns. If the Complex Types are Read Only and/or capable of returning Terms of the Simple Types, they should have the same Subtyping Relations as the Simple Types. This is known as Covariance. For example, A read-only List of Cat's can be used whenever a read-only List of Animal's is required, as each Term read from the read-only List of Cat's is of Type Cat, which is a Subtype of Animal. In other words, const List&lt;Cat&gt; is a Subtype of const List&lt;Animal&gt;. It is not safe to use a const List&lt;Animal&gt; where a const List&lt;Cat&gt; is required, as a Term read from a const List&lt;Animal&gt; may not be of Type Cat. In other words, const List&lt;Animal&gt; is not a Subtype of const List&lt;Cat&gt;. If the Complex Types are Write Only and/or capable of accepting Terms of the Simple Types as Parameters, they should have the opposite Subtyping Relations as the Simple Types. This is known as Contravariance. For example, A Function that takes a Term of Type Animal as a Parameter may be used where a Function that takes a Term of Type Cat as a Parameter is used, as each Term of Type Cat can also be passed as a Parameter of Type Animal. In other words, Animal -&gt; T is a Subtype of Cat -&gt; T. It is not safe to use a Cat -&gt; T where an Animal -&gt; T is required, as a Term of Type Animal may not be passed as a Parameter of Type Cat. In other words, Cat -&gt; T is not a Subtype of Animal -&gt; T. If the Complex Types are Read/Write, they should have no Subtying Relations. This is known as Invariance. For example, A Term written into a List&lt;Animal&gt; need not be of Type Cat, but a Term written into a (non-constant) List&lt;Cat&gt; must be of Type Cat. Thus, it is not safe to use a List&lt;Cat&gt; where a List&lt;Animal&gt; is required. In other words, List&lt;Cat&gt; is not a Subtype of List&lt;Animal&gt;. A Term read from a (non-constant) List&lt;Animal&gt; may not be of Type Cat. Thus it is not safe to use a List&lt;Animal&gt; where a List&lt;Cat&gt;is required. In other words, List&lt;Animal&gt; is not a Subtype of List&lt;Cat&gt;. References https://en.wikipedia.org/wiki/Polymorphism_(computer_science) https://stackoverflow.com/questions/36948205/why-is-c-said-not-to-support-parametric-polymorphism https://en.wikipedia.org/wiki/Subtyping https://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science) Having acquired such Background Knowledge, I will also summarize the insights that I gained while reading these Chapters. Acquired Insights An empty Bottom Type is useful, both as a way of expressing that a Function is not intended to return and telling the Typechecker that the Term can be associated with any Type. Implementing Ascription (Casting) in Subtyping is non-trivial, especially for Downcasting. As blindly following Type Assertions may lead to potentially serious consequences, the Compiler would need to insert a Runtime Type Check, essentially adding the Machinery for Typechecking to the Runtime System. This might incur a significant performance overhead. Different from an Inheritance Based Class Hierarchy, which is a physical relationship between Types, Subtyping generally is more of a logical relationship between Types. For example, in the alternative Coercion Semantics for Subtyping, we can consider that int and float, two Types that do not inherit from one another, have a Subtyping Relation, as they can be converted to one another. In this case, the Subtyping Relation is compiled to Coercions at runtime (instructions physically converting an int to a float, or vice versa), which are much more efficient than virtual function calls frequently seen in an Inheritance Based Class Hierarchy.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 13 and Chapter 14","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-13-and-Chapter-14","date":"2023-02-05T05:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2023/02/05/Paper-Reading-Types-and-Programming-Languages-Chapter-13-and-Chapter-14/","link":"","permalink":"https://jifengwu2k.github.io/2023/02/05/Paper-Reading-Types-and-Programming-Languages-Chapter-13-and-Chapter-14/","excerpt":"","text":"Summary Chapters 13 and 14 of \"Types and Programming Languages\" discuss adding Impure Features, also known as Computational Effects, into Simply Typed Lambda Calculus. Specifically, Chapter 13 discusses adding References to Mutable Cells that can be Allocated, Dereferenced, and Assigned and formalizes their Operational Behavior. Chapter 14 gradually adds Raising and Handling Exceptions, starting from a Term error of any Type that completely aborts Evaluation when applied as a Function or passed as an Argument to a Function, before supporting Exception Handling, as well as Raising a Value (potentially containing information about what unusual thing happened) as an Exception. Critique Overall, I believe these two Chapters are written very well, as they progressively add realistic features to Simply Typed Lambda Calculus. I will summarize takeaways from this paper before presenting some questions and comments. Takeaways From This Paper References to Mutable Cells The Formalization of the Operational Behavior of References to Mutable Cells encompasses Allocations (providing an initial value to a Mutable Cell), Dereferences (reading the current value of the referenced Cell), and Assignments (changing the value stored in the referenced Cell), but not Deallocations. Explicit Deallocations lead to the Dangling Reference Problem, which undermines Type Safety. Instead, References to Mutable Cells that are no longer needed should be Garbage Collected. An interpretation of how Aliasing makes Program Analysis tricky is that Aliasing essentially sets up \"Implicit Communication Channels in the form of Shared State\" between different parts of a Program. To formalize the Operational Behavior of References to Mutable Cells, we can consider a Reference \\(l \\in L\\), where \\(L\\) is the set of Locations of the Program's Store (a.k.a. Heap Memory) \\(\\mu\\). As the result of Evaluating an Expression depends on the current contents of the Store and may cause Side Effects for the Store, Evaluation Rules should, in addition to Terms and Types, take the Store as an Argument and return a new Store as part of the result of Evaluating an Expression. Furthermore, in a naive implementation of Typing Rules for References to Mutable Cells, the Type of the Reference depends on the Type of the Mutable Cell, e.g., \\(\\frac{\\Gamma \\vdash \\mu(l): T}{\\Gamma \\vdash l: \\text{Ref} \\: T}\\). However, this is inefficient where there are multiple levels of Indirection and is problematic where there are Cyclic References. To solve this problem, the Chapter proposes extending Typing Rules with a Store Typing \\(\\Sigma\\), which maps every Location \\(l \\in L\\) to a fixed, definite Type. In this case, the Typing Rule is written as \\(\\frac{\\Gamma | \\Sigma \\vdash \\Sigma(l): T}{\\Gamma | \\Sigma \\vdash l: \\text{Ref} \\: T}\\). Raising and Handling Exceptions The first (and most straightforward) Approach to Raising and Handling Exceptions, a Term error that completely aborts Evaluation when applied as a Function or passed as an Argument to a Function, effectively simulates Unwinding the Call Stack when it propagates error to the top level. The final approach that supports both Exception Handling and Raising a Value as an Exception considers an Exception to be a Value \\(t_{exp}\\) of Type \\(T_{exp}\\) (instead of a Term error). It proposes a Term Constructor raise t_&#123;exp&#125; that describes Raising a Value as an Exception, and models Exception Handling with try t_1 with t_2: T_1, in which \\(t_1: T_1\\) and \\(t_2: T_{exp} \\rightarrow T_1\\) (i.e., \\(t_2\\) is a function, called when an Exception is Raised, taking a Raised Exception as Input and Returning a Value of the same Type as \\(t_1\\) as Output). Questions and Comments After reading these two Chapters, the power of Functions as a Universal Abstraction has left a deep impression on me. For example: Arrays containing Terms of Type \\(T\\) can be modeled as References to Functions of type \\(Nat \\rightarrow T\\). The Referenced Function looks up the Element given an Index. Exception Handling is modeled with try t_1 with t_2, in which \\(t_2\\) is a function called when an Exception is Raised, taking a Raised Exception as Input and Returning a Value of the same Type as \\(t_1\\) as Output). This describes complex Side Effects in a realistic Programming Language in a Side Effect Free manner that is clean and easy to reason about while not sacrificing Expressiveness. Are there any other complex Side Effects that can be modeled like this using Functions?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: Bi-directional type checking","slug":"Paper-Reading-Bi-directional-type-checking","date":"2023-01-30T05:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2023/01/30/Paper-Reading-Bi-directional-type-checking/","link":"","permalink":"https://jifengwu2k.github.io/2023/01/30/Paper-Reading-Bi-directional-type-checking/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary The paper first explains that except Syntax Directed Systems, Typing Rules cannot be directly translated into Algorithms for Type Checking and Type Inference. It presents a motivating example of this using a Simply Typed Lambda Calculus having Bool and Function as Types and Bool Constants, Variables, Function Abstractions, Function Applications, and Conditional Expressions as Terms, in which the Typing Rule for Function Abstractions cannot be directly translated into a Function for Type Inference. It then presents Bidirectional Typing as a remedy to this problem. It explains what Bidirectional Typing is, discusses its advantage, and adds Bidirectional Typing into the previously presented Simply Typed Lambda Calculus, presenting how Bidirectional Typing works during the process. Finally, it discusses the limitations of Bidirectional Typing and presents academic literature for further reading. Critique Overall, I believe this paper is written very well, as I can grasp most of it after reading it. I will summarize my takeaways from this paper before presenting some questions and comments. My Takeaways From This Paper What Bidirectional Typing Is Bidirectional Typing splits each Typing Rule \\(\\Gamma \\vdash t: T\\) into: An Inference Rule \\(\\Gamma \\vdash t \\Rightarrow T\\), which infers \\(t\\)'s type to be \\(T\\) in Context \\(\\Gamma\\). A Type Checking Rule \\(\\Gamma \\vdash t \\Leftarrow T\\), which checks \\(t\\)'s type to be \\(T\\) in Context \\(\\Gamma\\). The Inference Rules and Type Checking Rules would work together and call each other. Advantages of Bidirectional Typing Makes general Typing Rules more Syntax Directed, thus, simplifying implementing Algorithms for Type Checking and Type Inference. Requires relatively few additional Type Annotations. Produces good error messages that report where the error occurs. Limitations of Bidirectional Typing Variables in a Derivation can no longer be replaced by the Derivation for a Term of the same Type. This is because Bidirectional Typing uses Inference Mode to check Variables but uses Checking Mode to check many other Terms. In some situations, explicit Type Annotations may need to be written within complex Terms, such as a direct Application of a Function Abstraction, like (λ b . if b then false else true) true: Bool Questions and Comments Page 8 mentions, \"remember that the derivation, like the bidirectional typing rules, should be read bottom-to-top and left-to-right.\" However, Inference Rules have the form of \\(\\frac{Premise}{Conclusion}\\). So, why should the derivation be read from Conclusion to Premise? What are the meanings of the small-step rule \\(\\frac{}{t : T \\rightarrow t}\\) and the large-step rule \\(\\frac{t \\Downarrow t&#39;}{t : T \\Downarrow t&#39;}\\) on Page 8? I believe explicit Type Annotations should be enforced for the Parameters within Function Abstractions, such as (λ b: Bool . if b then false else true) instead of (λ b . if b then false else true). This aligns with real-world programming languages (C++, Java, Rust, Swift, Haskell, etc.) This increases readability. This simplifies both the Typing Rules and the Inference Rules and Type Checking Rules of Bidirectional Typing. Feedback from the Class Discussion Small Step Semantics, represented using \\(\\rightarrow\\)'s, depict one step in Evaluation. For example, if \\(e\\) is \\(true\\) itself, \\(\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2\\) can be Evaluated in one step to \\(e_1\\). This can be represented using \\(\\frac{e \\rightarrow true}{\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2 \\rightarrow e_1}\\) Big Step Semantics, represented using \\(\\Downarrow\\)'s, depict Reducing a Subexpression to a Value through several Small Steps. For example, if \\(e\\) is a Subexpression that can be Reduced to \\(true\\) after several Small Steps, \\(\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2\\) can be Reduced to \\(e_1\\) after several Small Steps. This can be represented using \\(\\frac{e \\Downarrow true}{\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2 \\Downarrow e_1}\\). Syntax Directed means a one-to-one correspondence between the Type of the Term and the Syntax (Derivation of the Grammar Rules) of the Term. There is no precise definition for Bidirectional Typing. Instead, Bidirectional Typing points a direction toward implementing a Type Inference/Type Checking Algorithm. In Bidirectional Typing, we prefer to start from Inference Mode because if we can Infer the Type of a Term, we can Check the Type of the Term, while Checking falls back on Inference. Why is there only a Checking Rule and no Inference Rule for if t then t else t? This gives better error messages. Should the Terms in the then branch and the else branch have different types, it is possible to give an error message directly stating this information. If an Inference Rule had been proposed instead, it would blame one branch for having a wrong type, which may be confusing and go against programmer intent. We can read Typing Rules either from top to bottom or from bottom to top, with slightly different interpretations. Reading from top to bottom describes how to use the information for Type Checking. Reading from bottom to top describes how a Type Inference Algorithm works, e.g., what needs to be Checked to Infer the Type of a Term. From a historical perspective, there are two Design Philosophies for Type Systems. The first is to augment a Programming Language with more information, such as C which uses it to determine how much space a variable would take up in memory. The second is to express programmer intent. Type Annotations (Ascriptions) for Parameters are required for Functions that are not immediately used, such as Top Level Functions. However, it is helpful to omit Type Annotations (Ascriptions) for Parameters for immediately used Lambda Terms within Higher Order Functions.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 9 and Chapter 11","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-9-and-Chapter-11","date":"2023-01-25T05:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2023/01/25/Paper-Reading-Types-and-Programming-Languages-Chapter-9-and-Chapter-11/","link":"","permalink":"https://jifengwu2k.github.io/2023/01/25/Paper-Reading-Types-and-Programming-Languages-Chapter-9-and-Chapter-11/","excerpt":"","text":"Summary Chapter 9 of \"Types and Programming Languages\" presents the simply typed lambda calculus, which constructs a type system for pure lambda calculus, explaining theoretical aspects such as the typing relation and the Curry-Howard Correspondence along the way. Chapter 11 introduces simple extensions to the simply typed lambda calculus presented in Chapter 9, such as base types, derived forms, type ascriptions, let bindings, and some compound data structures (pairs, tuples, records, sums, variants, and lists), making it better resemble a real-world programming language. Critique Foreword I have found the textbook hard to follow in many places. Thus, I have followed the textbook and looked into many online resources to grasp the content. Below summarizes my understanding after studying the material. Basic Concepts in Type Theory Terms and Types In Type Theory, every Term has a Type, often written together as &lt;Term&gt;: &lt;Type&gt;. Types include Natural Numbers (nat) and Boolean Logic Values (bool). For example (assuming x: nat and y: nat): 0: nat x: nat 1 + 1: nat x + y: nat true: bool x + y: nat Functions Functions are also Terms with Types, represented as Lambda Terms. A Lambda Term looks like (λ &lt;First Parameter Name&gt;: &lt;First Parameter Type&gt; &lt;Second Parameter Name&gt;: &lt;Second Parameter Type&gt; ... . &lt;Term to Return&gt;). It has type &lt;First Parameter Type&gt; → &lt;Second Parameter Type&gt; → ... → &lt;Type of Term to Return&gt;. This indicates that the Lambda Term is a function that takes Parameters of &lt;First Parameter Type&gt;, &lt;Second Parameter Type&gt;, etc., and returns a Term of &lt;Type of Term to Return&gt;. Examples of Lambda Terms: (λ x: nat . (x + x)): nat → nat: a Function which takes in a Parameter x of Type nat and returns the doubled Parameter. (λ x: nat y: nat . (x + y)): nat → nat → nat: a Function which takes in two Parameters x, y all of Type nat and returns their sum. A Lambda Term is often called an Anonymous Function because it has no Name. We can use the notion to give a Name to a Lambda Term: add: nat → nat → nat ::= (λ x: nat y: nat . (x + y)) Function Applications In Type Theory, a Function Call is called a Function Application, which \"takes a Term of a Type and results in a Term of another Type.\" Function Application is written as &lt;Function&gt; &lt;Argument&gt; &lt;Argument&gt; ... (akin to Function Calls in Haskell and Commands in Unix Shell) instead of the conventional &lt;Function&gt;(&lt;Argument&gt;, &lt;Argument&gt;, ...) in Programming Languages. If we define a Function add that takes two nat's and returns a nat, the following are valid Terms: add 0 0: nat add 2 3: nat add 1 (add 1 (add 1 0)): nat Dependent Typing Sometimes, the Type returned by a Function depends on the Value of its Argument. This is known as Dependent Typing. For example, a function if takes three arguments, with if true b c returning b, and if false b c returning c. If b and c have different Types, then the type of if depends on the value of a. Dependent Typing is a reasonably complicated subject that is an active domain of research. Zero Type, Unit Type, and Universal Type Zero Type In some programming languages, there is a Zero Type or Bottom Type - a Type whose Set of Terms is the empty set and a Subtype of all other Types. In these programming languages, denoting the Zero Type as a Function's Return Type frequently indicates that the Function never returns (never completes computation) - instead, it may loop forever, throw an exception, or terminate the process. As a real-world example, in Rust, the Zero Type is called the Never Type and is denoted by !. It is the kind of calculation that never returns any result. For example, the exit function fn exit(code: i32) -&gt; ! terminates the process without returning. Unit Type In some programming languages, the Unit Type is a Type whose Set of Terms is a singleton set, i.e., the type allows only one value. It is typically used to describe the Argument Type of a Function that doesn't need arguments or the Return Type of a Function whose only goal is to have a side effect. For example: In Haskell, Rust, and Elm, the Unit Type is the Type of the 0-tuple (). In Python, the Unit Type is NoneType, which only has a single instance None. In JavaScript, both Null (which only has a single instance null) and Undefined (which only has a single instance undefined) are Unit Types. In languages such as C, C++, Java, and C#, void, which designates that a Function accepts no Arguments or does not return anything, plays a similar role to the Unit Type. However, there are also key differences: There are no Terms (Instances) of void. A proper Unit Type may always be the Type of an Argument to a Function, but void cannot be the Type of an Argument. Universal Type Most object-oriented programming languages include a universal base class. In Type Theory, this is known as a Universal Type or a Top Type. Its Set of Terms encompasses any valid Term in the programming language, and all other types in the programming language are subtypes. For example: Object in Smalltalk and JavaScript java.lang.Object in Java System.Object in C#, Visual Basic .NET, and other .NET Framework languages object in Python (can also be type-annotated as typing.Any) Any in Scala and Julia Some object-oriented programming languages, such as C++, Objective-C, and Swift, do not have a universal base class. In these languages, some constructs function similarly to the Universal Type. In C++, void * can accept any non-function pointer (even though void itself is more akin to the Unit Type). In Objective-C, id can accept pointers to any object. In Swift, the protocol Any can accept any type. Languages that are not object-oriented usually do not have a Universal Type. Typing Context A Typing Context (or Typing Environment) \\(\\Gamma\\) is a Mapping from Terms to Types (or a collection of Term - Type Pairs). The judgement \\(\\Gamma \\vdash e: \\tau\\) is read as \"\\(e\\) has type \\(\\tau\\) in Context \\(\\Gamma\\)\". In Statically Typed Programming Languages, these Typing Contexts are used and maintained by Typing Rules to Type Check a given Program or Expression. Type Inhabitation Given a Typing Environment, a Type is inhabitated if an existing Term of the Type is available or a Term of the Type can be readily obtained (i.e., via Function Application). Derived Forms In Type Theory, Syntactic Sugar is known as Derived Forms, while replacing a Derived Form with its lower-level definition (usually during compile time) is known as desugaring. For example: In C, a[i] and *(a + 1), a-&gt;x and (*a).x. In the tidyverse collection of R packages, x %&gt;% f(y) is equivalent to f(x, y). A programming language is typically divided into a compact core language, a rich set of syntax defined in terms of that core (Derived Forms), and a comprehensive standard library. This makes the language maintainable for engineers while making it convenient for users. Type Ascription Type Ascription is an assertion within source code that a term has a particular type. This can lead to cleaner, easier-to-understand code documentation. Important Derived Forms Tuple Record (Struct, Rows in a Database) - a collection of Fields, possibly of different Types Variant (Datatype, Tagged Union, Discriminated Union, Disjoint Union) A data structure to hold a Term that could take on \"several different, but fixed Types.\" Contains a Value field and a Tag field Widely used for defining recursive data structures (e.g. Trees containing Leaves and Internal Nodes) List Curry-Howard Correspondence The Curry-Howard Correspondence, independently discovered by logicians Haskell Curry in 1958 and William Howard in 1969, states that \"proofs in a given subset of mathematics are exactly programs from a particular programming language\". Specifically, Types correspond to logical formulas. A Term having a Type can be understood as evidence that the Type is inhabited. For example, 3110: int is evidence that int is inhabited. Logical Atoms \\(a\\), \\(b\\) correspond to whether Types A, B are inhabited. true corresponds to a Type that is always inhabited. The simplest of them all is the Unit Type. false corresponds to a Type that is never inhabited - the Zero Type. Conjunction \\(a \\land b\\) corresponds to a Type inhabited when both Types A and B are inhabited - Tuple[A, B]. Disjunction \\(a \\lor b\\) with the added condition that you know which one of \\(a\\), \\(b\\) is true when \\(a \\lor b\\) is true corresponds to a Type that is inhabited when one of A, B is inhabited, and you know which one is inhabited - Variant[A, B]. Implication \\(a \\rightarrow b\\) corresponds to a Type that, when inhibited, ensures B must be inhabited when A is inhabited - a Function Type, A -&gt; B. Programs correspond to proofs. Analyzing the types of expressions evaluated during the execution of a program corresponds to simplifying a proof. References https://en.wikipedia.org/wiki/Type_theory https://en.wikipedia.org/wiki/Bottom_type https://en.wikipedia.org/wiki/Typing_environment https://softwareengineering.stackexchange.com/questions/277197/is-there-a-reason-to-have-a-bottom-type-in-a-programming-language https://stackoverflow.com/questions/32505911/what-is-the-role-of-bottom-%E2%8A%A5-in-haskell-function-definitions https://doc.rust-lang.org/std/primitive.never.html https://en.wikipedia.org/wiki/Unit_type https://en.wikipedia.org/wiki/Top_type https://cs3110.github.io/textbook/chapters/adv/curry-howard.html#types-correspond-to-propositions https://wiki.haskell.org/Curry-Howard-Lambek_correspondence https://www.pédrot.fr/slides/inria-junior-02-15.pdf https://math.stackexchange.com/questions/2686280/what-do-logicians-mean-by-type https://homepages.inf.ed.ac.uk/stg/NOTES/node35.html https://cs.wellesley.edu/~cs251/s02/scheme-intro.pdf https://cs.brown.edu/~sk/Publications/Papers/Published/pk-resuarging-types/paper.pdf https://en.wikipedia.org/wiki/Syntactic_sugar https://www.wikidata.org/wiki/Q73072308 https://stackoverflow.com/questions/36389974/what-is-type-ascription https://github.com/rust-lang/rfcs/blob/master/text/0803-type-ascription.md https://medium.com/@andrew_lucker/things-you-cant-do-in-rust-type-ascription-5253951c7427 https://docs.scala-lang.org/style/types.html https://futhark-lang.org/examples/type-ascriptions.html https://en.wikipedia.org/wiki/Record_(computer_science) https://en.m.wikipedia.org/wiki/List_(abstract_data_type) Feedback from the Class Discussion An Introduction Rule describes how Elements of the Type can be Created, and is akin to a description of a Constructor. Similarly, an Elimination Rule describes how Elements of the Type can be used in an Expression, and is akin to a description of an Overloaded Operator. A lot of papers propose Typing Rules that don't make much sense in isolation, but can be plugged into other Type Systems to add a Feature (i.e., allow the non-intrusive addition of other Typing Rules). Well-designed Type Systems provide guarantees on a program's behavior (i.e., guarantee predictable runtime behavior). C introduced types, not for verification, but to determine how much space a variable would take up in memory. Uniqueness of Typing (i.e., a Term can only have one Type) doesn't hold when there is Subtyping. Curry Style allows representing errors explicitly and describing the type of errors, which is suitable for languages where things can go wrong. In comparision, Church Style does not allow errors The Erasure Property is built upon the assumption that the Execution of the Program doesn't rely on Types. Type Ascription woule be beneficial for giving hints to the Type Inference/Type Checking Algorithm. Usually, Desugaring happens before Type Checking, as the Type System does not directly handle the Syntactic Sugar. Tuples are also called Sum Types, and Variants are also called Product Types. This is based on how many possible values the Tuple or Variant Type has. For example, std::pair&lt;char, bool&gt; has 256 * 2 = 512 values, std::variant&lt;char, bool&gt; has 256 + 2 = 258 values, and std::optional&lt;char&gt; has 256 + 1 = 257 values. Enums can be seen as Variants where each value is associated with the Unit Type. Tuples and Records are distinct Types because Compilers implement them differently Programming in Dynamically Typed Programming is akin to programming with variables which are Variants of all possible types.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Date and Time Types in Python","slug":"Date-and-Time-Types-in-Python","date":"2022-12-31T05:00:00.000Z","updated":"2025-08-13T04:31:00.447Z","comments":true,"path":"2022/12/31/Date-and-Time-Types-in-Python/","link":"","permalink":"https://jifengwu2k.github.io/2022/12/31/Date-and-Time-Types-in-Python/","excerpt":"","text":"There are many types in Python which can store date and time information. These types can be broadly divided into two categories: JSON Serializable Formats UNIX Timestamp (e.g. 0) ISO 8601 String (e.g. '1970-01-01T00:00:00') UNIX Timestamp has its roots in the system time of Unix operating systems. It is now widely used in databases, programming languages, file systems, and other computer operating systems. It counts the number of seconds that have passed since the Unix epoch began on January 1, 1970 at 00:00:00 UTC, minus any modifications made for leap seconds. ISO 8601 is an international standard for the transmission and interchange of time- and date-related information on a global scale. Dates in the Gregorian calendar, hours based on the 24-hour timekeeping system, with an optional UTC offset, time intervals, and combinations of these are covered by ISO 8601. The standard offers a clear, unambiguous manner of expressing calendar dates and times in international communications, notably to prevent numeric dates and times from being misinterpreted when such data is sent between nations. As the categorization suggests, these formats can be used in JSON serialization, and are widely adopted in data exchange formats and APIs. For example, Stripe APIs use UNIX Timestamps, while Twitter and Dropbox APIs use ISO 8601 Strings. UNIX Timestamps are easier and more efficient to handle, while ISO 8601 Strings have the virtue of being human-readable. Widely Used In-memory Data Structures datetime.datetime (e.g. datetime.datetime(1970, 1, 1, 0, 0)) datetime.date (e.g. datetime.date(1970, 1, 1)) pandas.Timestamp (e.g. Timestamp('1970-01-01 00:00:00')) As the categorization suggests, these formats are in-memory, structured representations of date and time information. datetime.datetime and datetime.date are types implemented (and widely used) in the Python Standard Library. datetime.date represents a date (year, month, day) in an idealized calendar, which is the existing Gregorian calendar infinitely stretched in both directions, while datetime.datetime also combines the data from a time object (hour, minute, second, microsecond). pandas.Timestamp is implemented in pandas. It is the pandas replacement for datetime.datetime, and is the type used for the entries that make up a pandas.DatetimeIndex, and other time series-oriented data structures in pandas. Furthermore, it is also widely used across the Python Ecosystem for Data Science, such as being used by matplotlib as the xticks for plotting a pandas.Series with a pandas.DatetimeIndex, as shown below. References: https://en.wikipedia.org/wiki/Unix_time https://en.wikipedia.org/wiki/ISO_8601 https://dev.to/xngwng/do-you-prefer-unix-epoch-a-number-or-iso-8601-a-string-for-timestamps--28ll https://stackoverflow.com/questions/15554586/timestamps-iso8601-vs-unix-timestamp https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/ https://www.programiz.com/python-programming/datetime/timestamp-datetime https://stackoverflow.com/questions/3743222/how-do-i-convert-a-datetime-to-date https://stackoverflow.com/questions/969285/how-do-i-translate-an-iso-8601-datetime-string-into-a-python-datetime-object https://www.programiz.com/python-programming/datetime/timestamp-datetime https://pynative.com/python-iso-8601-datetime/ https://docs.python.org/3/library/datetime.html https://stackoverflow.com/questions/1937622/convert-date-to-datetime-in-python https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python https://stackoverflow.com/questions/41046630/set-time-formatting-on-a-datetime-index-when-plotting-pandas-series","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Python","slug":"Software-Design/Python","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Python/"}],"tags":[]},{"title":"Paper Reading: Efficient scalable thread-safety-violation detection: finding thousands of concurrency bugs during testing","slug":"Paper-Reading-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing","date":"2022-11-27T05:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/11/27/Paper-Reading-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing/","link":"","permalink":"https://jifengwu2k.github.io/2022/11/27/Paper-Reading-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. This paper presents Thread Safety Violation Detection (TSVD), a tool that dynamically detects thread safety violations with low runtime overhead, and which is compatible with real-world, distributed-developed code employing different synchronization mechanisms. The tool frames thread safety violations as two methods, with one of them being a write operation, occurring concurrently. It infers thread safety violations using a very creative approach. First, it instruments the program and detects method calls that access objects behind thread-safety contracts. Later on, during the execution of the program, TSVD injects delays into threads with method calls accessing those objects and monitors whether another thread also accesses the same objects during the delay. As this may incur significant overhead, the tool uses two strategies to determine when to inject delays - keeping track of \"near misses\", where the two method calls of two threads occur within a time threshold apart from each other, and inferring \"happens before\" relations, to rule out two accesses which are causally related. The tool was tested on 43000 .NET programs in Microsoft teams, and its bug-finding capability outperformed both existing tools and configuring TSVD to emulate the strategies of existing tools, which shows the feasibility of TSVD. There are two questions that come to mind after reading this paper: How does the tool acquire the information on which methods are thread-unsafe? The approach the tool uses to infer thread safely - injecting delays and monitoring the behavior of other threads - sounds very interesting to me. Have there been any other applications of such an approach? What is the sensitivity of the relevant parameters used in TSVD to its effectiveness and efficiency? Is there any guide on how to properly adjust these parameters? Feedback from the Class Discussion The proposed approach can handle different concurrency models, such as: async task-based thread-based But can it handle unstructured concurrency? The approach generalizes data race for objects and data structures at the method-level (e.g. there cannot be two simultaneous calls to add() for a List class). Using delays can handle many more cases than reasoning about thread scheduling. It is a \"simple thing\" which works for many cases (akin to fuzzing). The approach requires manually specifying read and write APIs. Is it possible to create a semi-automatic approach starting from contracts labeled for standard library APIs?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Hybrid dynamic data race detection","slug":"Paper-Reading-Hybrid-dynamic-data-race-detection","date":"2022-11-23T05:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/11/23/Paper-Reading-Hybrid-dynamic-data-race-detection/","link":"","permalink":"https://jifengwu2k.github.io/2022/11/23/Paper-Reading-Hybrid-dynamic-data-race-detection/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. The paper proposes a hybrid approach to dynamically (at runtime) data races in multithreaded Java programs. It first proposes two specific detection approaches, each with its strengths and weaknesses. The first is lockset-based detection, which identifies a data race when multiple threads use a shared memory location without holding a shared lock object. Such an approach is fast but may lead to false positives. As a result, the paper proposes another approach, happens-before detection, which uses several heuristics to reason about relations between events and infer whether a potential race has occurred at a particular memory location. In comparison, this approach is more computationally expensive and may lead to false negatives. Considering that neither approach is sound, they combine the two approaches by first using lockset-based detection to identify potential data races before using happens-before detection to reason whether these are probable. The paper then conducts an experimental study of their hybrid approach on various Java programs, demonstrating its effectiveness and efficiency. I like this paper's idea of combining a pessimistic and optimistic approach when doing program analysis. Are there any other works that use such an idea? However, I have a question concerning the applicability of the hybrid approach in real life. Although pessimistic, shouldn't lockset-based detection be enough to stamp out all potential data races by providing programmers with feedback to add relevant locks to prevent such possible data races? This is relevant to the requirements for defensive programming. Or are there design patterns where multiple threads can safely use a shared memory location without holding a common lock and not lead to data races? Feedback from the Class Discussion Difference Between Race Condition and Data Race: - Race Condition: There are multiple threads, and the behavior of program depends on thread scheduling. - Data Race: Different from race condition. This frequently happens when you parallelize a program that shouldn't be parallelized. Data race can be solved by using locks, but there may still be race coditions. 12345Thread-1:synchronized(...) &#123; x = 1;&#125; 12345Thread-2:synchronized(...)&#123; x = 2;&#125; Modelling in the Paper: - Lamport Timestamps/Vector Clocks - Thread events: statement executions in threads. A thread event is dependent on previous thread events. This is captured used using the happens-before formal definition in the paper, but leads to false negatives. - Thread communications: signals (enforce order) and locks (mutually exclusive). - Message send/receive: enqueue and dequeue. Architecure-dependednt atomic operations can also be a lock-free solution (e.g. C++'s atomic).","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Lightweight Verification of Array Indexing","slug":"Paper-Reading-Lightweight-Verification-of-Array-Indexing","date":"2022-11-16T05:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/11/16/Paper-Reading-Lightweight-Verification-of-Array-Indexing/","link":"","permalink":"https://jifengwu2k.github.io/2022/11/16/Paper-Reading-Lightweight-Verification-of-Array-Indexing/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Summary of the Paper The authors propose a methodology to detect out-of-bound array accesses statically. They first define that criteria that ideal techniques for detecting out-of-bound array accesses should satisfy, before analyzing the insufficiency of existing academic and industrial approaches, and presenting their own approach, Index Checker, implemented for Java. Index Checker reduces checking array bonds to identifying 7 kinds of knowledge, which concern array index and array length, and form a hierarchy. It models such hierarchical knowledge as a Type System, requires the user to write \"Type\" Annotations at procedure boundaries, and verifies that values have the given \"Type\" at runtime. This is implemented using Checker Framework, an \"industrial-strength, open-source tool for building Java type systems\". The authors evaluate Index Checker on 3 large-scale, well-tested Java projects (Google Guava, JFreeChart, Plume-lib), and compare Index Checker with 3 other approaches (FindBugs, KeY, and Clousot), proving the effectiveness of Index Checker (scalability, finding bugs in well-tested programs, and low false positive rate). They also assess the burden of writing type annotations for Index Checker. Questions What is the rationale behind the 7 kinds of knowledge concerning array index and array length proposed in the paper? I am not very familiar with Type Theory, which may have impeded my understanding of the value of the paper. What are the benefits of using Type Systems and Type Inference, and using Type Annotations to capture known constraints? Is it just to leverage the power of Checker Framework, an \"industrial-strength, open-source tool for building Java type systems\", for sound inference? Or are there any further benefits? No matter what the benefits are, from this paper, modeling hierarchical knowledge as a Type System, using Type Annotations to capture known constraints, and using Type Inference to verify such constraints sounds like a very innovative technique with many potential use cases. Have there been any other applications of such a technique? Feedback from the Class Discussion The hierarchy of knowledge is derived from Exploratory Data Analysis (trying stuff until it works, see Section 2.8). \"Subtype\" is a kind of Comparable Partial Ordering ('&lt;'). The Types in the Bottom have more information, while the Types in the Top have less information. In Java, aside from Inheritance, another form of Subtyping is Function Subtyping. e.g. Comparator (to compare two Dog's we can pass a function that compares two Animal's) the inputs can be more general types. Rules define what to do when a Pattern is encountered; however, it takes a (nontrivial) search to determine the order to apply the rules. Fixed Point: Convergence of Information. Reach a Fixed Point: Iterate until Convergence. The Paper uses Subtyping to implement Widening.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Modular Checking for Buffer Overflows in the Large","slug":"Paper-Reading-Modular-Checking-for-Buffer-Overflows-in-the-Large","date":"2022-11-13T05:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/11/13/Paper-Reading-Modular-Checking-for-Buffer-Overflows-in-the-Large/","link":"","permalink":"https://jifengwu2k.github.io/2022/11/13/Paper-Reading-Modular-Checking-for-Buffer-Overflows-in-the-Large/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Background Information Datalog Declarative Programming Language which began as a Query Language for Relational Databases, and is now used in Data Integration, Information Extraction, Program Analysis, Cloud Computing, Machine Learning, etc. Akin to SQL in many aspects. Not Turing Complete. Used as a Domain Specific Language. No Canonical Implementation, many different Implementations exist for different Applications (c.f. SQLite, MySQL, PostgreSQL, etc. for SQL). Follows the 'Logic Programming' Paradigm. A Program consists of Constants, Variables, Facts, and Rules (based on First Order Logic, in a form similar to \"a new Fact A is true if B, C, and D are already known to be true\"). The Execution of a Program is iteratively inferring new Facts given the Rules. Maps very nicely to many problems encountered during Program Analysis. Summary of the Paper The authors proposed a Methodology for detecting possible Buffer Overflow-based Security Exploits in C code and providing developers with instant feedback during the build process. The Methodology prefers usability over accuracy, and should be used alongside other tools in a Swiss Cheese Model against Security Exploits. First, the authors proposed a Simple Annotations Language for annotating Pointers passed as parameters to and returned from Functions, to denote Preconditions and Postconditions of Function Execution. The authors propose that for new code, annotation should be inserted manually, and code should be fully annotated before being checked in to Version Control. For legacy codebases and/or third-party code without such Annotations, the authors propose an Inference Engline, SALInfer, which tries to infer such Annotations, preferring Coverage over Accuracy. SALInfer supports specifying Inference Algorithms using Datalog. Finally, the authors propose a modular checker, ESPX, which tries to infer if a program is potentially vulnerable to Buffer Overflow-based Security Exploits by statically analyzing the annotations within the program's code. The confidence of the inference results vary based on the extent and quality of the annotations. Questions Regarding the Paper What is the relevance of such a technique to \"safe\" programming languages that do not allow using overflowable buffers? The authors state that \"control over annotation insertion is given to individual developers\". However, developers might be reluctant to insert Annotations, and inserting Annotations can negatively affect developer productivity. Furthermore, the quality of the inserted Annotations is not guaranteed. Last but not least, inferring Annotations for legacy codebases and/or third-party code without such Annotations prefers Coverage over Accuracy, which may not lead to sound results. Considering all these real concerns, the practical usability of this tool is seriously compromised. The authors did an evaluation on an unnamed Microsoft product. With little information regarding the product being disclosed, such an evaluation is far from convincing, and I suspect that there might be manipulation of some kind within the evaluation.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Precise Interprocedural Dataflow Analysis via Graph Reachability","slug":"Paper-Reading-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability","date":"2022-11-07T05:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/11/07/Paper-Reading-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability/","link":"","permalink":"https://jifengwu2k.github.io/2022/11/07/Paper-Reading-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. To be honest, I found the paper to be almost unreadable due to it being full of unfamiliar concepts and abstract formalizations. I tried my best to do some studying into the topic so that I can understand the problem that they are trying to solve, and important aspects of their algorithm, better. Graph Reachability Graph Reachability means whether it is possible to get from one vertex to another vertex within a graph. In an Undirected Graph \\(G(V, E)\\), Graph Reachability between one pair of nodes can be calculated using Breadth-First Search, while Graph Reachability between all pair of nodes can be reduced to calculating the Connected Components of the Undirected Graph, which is an efficient algorithm with \\(O(|V| + |E|)\\) time complexity. Connected Components within an Undirected Graph In a Directed Graph, Graph Reachability between one pair of nodes can also be calculated using Breadth-First Search. However, there is no efficient algorithm that can calculate Graph Reachability between all pair of nodes for all Directed Graphs. For any Directed Graph, calculating Graph Reachability between all pair of nodes can be reduced to calculating All Pairs Shortest Distance using the Floyd-Warshall Algorithm, which has an \\(O({|V|}^3)\\) time complexity. More efficient algorithms are only applicable to Planar Directed Graphs. Data Flow Analysis Constant Propogation (determining whether variables at a given point in the program are guaranteed to have constant values) and Live Variable Analysis (determining at a given point in the program, which variables might be used before being overwritten) are two commonly encountered examples of Data Flow Analysis. Given a program's Control Flow Graph, Data Flow Analysis: Associates each Node of the Control Flow Graph with Information concerning the Variables within that Node (known as Dataflow Fact's, usually a Mapping between Variables and their Values or Properties) Models the effect of executing a Node with a Dataflow Function. In most Data Flow Analysis problems, we take one of the following approaches to obtain the Dataflow Facts for each Node: Summarizing paths entering the Node from the Start, such as in Constant Propogation. Known as \"Forward Problem\"'s. Summarizing paths exiting the Node from the Exit, such as in Live Variable Analysis. Known as \"Backward Problem\"'s. How we summarize paths is known as the Confluence Operator. Data Flow Analysis problems can also be divided into \"may\" problems and \"must\" problems. In \"may\" problems, the Dataflow Facts for each Node include information about what may be true. An example is Live Variable Analysis, where we determine whether a variable may be used before being overwritten in a given point in the program. In \"must\" problems, the Dataflow Facts for each Node include information about what must be true. An example is Constant Propogation, where we determine whether a variable must have a given value in a given point in the program. Many interesting Data Flow Analysis problems, such as Live Variable Analysis, can be modeled as GEN/KILL problems, or bit-vector problems, in which: A set of variables, \\(KILL[n]\\), is defined at Node \\(n\\). A set of variables, \\(GEN[n]\\), is used at Node \\(n\\). We use Union or Intersection to summarize paths entering a Node to obtain the Dataflow Facts for the Node. Interprocedural Dataflow Analysis The goal of Interprocedural Dataflow Analysis is to capture an Abstraction of the Effect of calling a Procedure in Dataflow Analysis. A naive approach to Interprocedural Dataflow Analysis is to reduce it to Intraprocedural Dataflow Analysis in some way. Procedure Inlining Exponentially increases the Control Flow Graph Cannot handle recursion Context Sensitive Procedure Inlining Uses Context Information (often an Approximation of the Call Stack) to distinguish between different Calls of the same Procedure, and reduce the number of inlined Procedures. However, even after research, I have failed to understand the more complicated approaches (as well as the approaches proposed in this Paper). I can only get the point that the author shows that many Interprocedural Dataflow Analysis problems, in which: A finite set of Dataflow Facts Dataflow Functions distribute over the Confluence Operator (which I don't fully understand) including GEN/KILL problems, or bit-vector problems, can be reduced to a Graph Reachability Problem on a Directed Graph. Furthermore, I believe the main contribution of this paper is theoretical, but what is its value in real-world Dataflow Analysis problems, especially considering that the Time Complexity of Graph Reachability Problems on Directed Graphs are high? I honestly hope that I can get some insight into these approaches during our class on Monday. Thank you! Feedback from the Class Discussion Some of the paper's idea comes from Abstract Interpretation. It is nice theoretically, but it is far from implementation. Graph Reachability in the context of Interprocedure Analysis is also known as Context-Free Language Reachability and Dyck Reachability. In the context of this paper, there are multiple Dataflow Functions, one for each Node in the Control Flow Graph. Given a Node in the Control Flow Graph, we use Pattern Matching to determine what its Dataflow Function is. Lambdas are used to represent these Dataflow Functions. Explanation for the notations: \\(\\lambda &lt;parameters&gt;.&lt;return\\_value&gt;\\) means def f(&lt;parameters&gt;): return &lt;return_value&gt;. In the context of this paper, we require all Dataflow Functions to be distributive over the Meet Function (Confluence Function). This means that, given the Meet Function \\(\\Pi\\) and a Dataflow Function \\(f\\), \\(f(X~\\Pi~Y) = f(X)~\\Pi~f(Y)\\) for any two Dataflow Facts \\(X, Y\\). Each Dataflow Function can be visualized using a Graph Representation. The Edges represent Dependencies between Facts of the Variables in the Old Dataflow Facts and Facts of the Variables in the New Dataflow Facts. Graph Representation of Dataflow Functions, x is Shorthand for the Facts of x Worklist Algorithm: an Algorithm which takes Objects from a Worklist (a Queue of some sort) one at a time, processes it in some way, and perhaps further adds new Objects to the Worklist, until some Target is reached. Example: Breadth First Search.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Mining Input Grammars from Dynamic Taints","slug":"Paper-Reading-Mining-Input-Grammars-from-Dynamic-Taints","date":"2022-11-02T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/11/02/Paper-Reading-Mining-Input-Grammars-from-Dynamic-Taints/","link":"","permalink":"https://jifengwu2k.github.io/2022/11/02/Paper-Reading-Mining-Input-Grammars-from-Dynamic-Taints/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. A program usually accepts a formal language as input. Inferring the grammar of this formal language is an important task with many use cases. Helps humans understand the structure of the formal language. Manually writing valid inputs Reverse Engineering Generate inputs for testing and fuzzing The authors propose Autogram, a method that infers a Context Free Grammar given a set of sample inputs and a Java program that accepts that set of inputs and uses it in some way. Autogram adapts a Dynamic Taining-based approach: It monitors the data flow of each character within the input, with \"the Input Fragment it came from\" as the taint. It traces method entries, method exits, field accesses, and array accesses within the execution of the program. From such a trace, the Dynamic Call Tree is reconstructed, and the sets of Intervals (Input Fragments) processed by functions, stored in variables, and returned by functions is derived. This is used to build an Interval Tree, and the Interval Tree is refined into a Pure Input Tree free of conflicting overlaps (resulting from parsers using lookaheads). The pure input tree is assumed to be a Parse Tree, and Production Rules are derived from it. The leaf nodes are considered to be Terminals, and Regular Expressions matching them are learned. The authors then conduct an experimental study concerning the accuracy and completeness of the inferred Context Free Grammars using \"parts of the Java Standard API that are used to process URLs and property files\", and \"open source projects that implement support for CSV, INI, and JSON formats\". However, I had more questions than answers after reading this paper. One of the use cases that the authors mentioned is \"the grammar vastly simplifies the creation of parsing programs that decompose existing inputs into their constituents\". Why don't we directly extract the parsing logic out of the program Autogram runs on? The type of Context Free Grammar inferred by Autogram seems to be an LL(1) Grammar. This type of Grammar is only able to represent simple Grammars, and does not support for Left Recursion, which is pervasive in real-world Grammars. Why don't they infer an LALR(1) Grammar, which is both simple and expressive (it supports representing may real-world Programming Languages). Perhaps, a Hidden Markov Model could be trained to infer the Transitions between the States within the LALR(1) Parse Table should an LALR(1) Grammar be inferred? In the current implementation of Autogram, tracing is efficient, as the authors have mentioned: \"millions of calls result in traces of a few Megabytes\". However, the current implementation incurs a ~100x performance overhead, and there is a lot of room for performance optimization. Maybe ideas that we have discussed for TaintCheck and Qsym (direct Binary Analysis, preinstrumenting Bytecode, JIT compilation etc.) could be used here? The specific process of refining an Interval Tree into a Pure Interval Tree free of conflicting overlaps is not described clearly in the paper. Why don't the author present an example with figures showing the manipulation of nodes within the Interval Tree during this process? The author also mentions applying \"a simple heuristic that assumes left to right processing of the input\" to resolve possible ambiguities associated with parsers using lookaheads. However, what is the rationale behind this \"simple heuristic\"? The specific process of deriving Production Rules from the Pure Interval Tree is also unclear. What do the authors mean by \"We can thus check if nodes are compatible and can be used to derive productions for for the same nonterminal symbol\"? What is the meaning of \"compatible\" in this context? The programs used in the experimental study are all open-source programs of very high code quality (containing accurately named variables and functions). However, how well does Autogram work with closed-source programs, and/or programs with low code quality, containing obscure variable and function names? This is frequently the situation we encounter when we try to reverse engineer the (often closed-source and/or obscure) structure of a program's input, one of the major use cases of Autogram. Also some inspiration and ideas I got from the paper: The author mentions that \"dynamic tainting allows us to precisely identify which parts of a programs input are read, stored and processed at any point in time\". Could this technique be used in a Fuzzing context to identify which bits generated by a Coverage-Guided Fuzzer are used in which sections of a fuzzed program? The logic of building an Interval Tree is very interesting, and it reads like the \"Subset Tree\" mentioned in the KLEE paper. I conject that both these Tree Structures could be generalized and used in a much wider range of contexts. Feedback from the Class Discussion A Context Free Grammar may not capture the structure of binary files. How does the approach compare to unsupervised parsing in NLP or fine-tuning language models, especially with a lot of input? Is it possible to use feedback to improve the mined grammar? From one tree, we infer one set of grammar rules; from 1000 trees, we infer 1000 sets of grammar rules. They are merged together to derive the final Context Free Grammar.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software","slug":"Paper-Reading-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software","date":"2022-10-29T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/10/29/Paper-Reading-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/29/Paper-Reading-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. To combat worms spread by the Internet exploiting software vulnerabilities, the paper proposes TaintCheck, a dynamic taint analysis technique for automatic detection of exploits on software. Summary of TaintCheck: TaintCheck directly operates on an arbitrary executable and does not require its source code. It uses Valgrind to translate basic blocks being executed into Valgrind's RISC-like instruction set (UCode), inserts UCode instructions for instrumentation, and passes the modified UCode back to Valgrind for execution. TaintCheck by default considers data originating from the network as \"untrusted\" and taints it. It keeps track of \"the propagation of tainted data as the program executes\", which involves monitoring data movement instructions and arithmetic instructions, with the exception of constant functions such as xor eax, eax. To accomplish this, TaintCheck associates \"each byte of memory\" with a Taint data structure. Different instances of such a data structure are \"chained\" to record \"how tainted data is propagated\". TaintCheck checks whether tainted data is used in ways it considers illegitimate, such as being used as a return address, a function pointer, a format string, and (optionally) as an argument of a system call. When such illegitimate uses are detected, it is possible to collect information about a software vulnerability, especially \"the execution path from tainted data's entry and its use in a probable exploit\". The paper also proposes a new semantic-based automatic signature generation approach on top of TaintCheck. There are several questions that came to my mind when I was reading this paper: The paper mentions that \"the current implementation slows program execution between 1.5 and 40 times\", but also mentions that \"the prototype has not been optimized\", and proposes optimization techniques. Why didn't the authors implement these optimization techniques and conduct experiments on the optimized TaintCheck? There is no doubt that using Valgrind to translate basic blocks being executed into UCode greatly simplifies dynamic taint analysis on an arbitrary executable, as TaintCheck deals with an RISC-like instruction set instead of raw machine code. However, this incurs significant overhead. Would directly performing dynamic taint analysis on machine code at runtime using a dynamic binary instrumentation tool such as Intel Pin boost performance (like the case of QSym)? What about generating UCode, inserting instructions for instrumentation, and passing the modified UCode back to Valgrind before the executable is executed? What is the overhead of using the Taint data structure? Would the total size of all Taint data structures explode for long-running processes? And why do they use this Taint data structure, instead of using a conventional Data Flow Graph? What is the list of constant functions that TaintCheck supports? Is it representative, and is it extensible? Are the ways tainted data is used considered by TaintCheck to be illegitimate representative of real exploits? How well can TaintCheck discriminate from \"illegitimate\" uses with intentional uses, and/or uses with checks? Specifically, the paper mentions that TaintCheck can \"untaint the data immediately after it has been sanity checked\", but how is this situation detected? In the evaluation section, why are the benchmarks used in assessing \"compatibility and false positives\" different from those used in assessing \"attack detection\" on actual exploits? What does a \"signature\" look like, and how is it used to filter attacks? Feedback from the Class Discussion Performance is not a priority. The paper is more of a proof-of-concept, and even \"reads like a grant proposal\", especially Section 6. The \"taint data structure\" includes more information than a dataflow graph (snapshots of the stacks etc.). It can also use a \"memory arena\" instead of vanilla heap allocation to improve performance. Some of the detected attacks may not be present in \"safe\", managed languages. Due to the large overhead, the technique cannot be used to handle requests in production, but requests can be forked to it instead. Dynamic taint analysis can have applications outside of the security domain.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Python in a Functional Style: Closures, Generators, and Coroutines","slug":"Python-in-a-Functional-Style-Closures-Generators-and-Coroutines","date":"2022-10-28T04:00:00.000Z","updated":"2025-08-13T04:31:00.459Z","comments":true,"path":"2022/10/28/Python-in-a-Functional-Style-Closures-Generators-and-Coroutines/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/28/Python-in-a-Functional-Style-Closures-Generators-and-Coroutines/","excerpt":"","text":"Python in a Functional Style: Closures, Generators, and Coroutines Jifeng Wu 2022-10-28 Closures👈 Generators Coroutines All Python functions are closures. Function code. Execution environment of function code (variables it depend on). A nested function can be returned. This is a common design pattern for creating tailored functions. 1234def get_greeting_function(name): def greeting_function(): print(f&#x27;Hello, &#123;name&#125;&#x27;) return greeting_function All Python functions are closures. Function code. Execution environment of function code (variables it depend on). A nested function can be returned. This is a common design pattern for creating tailored functions. 1234567&gt;&gt;&gt; function_greeting_a = get_greeting_function(&#x27;A&#x27;)&gt;&gt;&gt; function_greeting_a()Hello, A&gt;&gt;&gt;&gt;&gt;&gt; function_greeting_b = get_greeting_function(&#x27;B&#x27;)&gt;&gt;&gt; function_greeting_b()Hello, B Look into a closure's cell_contents: 12345678910111213&gt;&gt;&gt; function_greeting_a.__closure__(&lt;cell at 0x7f3c81849ca8: str object at 0x7f3c8185ac70&gt;,)&gt;&gt;&gt; function_greeting_a.__closure__[0]&lt;cell at 0x7f3c81849ca8: str object at 0x7f3c8185ac70&gt;&gt;&gt;&gt; function_greeting_a.__closure__[0].cell_contents&#x27;A&#x27;&gt;&gt;&gt; &gt;&gt;&gt; function_greeting_b.__closure__(&lt;cell at 0x7f3c81849c18: str object at 0x7f3c82f18e30&gt;,)&gt;&gt;&gt; function_greeting_b.__closure__[0]&lt;cell at 0x7f3c81849c18: str object at 0x7f3c82f18e30&gt;&gt;&gt;&gt; function_greeting_b.__closure__[0].cell_contents&#x27;B&#x27; Should an inner function use an outer function's local variable (instead of shadowing it), that local variable should be declared nonlocal within the inner function. Not using nonlocal: 1234567def outer_function(): string = &#x27;Hello&#x27; def inner_function(): # Shadows the local variable `string` of `outer_function` string = &#x27;World&#x27; inner_function() return string 12&gt;&gt;&gt; outer_function()&#x27;Hello&#x27; Should an inner function use an outer function's local variable (instead of shadowing it), that local variable should be declared nonlocal within the inner function. Using nonlocal: 12345678def outer_function(): string = &#x27;Hello&#x27; def inner_function(): # Uses the local variable `string` of `outer_function` nonlocal string string = &#x27;World&#x27; inner_function() return string 12&gt;&gt;&gt; outer_function()&#x27;World&#x27; Creating and returning a nested function based on a function argument is widely used in Python, called decorating a function. 1234567891011121314def cached(function): cache = &#123;&#125; def cached_function(*args): nonlocal function, cache if args in cache: print(f&#x27;Cache hit with args: &#123;args&#125;&#x27;) return cache[args] else: print(f&#x27;Cache miss with args: &#123;args&#125;&#x27;) result = function(*args) print(f&#x27;Writing f(&#123;args&#125;) =&gt; &#123;result&#125; to cache&#x27;) cache[args] = result return result return cached_function Python even has special syntatical support for this. 12345678@cacheddef fib(n): if n &lt; 1: return 0 elif n &lt; 2: return 1 else: return fib(n - 1) + fib(n - 2) 1234567891011121314151617In [4]: fib(5) Cache miss with args: (5,)Cache miss with args: (4,)Cache miss with args: (3,)Cache miss with args: (2,)Cache miss with args: (1,)Writing f((1,)) =&gt; 1 to cacheCache miss with args: (0,)Writing f((0,)) =&gt; 0 to cacheWriting f((2,)) =&gt; 1 to cacheCache hit with args: (1,)Writing f((3,)) =&gt; 2 to cacheCache hit with args: (2,)Writing f((4,)) =&gt; 3 to cacheCache hit with args: (3,)Writing f((5,)) =&gt; 5 to cacheOut[4]: 5 \\(O(n)\\) time complexity. LeetCode problem: Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. Example 1: 12Input: n = 3Output: [&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;] Example 2: 12Input: n = 1Output: [&quot;()&quot;] bg left contain We write a Context Free Grammar and analyze it: 12S -&gt; S S&#x27; | S&#x27; .S&#x27; -&gt; ( S ) | ( ) . https://mdaines.github.io/grammophone/# 123456789101112131415161718192021@cacheddef s_generator(number_of_parenthesis): print(f&#x27;s_generator(&#123;number_of_parenthesis&#125;)&#x27;) return_value = [] # s -&gt; ss . if number_of_parenthesis &gt;= 1: for ss_string in ss_generator(number_of_parenthesis): return_value.append(ss_string) # s -&gt; s ss . if number_of_parenthesis &gt;= 2: for i in range(1, number_of_parenthesis): for s_string, ss_string in itertools.product( s_generator(i), ss_generator(number_of_parenthesis - i) ): return_value.append(s_string + ss_string) return return_value 123456789101112131415@cacheddef ss_generator(number_of_parenthesis): print(f&#x27;ss_generator(&#123;number_of_parenthesis&#125;)&#x27;) return_value = [] # ss -&gt; ( ) . if number_of_parenthesis == 1: return_value.append(&#x27;()&#x27;) # ss -&gt; ( s ) . if number_of_parenthesis &gt; 1: for s_string in s_generator(number_of_parenthesis - 1): return_value.append(&#x27;(&#x27; + s_string + &#x27;)&#x27;) return return_value 12Input: n = 3Output: [&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;] 1234567891011121314In [4]: s_generator(3) s_generator(3)ss_generator(3)s_generator(2)ss_generator(2)s_generator(1)ss_generator(1)Out[4]: [&#x27;((()))&#x27;, &#x27;(()())&#x27;, &#x27;()(())&#x27;, &#x27;(())()&#x27;, &#x27;()()()&#x27;]In [5]: s_generator.cache_info() Out[5]: CacheInfo(hits=3, misses=3, maxsize=None, currsize=3)In [6]: ss_generator.cache_info() Out[6]: CacheInfo(hits=3, misses=3, maxsize=None, currsize=3) Closures also provide an efficient mechanism for maintaining state between several calls. Traditional (OOP) approach: 12345678class Countdown: def __init__(self, n): self.n = n def next_value(self): old_value = self.n self.n -= 1 return old_value Closure-based approach: 12345678def countdown(n): def get_next_value(): nonlocal n old_value = n n -= 1 return old_value return get_next_value This is not only clean but also fast. 12345678910111213def test_object_oriented_approach(): c = Countdown(1_000_000) while True: value = c.next_value() if value == 0: breakdef test_functional_approach(): get_next_value = countdown(1_000_000) while True: value = get_next_value() if value == 0: break 12345In [5]: %timeit test_object_oriented_approach()182 ms ± 2.61 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)In [6]: %timeit test_functional_approach()96.8 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Why? 12345678910111213141516In [9]: c = Countdown(1_000_000)In [10]: dis(c.next_value) 6 0 LOAD_FAST 0 (self) 2 LOAD_ATTR 0 (n) 4 STORE_FAST 1 (old_value) 7 6 LOAD_FAST 0 (self) 8 DUP_TOP 10 LOAD_ATTR 0 (n) 12 LOAD_CONST 1 (1) 14 INPLACE_SUBTRACT 16 ROT_TWO 18 STORE_ATTR 0 (n) 8 20 LOAD_FAST 1 (old_value) 22 RETURN_VALUE 12 instructions, 2 LOAD_ATTR instructions, 1 STORE_ATTR instruction. 123456789101112In [11]: get_next_value = countdown(1_000_000)In [12]: dis(get_next_value) 4 0 LOAD_DEREF 0 (n) 2 STORE_FAST 0 (old_value) 5 4 LOAD_DEREF 0 (n) 6 LOAD_CONST 1 (1) 8 INPLACE_SUBTRACT 10 STORE_DEREF 0 (n) 6 12 LOAD_FAST 0 (old_value) 14 RETURN_VALUE 8 instructions, NO LOAD_ATTR, STORE_ATTR instructions. Closures Generators👈 Coroutines When we define a function containing the yield keyword, we define a generator. Defining a generator allows the user to define a custom iterator in the style of defining a function. 1234def countdown(n): while n &gt; 0: yield n n -= 1 We create a generator object when we call a generator definition. The generator object can be used like any iterator: 1234567891011121314In [2]: c = countdown(5)In [3]: next(c)Out[3]: 5In [4]: next(c)Out[4]: 4In [5]: for value in c: ...: print(value) ...:321 When we call next() on a generator object, it will execute code, until it encounters a yield statement. The yield statement tells the generator object to return a value, and continue execution from here when next() is called again. 1234In [2]: c = countdown(5)In [3]: next(c)Out[3]: 5 This executes: 12while n &gt; 0: yield n When we call next() on a generator object, it will execute code, until it encounters a yield statement. The yield statement tells the generator object to return a value, and continue execution from here when next() is called again. 12In [4]: next(c)Out[4]: 4 This executes: 123 n -= 1while n &gt; 0: yield n This is called lazy evaluation. This can dramatically boost performance and reduce memory usage in some applications. For example: 1234567891011def get_comments_from_file(file): with open(file, &#x27;r&#x27;) as fp: for line in fp: # strip whitespace stripped_line = line.strip() # check if the line is empty after stripping whitespace if stripped_line: # check if the line is a comment if stripped_line[0] == &#x27;#&#x27;: # if it is, yield it yield stripped_line This will NOT read the whole file into memory. Only when the user calls next() on the generator object, will the generator read the file LINE BY LINE (with only ONE LINE of the file in memory at once), and return the next comment line. This is an efficient way of extracting comments from GB-sized files (such as logs). itertools Python provides many functions for creating an iterator from another iterator. For example: itertools.permutations(iterable [, r]) itertools.combinations(iterable, r) itertools.product(iter1, iter2, iterN, [repeat=1]) Widely used in algorithms: itertools.permutations(iterable [,r]) 123456789101112131415In [1]: import itertoolsIn [2]: numbers = range(4)In [3]: permutations_of_two_numbers_iterator = itertools.permutations(numbers, r=2)In [4]: next(permutations_of_two_numbers_iterator)Out[4]: (0, 1)In [5]: next(permutations_of_two_numbers_iterator)Out[5]: (0, 2)In [6]: next(permutations_of_two_numbers_iterator)Out[6]: (0, 3) Widely used in algorithms: itertools.combinations(iterable ,r) 12345678910111213In [1]: import itertoolsIn [2]: numbers = range(4)In [3]: for first, second in itertools.combinations(numbers, 2): ...: print(first, second) ...:0 10 20 31 21 32 3 Widely used in algorithms: itertools.product(iter1, iter2, iterN, [repeat=1]) 1234567891011121314In [1]: import itertoolsIn [2]: first_list = [1,2,3]In [3]: second_list = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;]In [4]: third_list = [True,False]In [5]: it = itertools.product(first_list, second_list, third_list)In [6]: next(it)Out[6]: (1, &#x27;a&#x27;, True)In [7]: next(it)Out[7]: (1, &#x27;a&#x27;, False)In [8]: next(it)Out[8]: (1, &#x27;b&#x27;, True) Closures Generators Coroutines👈 Starting from Python 2.5, the yield statement can be used as an right value: 1captured_input = yield value_to_yield Generators defined like this can accept sent input while providing output. These generators are called coroutines. The concept of coroutines was proposed in the 60s, but only gained traction in recent years. Coroutines can be seen as a combination of subroutines and threads. Can pause and restart during execution. Controlled by itself instead of the operating system. Different coroutines run within a thread are concurrent instead of parallel. Simple example: 1234567891011121314import mathdef update_mean(): current_input = yield sum = current_input count = 1 while True: current_input = yield sum / count sum += current_input count += 1 Simple example: 123In [3]: updater = update_mean()In [4]: next(updater) This executes: 1current_input = yield And the coroutine waits for an input to be sent. Send an input: 12In [5]: updater.send(2)Out[5]: 2.0 The coroutine receives the input, and executes: 1234sum = current_inputcount = 1while True: current_input = yield sum / count And the coroutine waits for an input to be sent. Send an input: 12In [6]: updater.send(4)Out[6]: 3.0 The coroutine receives the input, and executes: 1234 sum += current_input count += 1while True: current_input = yield sum / count And the coroutine waits again for an input to be sent. More complicated example: set-associative cache simulation number_of_cache_sets * Set number_of_ways_of_associativity * Block block_size_in_bytes * Byte The whole set-associative cache is a coroutine receiving (address, is_write) tuples as input, and calculating (cache_hit, writeback_address) tuples as output. It models each set as a coroutine receiving (tag, is_write) tuples as input, and calculating (cache_hit, writeback_address) tuples as output. Different coroutine definitions for round-robin, LRU, etc. The whole set-associative cache 1234567891011121314151617181920212223242526272829303132def cache_coroutine(cache_set_coroutine_function, block_size_in_bytes, number_of_ways_of_associativity, number_of_cache_sets): # create cache_set_coroutine_list and activate each cache_set_coroutine cache_set_coroutine_list = [ cache_set_coroutine_function(number_of_ways_of_associativity) for _ in range(number_of_cache_sets) ] for cache_set_coroutine in cache_set_coroutine_list: next(cache_set_coroutine) # get function_to_split_address and function_to_merge_address function_to_split_address, function_to_merge_address = get_functions_to_split_and_merge_address( block_size_in_bytes, number_of_cache_sets ) # receive address, is_write # yields nothing address, is_write = yield while True: # splits address tag, cache_set_index, offset = function_to_split_address(address) # send (tag, is_write) to the appropriate cache_set_coroutine cache_hit, victim_tag, writeback_required = cache_set_coroutine_list[cache_set_index].send((tag, is_write)) # create writeback_address if (victim_tag is not None) and writeback_required if (victim_tag is not None) and writeback_required: writeback_address = function_to_merge_address(victim_tag, cache_set_index, 0) else: writeback_address = None # receive address, is_write # yield cache_hit, writeback_address address, is_write = yield cache_hit, writeback_address Cache Set with LRU replacement policy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def lru_cache_set_coroutine(associativity): tag_list = [ None for _ in range(associativity) ] dirty_bit_list = [ False for _ in range(associativity) ] indices_in_lru_order = OrderedDict() for index in range(associativity - 1, -1, -1): indices_in_lru_order[index] = None # receive first tag and is_write tag, is_write = yield while True: cache_hit = False victim_tag = None writeback_required = False try: # find tag_index tag_index = tag_list.index(tag) # tag_index found cache_hit = True if is_write: dirty_bit_list[tag_index] = True # move tag_index to the end of indices_in_lru_order indices_in_lru_order.move_to_end(tag_index) except ValueError: # tag_index not found # get index_of_victim from indices_in_lru_order index_of_victim, _ = indices_in_lru_order.popitem(last=False) victim_tag = tag_list[index_of_victim] if dirty_bit_list[index_of_victim]: writeback_required = True tag_list[index_of_victim] = tag if is_write: dirty_bit_list[index_of_victim] = True else: dirty_bit_list[index_of_victim] = False # insert index_of_victim to the end of indices_in_lru_order indices_in_lru_order[index_of_victim] = None # receive tag and is_write # yield (cache_hit, victim_tag, writeback_required) tag, is_write = yield (cache_hit, victim_tag, writeback_required) Suppose our cache has only eight blocks and each block contains four words. The cache is 2-way set associative, so there are four sets of two blocks. The write policy is write-back and write-allocate. LRU replacement is used. https://courses.cs.washington.edu/courses/cse378/02sp/sections/section9-3.html 12345678910111213141516171819202122232425In [3]: cache = cache_coroutine(lru_cache_set_coroutine, block_size_in_bytes=4 * ...: 2, number_of_ways_of_associativity=2, number_of_cache_sets=4) In [4]: next(cache) In [5]: cache.send((0, True)) Out[5]: (False, None)In [6]: cache.send((64, False)) Out[6]: (False, None)In [7]: cache.send((4, True)) Out[7]: (True, None)In [8]: cache.send((40, True)) Out[8]: (False, None)In [9]: cache.send((68, False)) Out[9]: (True, None)In [10]: cache.send((128, True)) Out[10]: (False, 0)In [11]: cache.send((0, False)) Out[11]: (False, None)","categories":[{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Python","slug":"Software-Design/Python","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Python/"}],"tags":[]},{"title":"Paper Reading: The Fundamentals of Writing Questions","slug":"Paper-Reading-The-Fundamentals-of-Writing-Questions","date":"2022-10-26T04:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2022/10/26/Paper-Reading-The-Fundamentals-of-Writing-Questions/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/26/Paper-Reading-The-Fundamentals-of-Writing-Questions/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. This part of the book addresses the problem of crafting survey questions that respondents are willing to answer and respond to accurately. It first discusses issues to consider when designing survey questions, then presents the structure of a survey question and different question formats, before providing specific guidelines on wording survey questions. Issues to consider when designing survey questions What concepts to measure Recommended: Adopt established measures from existing surveys What data to collect Factual information: precise, readily available Opinion: requires time to formulate, strongly influenced by context Behavior: better memory of recent, memorable events compared with distant, mundane events What question format to use Different cognitive information processing for aural and visual surveys What mode to adopt The presence of an interviewer may speed up surveys, but may induce social desirability and acquiescence, leading to interviewer bias. Lack of standardization among different interviewers may lead to interviewer variance. What to modify (from existing surveys) no changes or only minimal changes when replicating or comparing results questions should also be asked in a similar fashion How to motivate respondents think about the cognitive process respondents go through pay attention to the context and wording The structure of a survey question Question stem Additional instructions Answer spaces or choices Different question formats Open-ended rich, detailed more prone to skipping requires lengthy data processing Closed-ended nominal or ordinal categories set of answer choices known in advance easy to analyze Partially closed-ended closed-ended with \"other\" response respondents more likely to select the options instead of \"other\" Specific guidelines on wording survey questions Choose the appropriate question format. Make sure the question applies to the respondent. Ask one question at a time. Make sure the question is technically accurate. Use simple, familiar and specific words. Use short, simple sentences that take a question form. Avoid double negatives. Organize questions in a more straightforward, comprehensible way. Also sprinkled throughout the section is the notion that the crafter should get into a respondent's state of mind when crafting survey questions, and also test the survey questions to evaluate their quality. This section is very comprehensive and convincing, as the author supports his arguments by analyzing specific examples from actual surveys, and also frequently quoting previous work on the topic. From such a chapter we can gain a deep understanding of the nature of survey questions, especially the underlying cognitive, psychology and sociology problems, as well as the best practices within the domain, and we can also refer to this chapter as a guide and checklist when we craft survey questions ourselves.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: All You Ever Wanted to Know About Dynamic Taint Analysis and Forward Symbolic Execution (but might have been afraid to ask)","slug":"Paper-Reading-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask","date":"2022-10-25T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2022/10/25/Paper-Reading-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/25/Paper-Reading-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Forward symbolic execution and dynamic taint analysis are quickly becoming \"staple techniques in security analyses\". Dynamic taint analysis runs a program and observes which computations are affected by predefined taint sources such as user input. Dynamic forward symbolic execution automatically builds a logical formula describing a program execution path, which reduces the problem of reasoning about the execution to logic, allowing us to reason about the behavior of a program on many different inputs at one time. The two analyses can be used in conjunction to build formulas representing only the parts of an execution that depend upon tainted values. Forward symbolic execution: Test Case Generation (automatically generate inputs to test programs, generate inputs that cause two implementations of the same protocol to behave differently) Automatic Input Filter Generation (input filters that detect and remove exploits from the input stream) Cristian Cadar, Daniel Dunbar, and Dawson Engler. Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs. In Proceedings of the USENIX Symposium on Operating System Design and Implementation, 2008. Cristian Cadar, Vijay Ganesh, Peter Pawlowski, David Dill, and Dawson Engler. EXE: A system for automatically generating inputs of death using symbolic execution. In Proceedings of the ACM Conference on Computer and Communications Security, October 2006. Patrice Godefroid, Nils Klarlund, and Koushik Sen. DART: Directed automated random testing. In Proceedings of the ACM Conference on Programming Language Design and Implementation, 2005. Dynamic taint analysis: Unknown Vulnerability Detection (misuses of user input) Automatic Network Protocol Understanding. Dynamic taint analysis has been used to automatically understand the behavior of network protocols when given an implementation of the protocol. Malware Analysis (analyze how information flows through a malware binary, explore trigger-based behavior, and detect emulators) However, there has been little effort to formally define them and summarize critical issues that arise when applying these techniques in \"typical security contexts\". The authors formalize the runtime semantics of dynamic taint analysis and forward symbolic execution by using SIMPIL (Simple Intermediate Language), which is \"representative of internal representations used by compilers and is powerful enough to express typical languages\". Concepts: Statements: assignments, assertions, jumps, conditional jumps. Expressions: constants, variables, binary operators, unary operators, get_input. Execution state: the list of program statements, the current memory state, the current value for variables, the program counter, the current statement. Notation: \\(\\Sigma\\): list of program statements \\(\\Sigma[v_1]\\): statement at \\(pc = v_1\\). \\(\\mu\\): memory state \\(\\mu[v_1]\\): memory content at address \\(v_1\\) \\(\\Delta\\): register state (values of all variables) \\(\\Delta[x]\\): value of variable \\(x\\) \\(\\Delta[x \\leftarrow 10]\\): setting the value of variable \\(x\\) to 10 \\(pc\\): program counter. \\(\\mu, \\Delta \\vdash e \\Downarrow v\\): Given memory state \\(\\mu\\) and register state \\(\\Delta\\), the value of expression \\(e\\) is \\(v\\). \\(\\Sigma, \\mu, \\Delta, pc, EXPRESSION \\rightsquigarrow \\Sigma, \\mu&#39;, \\Delta&#39;, pc&#39;, {EXPRESSION}&#39;\\): Given list of program statements \\(\\Sigma\\), memory state \\(\\mu\\), register state \\(\\Delta\\), program counter \\(pc\\), executing expression \\(EXPRESSION\\) leads to new memory state \\(\\mu&#39;\\), new register state \\(\\Delta&#39;\\), new program counter \\(pc&#39;\\), and the next expression is \\({EXPRESSION}&#39;\\). Other high-level language constructs such as functions or scopes can be easily represented using these constructs. Dynamic taint analysis tracks values in a program dependent on data derived from a \"taint source\" at runtime. As it is conducted at runtime, it can be expressed by extending SIMPIL. Dynamic taint analysis is conducted in different ways (i.e., under different \"taint policies\") for different applications. The differences lie in \"how new taint is introduced to a program\", \"how taint propagates as instructions execute\", and \"how taint is checked during execution\". The author presents the example of the \"tainted jump policy\" for attack detection, points out several challenges it faces, and analyzes the proposed solutions. \"Distinguishing between memory addresses and cells is not always appropriate\". An alternative \"tainted addresses policy\" could be used, but this may also overtaint. Information flow can occur through control dependencies in addition to dataflow dependencies. This requires \"reasoning about multiple paths\", while pure dynamic taint analysis \"executes on a single path at a time\". Solutions include \"supplementing dynamic analysis with static analysis\" and \"using heuristics\". Taint is only added and never removed (i.e., \"sanitized\"), leading to the problem of \"taint spread\", reducing precision. Well-known constant functions (i.e. using XOR to zero out registers in x86 code) can be checked. In addition, we can consider the outputs of some functions like cryptographic hash functions as untainted, due to limited influence of input on output. This can be quantified (Newsome et al.) to automatically recognize such cases. Furthermore, values can be untainted \"if the program logic performs sanitization itself\" (e.g., index bounds checking). In conclusion, this paper is an useful introductory paper in forward symbolic execution and dynamic taint analysis, and I have mainly learned the following two things from the paper: The idea of formalizing runtime semantics using RISC-like bytecode An introduction to dynamic taint analysis - what it is, what it can do, and what challenges it faces Feedback from the Class Discussion What is the difference between a statement and an expression? A statement can modify program state when it is executed, while an expression doesn't modify program state. In the formalism of SIMPIL, we determine which expression to evaluate by pattern-matching the rule. LLVM has a dataflow sanitization pass, which may be useful for implementing taint analysis. Dynamic program analysis only looks at a single path. If we are to prove something about a program, static program analysis would be a better direction.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: ReCrash: Making Software Failures Reproducible by Preserving Object States","slug":"Paper-Reading-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States","date":"2022-10-25T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/10/25/Paper-Reading-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/25/Paper-Reading-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. Background Reproduction is key to finding and fixing software problems and verifying proposed solutions, but reproduction can be difficult. Nondeterminism: A problem may depend on timing (e.g., context switching), memory layout (e.g., hash codes), or random number generators. Remote detection: A problem may be discovered by someone other than the developer, and it may depend on implicit program inputs such as user GUI actions, environment variables, the state of the file system, operating system behavior, etc. This information may be easy to miss, difficult to collect, or confidential. Test case complexity: The exposing execution might be complex, and the buggy method might be called multiple times before the bug is triggered. Proposed Solution: ReCrash ReCrash maintains a shadow stack with copies of the receiver and arguments to each method during execution of the target program. Several copy strategies Several optimizations When the program crashes, ReCrash serializes the shadow stack, and generates unit tests by calling each method on the shadow call stack with their receiver and arguments. Calling the method at top of the call stack may not provide enough context. Calling a method closer to the bottom provides more context, but is less likely to reproduce the original failure. Proposed Solution: ReCrash Assumption: It is possible to reproduce many failures with only some of the information available on entry to the methods on the stack at the time of the failure. Many bugs are dependent on small parts of the heap. Good object-oriented style encapsulates important state nearby. Good object-oriented style avoids excessive use of globals. ReCrash has access to and will store any parts of the global state or environment that are passed as method arguments. Question: What if global state is read or written in the method? Monitoring Phase Several copy strategies Several optimizations Monitoring fewer methods Second-chance mode Copy Strategies An argument may be side-effected between the method entry and the point of the failure in the method. Copying strategies: Reference: copying only the reference to the argument. Shallow: copying the argument itself. Depth-i: copying all the state reachable with \\(\\le i\\) dereferences from the argument. Deep-copy: copying the entire state. Options: Used-fields: deeper copying on fields that are used (read or written) in the method. ReCrash always uses the reference strategy for immutable parameters. Monitoring Fewer Methods Dosen't monitor methods that cannot be used in the generated tests, or are unlikely to expose problems. non-public methods empty methods simple methods such as getters and setters (no more than 6 opcodes) Second-chance Mode ReCrash initially monitors no method calls. Each time a failure occurs, ReCrash enables method argument monitoring for all methods found on the stack trace. Efficient, but requires a failure to be repeated twice. If the developer doesn't mind missing the first time a failure happens, and the failure occurs relatively often, second chance mode is a good fit. Question: could recording all inputs provided to the program be used in tandom with second-chance mode (such that the failure is probable to happen the second time)? Test Generation Phase ReCrash generates a test for each of the methods in the shadow stack. Restores the state of the arguments that were passed to a method. Invokes the method the same way it was invoked in the original execution. Only tests that end with the same exception as the original failure are saved. Storing more than one test that ends with the same failure is useful. Some tests reproduce a failure, but would not help the developer understand, fix, or check her solution. Experimental Study Subject programs: Javac-jsr308: the OpenJDK Java compiler, extended with JSR308 (\"Annotations on Java Types\"), with four crashes provided by the developers. SVNKit: a subversion client, with three crash examples from bug reports. Eclipsec: a Java compiler included in the Eclipse JDT, with a crash found in the Eclipse bug database. BST: a toy subject program used by Csallner in evaluating CnC, with three crashes found by CnC. Experimental Study For each subject program: Run PIDASA for parameter immutability classification. For different argument copying strategies, with and without second-chance mode: Run ReCrash on inputs that made the subject programs crash. Count how many test cases reproduced each crash. Question: how useful would ReCrash be in reality where it is unknown whether the subject projects could crash, and which inputs would make the subject programs crash? Experimental Study Research questions: How reliably can ReCrashJ reproduce crashes? What is the size of the stored deep copy of the shadow stack? Are the tests generated by ReCrash useful for debugging? Like a case study: an analysis of two crashes, and comments from developers What is the overhead (time and memory) of running ReCrash? Aspects assessed: different argument copying strategies with and without second-chance mode How reliably can ReCrash reproduce crashes? ReCrash was able to reproduce the crash in all cases. For some crashes, every candidate test case reproduces the crash. For other crashes, only a subset of the generated test cases reproduces the crash. In most cases, simply copying references is enough to reproduce crashes. In other cases, using the shallow copying strategy with used-fields was necessary. What is the size of the stored deep copy of the shadow stack? Subject Programs and Crashes Used in our Experimental Study Question: why isn't it compared with the program size and the program memory usage? An analysis of two crashes Eclipsec bug e1: Eclipsec crashes in callee canBeInstantiated because an earlier if statement in the caller resolveType failed to set a boolean flag hasError to true. The test case for canBeInstantiated will reproduce the crash, but is not helpful. Demonstrates importance of generating tests for multiple methods on the stack. Javac-jsr308 bug j4: Compiling source code containing an annotation with too many arguments results in an index-out-of-bounds exception in method visitMethodInvocation. The generated test does not require the whole source code and encodes only the necessary minimum to reproduce the crash. Useful when the compiler crash happens in the field, and the user cannot provide the entire source code for debugging. Comments from Developers We gave the tests for j1-4 to two Javac-jsr308 developers and asked for comments about the tests' usefulness, receiving positive responses. I often have to climb back up through a stack trace when debugging. ReCrash seems to generate a test method for multiple levels of the stack, making it useful. I find that you wouldn't have to wait for the crash to occur again useful. When I set a break point, the break point maybe be executed multiple times before the error. Using ReCrash, I was able to jump (almost directly) to the necessary breakpoint. Question: Why only analyze two crashes and ask only two developers? What is the overhead (time and memory) of running ReCrash? Time overhead Non second-chance mode: Copying only the references can be expensive (11%-42%), and shallow copying with used-fields is similar (13%–60%). Usable for in-house testing. Deep copying is completely unusable (12,000%-638,000%). Second-chance mode: A barely noticeable 0%–1.7% under copying only the references and shallow copying with used-fields, after a crash has already been observed. What is the overhead (time and memory) of running ReCrash? Memory overhead Non second-chance mode: 0.2M–4.7M (2.6%-90.3%) under shallow copying with used-fields. Second-chance mode: negligible Conclusions ReCrashJ is usable in real software deployment Simple to implement Scalable Generates simple, helpful test cases that effectively reproduce failures Time and memory overhead (13%–60%, 2.6%-90.3%) under non second-chance mode and shallow copying with used-fields usable for in-house testing Extremely efficient under second-chance mode","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Evolutionary Generation of Whole Test Suites","slug":"Paper-Reading-Evolutionary-Generation-of-Whole-Test-Suites","date":"2022-10-24T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/10/24/Paper-Reading-Evolutionary-Generation-of-Whole-Test-Suites/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/24/Paper-Reading-Evolutionary-Generation-of-Whole-Test-Suites/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Background Automatically deriving test cases for realistically sized programs: Select one coverage goal (e.g., program branch) at a time, and derive a test case that exercises this particular goal. Solving path constraints generated with symbolic execution / dynamic symbolic execution Meta-heuristic search techniques Mutation testing Alternative approaches not directly aimed to achieve code coverage Randoop incrementally generate sequences of function calls to find buggy test sequences requires automated oracles (e.g. developer-written assertions and exceptions) Problems Many coverage goals are unreachable. 1234567891011void push(int x) &#123; if (size &gt;= values.length) &#123; resize(); &#125; if (size &lt; values.length) &#123; values[size++] = x; &#125; else &#123; // UNREACHABLE &#125;&#125; Problems Some coverage goals are more difficult to satisfy than others. The order of coverage goals is important: a lucky choice can result in a good test suite, while an unlucky choice can result in a waste of resources. 12345678910111213void push(int x) &#123; if (size &gt;= values.length) &#123; // HARD resize(); &#125; else &#123; // EASY &#125; if (size &lt; values.length) &#123; values[size++] = x; &#125;&#125; Problems Satisfying a particular coverage goal frequently entails satisfying further coverage goals by accident. The order of coverage goals is important. 123456789int pop() &#123; if (size &gt; 0) &#123; // May imply coverage in `push` and `resize` return values[size]; &#125; else &#123; throw new EmptyStackException(); &#125;&#125; Our Solution: EvoSuite Optimize an entire test suite at once instead of considering distinct test cases. Evolve a population of test suites towards satisfying a coverage criterion. Assume automated oracles are not available, and require the outputs of the test cases to be manually verified. The generated test suites should be of manageable size. Solves the problem of: difficult and unreachable coverage goals order of coverage goals accidentally satisfying further coverage goals Our Solution: EvoSuite Questions: We are interested in sequences in OOP. Should coverage in terms of a new ordering seen in the last \\(n\\) function calls in the sequence should make more sense? (Praveen) It seems like Evosuite offloads the responsibility of adding in correct assertions to the developers. How easy is it for the developers to do this, especially when compared with manually writing all of the test suite? (Shizuko, ToTo, Larry) EvoSuite Modeling Population 1 .. M Test Suite 1 .. N Test Case 1 .. L Statement Four types of statements are modeled. Primitive statements: numeric variables (e.g. int var0 = 54;) Constructor statements: new instances of a class (e.g. Stack var1 = new Stack();). All parameters of the constructor call have to be values of previous statements. Field statements: public fields of objects (e.g. int var2 = var1.size;). If the field is non-static, then the source object of the field has to be a value of a previous statement. Method statements: public methods of objects (e.g. int var3 = var1.pop();). The source object and all parameters have to be values of previous statements. EvoSuite Modeling The set of available classes, their public constructors, methods, and fields are extracted from the given software under test. An optimal solution is a test suite that covers all the feasible branches/methods and is minimal in the number of statements. EvoSuite Process Overview Randomly generate a set of initial test suites. Evolve using evolutionary search towards satisfying a coverage criterion. Minimize the best resulting test suite. Questions: How are test suites randomly generated? The author discusses \"sampling\". Where are we sampling from? (Larry, Jifeng) Evolutionary Search Test Suite Fitness Function Crossover Accepting the Mutated Offspring Bloat Control Test Suite Fitness Function Covering all branches \\(B\\) and methods \\(M\\) of a program. To estimates how close a test suite \\(T\\) is to covering all branches \\(B\\) of a program, for each branch \\(b\\), minimal branch distance \\(d_{min}(b, T)\\) is measured. If the branch predicate is \\(x \\ge 10\\), and during execution, \\(x == 5\\), then the minimal branch distance is \\(10 - 5 = 5\\). The minimal branch distance is then normalized to get the branch distance \\(d(b, T) = f(d_{min}(b, T))\\), where \\(f(x) = \\frac{x}{x + 1}\\). \\(fitness(T) = |M| - |M_T| + \\sum_{b \\in B}{d(b, T)}\\) If execution exceeds a time limit of 5 minutes, maximum fitness is automatically assigned. Test Suite Fitness Function Questions: What does branch distance actually mean? Why do we use it? (Eric, Rut, Yayu, Udit, Jifeng) Doesn't \\(\\sum_{b \\in B}{d(b, T)}\\) already consider that branch distances are maximal in unvisited methods? Why do we need an additional \\(|M| - |M_T|\\) term? Furthermore, different methods could have a different number of branches. Should the branch distance sum for all branches within a method be normalized? (Jifeng) Crossover Rank selection based on the fitness function is used to select two parent test suites \\(P_1\\) and \\(P_2\\) for crossover. In case of ties, smaller test suites are assigned better ranks. During crossover: a random value \\(\\alpha\\) is chosen from \\((0, 1)\\) the first offspring test suite \\(O_1\\) will contain the first \\(\\alpha |P_1|\\) test cases from \\(P_1\\) and the last \\((1 - \\alpha)|P_2|\\) test cases from \\(P_2\\) the second offspring test suite \\(O_2\\) will contain the first \\(\\alpha |P_2|\\) test cases from \\(P_2\\) and the last \\((1 - \\alpha)|P_1|\\) test cases from \\(P_1\\) because test cases are independent, \\(O_1\\) and \\(O_2\\) will always be valid Mutation The two offspring test suites \\(O_1\\) and \\(O_2\\) are then mutated. When a test suite T is mutated, each of its test cases is mutated with probability \\(\\frac{1}{|T|}\\). If a test case \\(t\\) is mutated, remove statements, change statements, and insert statements are each applied with probability \\(\\frac{1}{3}\\). Then, a number of new random test cases are added to \\(T\\). Remove Statements If a test case \\(t\\) contains \\(n\\) statements, each statement is removed with probability \\(\\frac{1}{n}\\). If the removed statement \\(s_i\\) is subsequently used by \\(s_j (j &gt; i)\\), try to replace this use with another statement before \\(s_j\\). If this is not possible, recursively remove \\(s_j\\). If all statements have been removed from \\(t\\), remove \\(t\\) from \\(T\\). Change Statements If a test case \\(t\\) contains \\(n\\) statements, each statement is changed with probability \\(\\frac{1}{n}\\). If the changed statement \\(s_i\\) is a primitive statement, its numeric value is changed by a random value. Otherwise, a method, field, or constructor with the same return type is randomly chosen. Insert Statements With probability \\(p\\), a new statement is inserted at a random position in the test case. With probability \\(p^2\\), a second statement is inserted, and so on. Questions: What are the justifications for the probabilities? (Kevin) Can we change the probabilities used in the mutation and insertion by using method calls they kept track of and variables generated in each iteration? (Joyce) When deleting, if the statement is chosen from the beginning few statements, is there a high probability that many/multiple following statements would be removed? Because an initial statement usually has a higher probability of containing an initialization/declaration function. (Rut) Why is the probability of inserting the first, second, etc. statement different? This is not the case with remove statements and change statements. (Jifeng) To mutate and generate test cases, the GA algorithm should have knowledge of the programming language constructs, fields &amp; methods of the software under test, etc. Does this require a significant engineering effort? (Udit) Accepting the Mutated Offspring The coverage achieved by the Mutated Offspring is measured by the Test Suite Fitness Function. Conditions for accepting the mutated offspring: The coverage achieved by the Mutated Offspring exceeds that achieved by its parents, or is on par with that achieved by its parents, and that the mutated offspring are shorter. Their length do not exceed twice that of the Test Suite with the best coverage in the community. Accepting the Mutated Offspring Questions: Are the parents removed before adding the children? (Rut) Compared with the single branch strategy, only the crossover is different, and the mutation is done in the same way. (Tarcisio) Bloat Control A variable size representation could lead to bloat, where small negligible improvements in the fitness value are obtained with larger solutions. This is a very common problem in Genetic Programming. The following measures are used for bloat control: Limit the maximum number \\(N\\) of test cases within a test suite and the maximum number of statements \\(L\\) within a test case. (still need to choose comparatively larger \\(N\\) and \\(L\\) and then reduce their length during/after the search to dramatically boost coverage) Crossover selection policy Mutated offspring acception policy Bloat Control Questions: Does coverage-guided fuzzing, which uses a variant of Genetic Programming, suffer from bloat? If so, could any measures be applied to solve this problem? (Jifeng) How to reduce the length during/after the search? (Yayu, Jifeng) Evaluation EvoSuite is compared with the traditional single branch approach on top of EvoSuite infrastructure. Offspring is generated using the crossover function, but is conducted on two sequences of statements. Because there are dependencies between statements, the statements of the second part are appended one at a time, trying to satisfy dependencies with existing values, generating new values if necessary. The traditional approach level plus normalized branch distance fitness function is used. The two approaches are compared on five open source libraries and a subset of an industrial case study project previously used by Arcuri et al. The units are testable without complex interactions with external resources and are not multithreaded. Evaluation \"Best practices\" based on past experience are used for EvoSuite: Population size: 80 Maximum test suite size \\(N = 100\\) Maximum test case size \\(L = 80\\) The initial test suites are generated with 2 test cases each Initial probability for test case insertion: 0.1 Crossover probability: 3 / 4 Initial probability for statement insertion: 0.5 Evaluation The search operators for test cases make use of only the type information in the test cluster, and so difficulties can arise when method signatures are imprecise. To overcome this problem for container classes, we always put Integer objects into container classes, and cast returned Object instances back to Integer. As the length of test cases can vary greatly and longer test cases generally have higher coverage, we decided to take the number of executed statements as execution limit. The search is performed until either a solution with 100% branch coverage is found, or \\(k = 1,000,000\\) statements have been executed as part of the fitness evaluations. Evaluation Questions: Why not compare EvoSuite to any other (non genetic-testing based) approach? (Zack) Why \"the units are testable without complex interactions with external resources and are not multithreaded\"? (Marie) Is there a justification for these \"best practices\"? (Praveen, Kevin, Madonna, Jifeng) Do the \"best practices\" overfit the 5 open-source libraries? (Joyce) Why the choice of an Integer? And does it work in practice? Given that the internals of the program might be expecting something else? (Rut) Results Whole test suite generation achieves higher coverage than single branch test case generation. Whole test suite generation produces smaller test suites than single branch test case generation. Results Questions: While we have focused on branch coverage in this paper, the findings also carry over to other test criteria is an unwarranted extrapolation. (Zack) Evosuite claims that the test cases are smaller, but how much smaller? (not obvious from Figure 7) (ToTo) High coverage test suite does not necessary mean high bug-finding abilities. How does the performance compare to other tools? (ToTo, Praveen, Kevin, Madonna) The authors did not evaluate EvoSuite against a human in software engineering. Whether EvoSuite will improve the ability to test software from a software developer's point of view is unknown. (Marie)","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Personal Website Design Considerations","slug":"Personal-Website-Design-Considerations","date":"2022-10-20T04:00:00.000Z","updated":"2025-08-31T21:03:30.323Z","comments":true,"path":"2022/10/20/Personal-Website-Design-Considerations/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/20/Personal-Website-Design-Considerations/","excerpt":"","text":"Hosting We host our personal website on GitHub Pages, a static site hosting service. Considerations: No need to buy/rent and set up infrastructure, such as Elastic Computing instances, Domain Name, Content Distribution Network, Load Balancer, DDoS protection Hosted directly from GitHub repository Our personal website meets its limitations: Non-commercial. No confidential information. Published GitHub Pages sites may be no larger than 1 GB. GitHub Pages sites have a soft bandwidth limit of 100 GB per month. GitHub Pages sites have a soft limit of 10 builds per hour. Implications: Static pages. Limit content of our personal website to text and lightweight multimedia, such as vector graphics and vector PDFs. Use raster graphics sparingly, and avoid heavyweight multimedia such as audio and video. Do not rebuild too frequently (&gt;10 builds per hour). Framework Our personal website uses the Hexo blog framework. Considerations: Support for GitHub Flavored Markdown. Easy-to-use CLI. One-command deploy to GitHub Pages. Support for two types of pages (Posts and Pages), adequate for a personal website. Huge library of spectacular, feature-packed and customizable themes. Theme Our personal website uses the fluid theme for Hexo. Considerations: Appropriate Features Support for many third-party commenting systems. Mathjax support, renders equations like \\(E=mc^2\\). Mermaid support. Social network links. Extremely Detailed Documentation. Actively Maintained. Our Considerations When Writing Posts Make the Markdown file as self-contained as possible. This includes: Using third-party pictures from the Internet with stable URLs whenever possible. Utilize fluid's support for Mermaid, and use Mermaid to describe and render in real-time diagrams such as Flowcharts, Sequence Diagrams, Class Diagrams, State Diagrams, and Mindmaps whenever possible, as opposed to including diagrams generated using other tools.","categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"}],"tags":[]},{"title":"Paper Reading: Feedback-Directed Random Test Generation","slug":"Paper-Reading-Feedback-Directed-Random-Test-Generation","date":"2022-10-18T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/10/18/Paper-Reading-Feedback-Directed-Random-Test-Generation/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/18/Paper-Reading-Feedback-Directed-Random-Test-Generation/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Overview The authors present Randoop, a feedback-directed random unit test generator for object-oriented programs which generates sequences of method calls that create and mutate objects, and uses feedback obtained from executing the sequences to guide the search towards new sequences. Logic of Randoop Randoop builds sequences incrementally starting from an empty set of sequences. In each iteration, it generates and executes a new sequence. Sequence Generation First, it selects a method randomly among the public methods of classes. Second, it finds arguments to provide to the method. If an argument is a primitive type, select a primitive value from a fixed pool of values. If an argument is a reference type, select an extensible value of the corresponding type from a previously generated sequence in \\(nonErrorSeqs\\) and put the previous generated sequence into a temporary list if possible, or select null otherwise. Third, a new sequence is formed by concatenating the sequences in the temporary list and the randomly selected method. Fourth, the new sequence is checked whether it has been generated before. If so, the process is repeated. Furthermore, the authors considers that repeated calls to a method may increase code coverage (e.g. reach code that increases the capacity of a container object, or reach code that goes down certain branches). Thus, with a probability \\(p = 0.1\\), instead of appending a single call of a chosen method, a maximum of \\(N = 100\\) calls are appended. Sequence Execution After a new sequence is generated, each method call in the sequence is executed, and after each call, contracts are checked. Default contracts checked by Randoop include: method throws no NullPointerException if no input parameter was null method throws no AssertionError o.equals(o) returns true and throws no exception o.hashCode() throws no exception o.toString() throws no exception If at least one contract is violated, the sequence is put in \\(errorSeqs\\), and no values within the sequence can be extended. If all contracts are not violated, the sequence is put in \\(nonErrorSeqs\\), and all values within the sequence are checked whether they can be extended. If the value has been encountered before, is null, or an exception occurs when executing the sequence leading to the value, the value cannot be extended. Experimental Study The authors evaluate the effectiveness of Randoop through three experiments. Comparing the basic block and predicate coverage of Randoop and five systematic input generation techniques on four container data structures used previously to evaluate these systematic input generation techniques. Comparing Randoop with JPF (a systematic testing technique) and undirected random testing on 14 widely-used libraries. A case study using Randoop to find regression errors between different implementations of the Java JDK. The experimental results strongly suggest that Randoop outperforms systematic and undirected random test generation in both coverage and error detection. Personal Thoughts In my opinion, a key advantage of Randoop is the \"sparse, global sampling\" that it performs, which \"retains the benefits of random testing (scalability, simplicity of implementation)\", while avoiding undirected random testing's pitfalls (generation of redundant or meaningless inputs), and is better adapted to large-scale library code than the \"dense, local sampling\" of systematic test generation. The sequences Randoop builds are akin to seeds in coverage-guided fuzzing, and I believe the efficiency and effectiveness of Randoop may be further boosted by applying a power schedule to the built sequences, much like applying a power schedule to the seeds in coverage-guided fuzzing. The built sequences could possibly have overlapping prefixes. Would using a tree structure be better than storing each sequence on its own? Randoop only supports a limited number of contracts, and its error-detection ability is rather weak. It may be appropriate on library code filled with assertions and checks, but may not work well on client code where these may be sparse.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Software tools to facilitate research programming","slug":"Paper-Reading-Software-tools-to-facilitate-research-programming","date":"2022-10-18T04:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2022/10/18/Paper-Reading-Software-tools-to-facilitate-research-programming/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/18/Paper-Reading-Software-tools-to-facilitate-research-programming/","excerpt":"","text":"This is a paper recommended to me by Margo Seltzer during a conversation. The original paper can be found here. Definition and Ubiquity of Research Programming Research programming is a unique form of programming where the primary objective is to derive insights from data. It's a widespread activity. Not only is it crucial for academic advancements across various disciplines including natural sciences, engineering, and social sciences, but it also extends beyond academia. By some estimations, the number of individuals engaged in research programming dwarfs the number of professional software developers, suggesting its vast scale and significance. Moreover, even professionals in fields like science, engineering, business, finance, public policy, and journalism engage in research programming. Challenges in Research Programming Data Management and Provenance. Keeping track of where each piece of data originates and ensuring it remains up-to-date is crucial. This process can be tedious and difficult, especially when large amounts of data are involved. Organizing, naming, and managing various versions of data files present challenges. Data Preparation: A significant portion of time is spent on data cleaning and reformatting. This task can often be labor-intensive and not directly contribute to deriving insights, yet it's unavoidable. The process is more than just computational number crunching. It often involves transferring data between different tools, converting data formats, and managing extensive datasets. Analysis Phase The core activity involves writing and refining programs to analyze data. Challenges arise from scripts that take excessive time to run, especially after incremental edits, and from scripts crashing due to various errors. Managing output files, including keeping track of metadata, presents additional challenges. Reflection Phase Researchers analyze outputs, take notes, hold meetings, and make comparisons. Graphs play a significant role in visualizing and interpreting results. Managing and comparing these graphical outputs is vital. Dissemination Phase Once the research is complete, results need to be consolidated and communicated, often in the form of reports or academic publications. Reproducing results becomes challenging with time, especially with evolving software environments. Sharing code and data in collaborative settings introduces its own set of challenges. Distinct Nature of Research Programming Compared to Traditional Software Engineering Purpose: Unlike software engineering, which focuses on creating robust software, research programming prioritizes insights. Environment: Research programmers work in a diverse environment using various languages and tools, making it inherently heterogeneous. Specifications: The research programming process is more fluid and iterative, with changing specifications based on new discoveries. Priorities: The emphasis is on quick iteration for faster discoveries rather than perfecting the code. Expertise: A broad range of individuals, not just professional programmers, engage in research programming. The Role of Modern-Day Tools Modern tools designed for general programming can be beneficial for research programmers. However, these tools are often not optimized for the unique characteristics of research programming. A balance needs to be struck between the robustness of software engineering tools and the flexibility required for research programming. Evolution of Documentation in Research Historically, scientific research was documented meticulously in handwritten lab notebooks. But with the rapid pace of computational research, such traditional methods are no longer sufficient. While many research programmers use digital note-taking methods, an ideal solution would seamlessly integrate notes with source code and data files. Closing Thought Recognizing and understanding these challenges are the first steps. It then becomes possible to leverage techniques from various domains, such as dynamic program analysis and recommendation systems, to enhance the productivity of research programmers. High-level Comments I am very interested in investigating existing formalizations and crystallized best practices for specific tasks in Research Programming, such as \"The Grammar of Graphics\" for visualization tasks, and how functional programming can synergize with them. A clean-sheet functional design has the potential to open new windows in addressing many of these challenges, especially those that lack adequate tool support. Ideally, while embracing a functional cleaniness, it should follow several aspects of the UNIX and C++ philosophies. It must be driven by actual problems and its features should be immediately useful in real world programs. It should support and encourage the user to design and build software, even large-scale ones such as operating systems, to be tried early. It should provide facilities for organising programs into separate, well-defined parts, and provide facilities for combining separately developed parts. It should make it easy to make the output of every parts become the input to another, as yet unknown, parts. Allowing a useful feature is more important than preventing every possible misuse. It should work alongside other existing programming languages, rather than fostering its own separate and incompatible programming environment. If the programmer's intent is unknown, it should allow the programmer to specify it by providing manual control.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Research Programming","slug":"Paper-Reading/Research-Programming","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Research-Programming/"}],"tags":[]},{"title":"Paper Reading: A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering","slug":"Paper-Reading-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering","date":"2022-10-16T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/10/16/Paper-Reading-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/16/Paper-Reading-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Overview There are many problems in software engineering which are undecidable and use randomized algorithms, such as automated unit test generation, random testing, and search algorithms (including Genetic Algorithms). As the outcomes of these randomized algorithms vary greatly from run to run, assessing their effectiveness is an important topic. To uncover whether randomized algorithms are properly assessed in software engineering research, the authors conducted a small-scale systematic review on three representative software engineering venues, namely IEEE Transactions of Software Engineering (TSE), IEEE International Conference on Software Engineering (ICSE) and International Symposium on Search Based Software Engineering (SSBSE), in the year 2009. The review shows that the analyses \"are either missing, inadequate, or incomplete\", and \"randomness is not properly taken into account\". The authors then put forward guidelines for properly assessing randomized algorithms in software engineering research. Definitions Censoring a condition in which only the range (i.e. above a certan value, below a certain value, within an interval) of a measurement or observation is known, and its precise value is unknown. Akin to clamping in saturated arithmetic. Commonly encountered in software engineering experiments when time limits are used. Assessment Procedure and Guidelines A novel randomized algorithm is commonly compared against an existing technique. After determing a measure to compare (e.g. source code coverage, execution time), we should run both algorithms a large enough number of times independently (the author recommends \"a very high number of runs\" and not the rule of thumb of \\(n = 30\\) in medicine and behavioral science, as human aspects are not involved. With the collected measure data, we conduct the following: Statistical Testing We use a statistical test to assess \"whether there is enough empirical evidence to claim a difference between the two algorithms\". In such a statistical test, the null hypothesis is typically \"there is no difference\", and we verify whether we should reject the null hypothesis. Definitions related to Statistical Testing There are two conflicting types of error when performing statistical testing: (I) we reject the null hypothesis when it is true, and (II) we accept the null hypothesis when it is false. The p-value of a statistical test is the probability of rejecting the null hypothesis when it is true. The significant level \\(\\alpha\\) of a statistical test is the highest p-value we accept for rejecting the null hypothesis. There is a tradition of using \\(\\alpha = 0.05\\) in the natural sciences. However, an increasing number of researchers believe that, and the author endorses that, such thresholds are arbitrary, and that researchers should \"simply report p-values and let the reader decide in context\". The statistical power of a statistical test is the probability of rejecting the null hypothesis when it is false. Selection of Statistical Test In different statistical tests, different probability distributions of the collected measures are assumed, and different aspects of the probability distributions of the collected measures are being compared. Common statistical tests include: parametric Student's t-test Welch's t-test F-test ANOVA nonparametric Fisher exact test Wilcoxon signed ranks test Mann-Whitney U-test When selecting a statistical test, tt is worth paying attention to the probability distributions of the collected measures: there may be a \"very strong departure from normality\" the mean and variance may not exist the data may be censored Effect Size Measurement In addition to using a statistic test to assess improvement of one algorithm over another, it is also critical to assess \"the magnitude of the improvement\", for which effect size measures are used. Unstandardized effect size measures: dependent on the unit of measurement difference in mean Standardized effect size measures: d family / Mahalanobis distance, assumes the normality of the data Common Language (CL) Statistic. The probability that a randomly selected score from the first population \\(X_1\\) is greater than a randomly selected score from the second population \\(X_2\\), \\(P(X_1 &gt; X_2)\\). Measure of Stochastic Superiority. A generalization of Common Language Statistic, \\(A_{12} = P(X_1 &gt; X_2) + 0.5 P(X_1 = X_2)\\). Recommended. Odds ratio. A measure of \"how many times greater the odds are that a member of a certain population will fall into a certain category than the odds are that a member of another population will fall into that category\". If the total number of runs is \\(n\\), and the number of times two algorithms find optimal solutions are \\(n_1\\) and \\(n_2\\), then the odds ratio is \\(\\psi = \\frac{\\frac{n_1}{n - n_1}}{\\frac{n_2}{n - n_2}}\\). Recommended. Multiple Statistical Tests and Effect Size Measurements When comparing \\(k\\) algorithms, we frequently would like to know the performance of each algorithm \"compared against all other alternatives individually\". This incurs \\(\\frac{k (k - 1)}{2}\\) comparisons. However, when doing multiple stastical tests, given a significant level \\(\\alpha\\) and the number of tests \\(n\\), the probability that at least one null hypothesis is true is \\(1 - {(1 - \\alpha)}^n\\), which converges to \\(1\\) as \\(n\\) increases. A remedy is the Bonferroni adjustment, in which we use an adjusted significant level \\(\\frac{\\alpha}{n}\\). However, this has been \"seriously criticized in the literature\", and the author recommends \"simply report p-values and let the reader decide in context\" instead.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Exploiting Dynamic Information in IDEs Improves Speed and Correctness of Software Maintenance Tasks","slug":"Paper-Reading-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks","date":"2022-10-16T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/10/16/Paper-Reading-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/16/Paper-Reading-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. Summary The pervasive use of inheritance, interfaces, and runtime polymorphism in object-oriented software systems leads to it being unclear which concrete method is invoked at a call site. Modern IDEs such as Eclipse offer static views of the source code, but provide little help in revealing the execution paths an object-oriented software system actually takes at runtime. In this situation, developers usually resort to debuggers or profilers. However, the information extracted by such tools are volatile, and developers cannot continuously benefit from dynamic information integrated in the static source views in the IDE. To solve this problem, the authors propose Senseo, an Eclipse plugin that collects dynamic information by running unit and/or system tests of the project with a customized JVM, that enriches the source views of Eclipse with dynamic information, such as: which concrete methods a particular method invokes, and how often which methods invoke this particular method how many objects are allocated in methods the dynamic collaborations between different source artifacts a visualization of the system's Calling Context Tree These are displayed in tooltips, ruler columns, the Package Explorer, and a dedicated Collaboration Overview. The authors conducted an experiment with 30 professional Java developers solving five typical software maintenance tasks in JEdit, an unfamiliar, medium-sized software system, measured the time and correctness of the tasks, and conducted statistical tests on the measurements. Senseo yields a significant decrease in time of 17.5 percent and a significant increase in correctness of 33.5 percent, which validates the practical usefulness of Senseo. Personal Thoughts There is no doubt that the idea of enriching the source views of an IDE with dynamic information, as well as its implementation Senseo, is of great practical value to developers writing object-oriented software systems. However, I do have a few concerns after reading the paper. To enrich the source views of Eclipse with dynamic information, Senseo runs unit and/or system tests of the project with a customized JVM. There are several concerns here. The project should have unit and/or system tests that thoroughly exercise all units in a manner resembling an actual execution of the project in production, otherwise, the dynamic information for some units may be missing and/or inaccurate. The unit and/or system tests should be self-contained and not rely on interacting with the environment, such as getting input from the user, using OS services, etc. If so, a possible remedy would be to carve unit tests from such executions. There is significant overhead in the process of collecting dynamic information. As the authors have reported: \"On average (geometric mean), CCT creation alone causes an overhead of factor 2.68. CCT creation and collection of dynamic information result in an overhead of factor 9.07. The total overhead, including serialization/transmission, is of factor 9.47.\" Although the authors claim that \"even though the overall overhead is high when gathering dynamic information, we do not consider this a major issue as the application does not need to run at productive speed while analyzing it\", this could be a problem for lengthy system tests, especially if units in the system tests are frequently modified, and new dynamic information has to be reacquired. Carving unit tests from such system tests would also be a possible remedy. Furthermore, aside from the idea and implementation of the tool, something else I appreciate and have learned from this paper is the experimental study, in which two measures, the time and correctness of the tasks, are selected, and statistical tests on the measurements are conducted. This convincingly proves the effectiveness of Senseo.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Breaking the Barriers to Successful Refactoring: Observations and Tools for Extract Method","slug":"Paper-Reading-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method","date":"2022-10-13T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/10/13/Paper-Reading-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/13/Paper-Reading-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. Refactoring is important to software development. Performing a refactoring is not trivial, for which refactoring tools have been developed. Nevertheless, programmers do not use refactoring tools as often as they could. To investigate this problem, the authors focus on one type of refactoring and one specific tool - the Extract Method tool in the Eclipse IDE. Fowler reports that Extract Method is \"one of the most common refactorings\", \"a key refactoring\" which if successful, means \"you can go on [to do] more refactorings\". The Extract Method tool in the Eclipse IDE it is a mature, non-trivial refactoring tool. Most refactoring tool user interfaces are very similar. The authors first conject tools are non-specific and unhelpful in diagnosing problems, and undertake a formative study observing 11 programmers perform a number of Extract Method refactorings on several large, open-source projects, which suggest that programmers fairly frequently encounter a variety of errors arising from violated refactoring preconditions. The authors further conjecture error messages were conflated, insufficiently descriptive, and discouraged programmers from refactoring, and built three visualization tools within the Eclipse IDE as solutions. Then, they conducted a study to assess whether or not the new tools overcome these usability problems by comparing the accuracy and time to complete refactoring tasks with and without the new tools, and administered a post-test questionnaire for the subjects to express their preferences. The results of the study were very positive, and subjects found the new tools superior and helpful outside of the context of the study. Finally, the authors provide recommendations for future tools. Code Selection: A selection tool should be lightweight, task-specific, and help the programmer overcome unfamiliar/unusual code formatting. Displaying Violated Preconditions: quickly comprehensible, indicate location, easily distinguishable from warnings and advisories, display amount of work required, display relations between precondition violations, distinguish different types of violations. The experimental study is very concise, and there are many aspects that can be borrowed. Undertaking a formative study to verify conjections about problems within current tools, before building new tools based on the verified conjections, and evaluating them. The visualization comparing the the accuracy and time of each participant to complete refactoring tasks with and without the new tools is accurate and straightforward. Using a questionnaire to acquire subjective feedback complimentary to an objective evaluation. However, there are still some flaws. Only one type of refactoring (Extract Method) and one specific tool was considered. The takeaways may not apply to other types of refactoring. Several key variates were not controlled in the formative study, such as participants were free to refactor whatever code they thought necessary. Future directions of work include: Replicating the study for other types of refactoring. Build and assess new refactoring tools with increased usability.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing","slug":"Paper-Reading-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing","date":"2022-10-11T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/10/11/Paper-Reading-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/11/Paper-Reading-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? How was it addressed by prior work? There are two notable technologies to automatically find vulnerabilities in software: Coverage-guided fuzzing, quickly explores the input space, but only good at discovering inputs leading to an execution path with loose branch conditions Concolic execution, good at finding inputs driving the program into tight and complex branch conditions, but very expensive to formulate and solve constraints A hybrid approach, hybrid fuzzing, was recently proposed. The fuzzer will quickly explore trivial input spaces (loose conditions) The concolic execution will solve the complex branches (tight conditions) Still suffer from scaling to find real bugs in real-world applications. Bottlenecks are their concolic executors. The symbolic emulation is too slow in formulating path constraints, and it is often not even possible to generate constraints due to incomplete and erroneous environment models. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you? Concolic executors adopt IR in their symbolic emulation. Although IR makes implementation easy, it incurs additional overhead and blocks further optimization. According to our measurement with real-world software, only 30% of instructions require symbolic execution. This implies an instruction-level approach has an opportunity to reduce the number of unnecessary symbolic executions. Concolic execution engines use snapshot techniques to reduce the overhead of re-executing a target program when exploring its multiple paths. However, in hybrid fuzzing, test cases from the fuzzer are associated with greatly different paths, rendering snapshoting inefficient. Furthermore, snapshots cannot reflect external status, and solving this problem through full system concolic execution or external environment modeling is expensive and/or inaccurate. Concolic execution tries to guarantee soundness by collecting complete constraints. However, this can be expensive, and also over-constrain a path, limiting finding future paths. To solve these problems, Qsym uses Intel Pin along with a coverage-guided fuzzer: Get input test cases and validate newly produced test cases (potentially unsound) from the fuzzer. Employ instruction-level taint tracking, and only symbolically execute tainted instructions. Generate more relaxed (incomplete) forms of constraints that can be easily solved (can result in unsound test cases, but quickly checked with fuzzer). Fast execution makes re-execution much preferable to snapshoting for repetitive concolic testing. Considers external environments as \"black-boxes\" and simply executes them concretely (can result in unsound test cases, but quickly checked with fuzzer). Chooses the last constraint of a path for optimistic solving. It typically has a very simple form, and avoids solving irrelevant constraints repeatedly tested by fuzzers. This can be applied to other domains to speed up symbolic execution, if the domain has an efficient validator like a fuzzer. If a basic block has been executed too frequently in a context (a call stack of the current execution), Qsym stops generating further constraints from it. Extremely suitable for loops. This can directly be applied to other concolic executors as a heuristic path exploration strategy. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? A series of experiments are conducted. To highlight the effectiveness, we applied QSYM to non-trivial programs that are large in size and well-tested - all applications and libraries tested by OSS-Fuzz. To show how effectively our concolic executor can assist a fuzzer in discovering new code paths, we measured the achieved code coverage during the fuzzing process using Qsym and AFM with a varying number of input seed files. We selected libpng as a fuzzing target because it contained various narrow-ranged checks. To show the performance benefits of QSYM's symbolic emulation, we used the DARPA CGC dataset to compare QSYM with Driller, which placed third in the CGC competition. To evaluate the effect of optimistic solving, we compared Qsym with others using the LAVA dataset, a test suite that injects hard-to-find bugs in Linux utilities to evaluate bug-finding techniques. To show the effect of basic block pruning, we evaluated Qsym with and without this technique with four widely-used open-source programs - libjpeg, libpng, libtiff, and file. The author then analyzes new bugs found by Qsym. These experiments comprehensively assess different innovations and support the notion that Qsym \"scales to find real bugs in real-world applications\". However, I do have some questions concerning the experimental study, stated below. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Qsym generates more relaxed (incomplete) forms of constraints that can be easily solved. Specifically how this is done is not clear. Questions concerning the experimental study: The experiments \"to highlight the effectiveness\" and \"to show the performance benefits of QSYM's symbolic emulation\" seem to be redundant. To show how effectively our concolic executor can assist a fuzzer in discovering new code paths, we compared Qsym with AFM on libpng, because it contained various narrow-ranged checks. The benchmark appears to be cherry-picked. This is also the case with \"to show the effect of basic block pruning\". Why are completely different datasets used in different experiments? Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? The coverage-guided fuzzer used within Qsym is \"vanilla\" AFL. Other coverage-guided fuzzers exist that enhance AFL. How Qsym can complement these fuzzers can be a direction for future research. Unlike other IR-based executors, QSYM cannot test programs targeting other architectures. We plan to overcome this limitation by improving QSYM to work with architecture specifications, rather than a specific architecture implementation. (Is taint analysis on IR+JIT also a possible solution?) QSYM currently supports only memory, arithmetic, bitwise, and vector instructions. Other instructions, including floating-point operations, remain to be supported.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Semantic Fuzzing with Zest","slug":"Paper-Reading-Semantic-Fuzzing-with-Zest","date":"2022-10-11T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/10/11/Paper-Reading-Semantic-Fuzzing-with-Zest/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/11/Paper-Reading-Semantic-Fuzzing-with-Zest/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? The paper tackles the problem of generating random, syntactically valid inputs to exercise various code paths in the semantic analysis stages of programs and leveraging feedback to generate new inputs via mutations. How was it addressed by prior work? On one hand, QuickCheck-like random-input generators allow generating random, syntactically valid inputs. On the other hand, coverage-guided fuzzing tools such as AFL and libFuzzer randomly mutate known byte sequences to produce new byte sequences, and if the mutated byte sequences lead to new code coverage in the test program, they are saved for subsequent mutation. What are the innovation(s) proposed in this paper? The paper proposes Zest, a technique for automatically guiding QuickCheck-like random-input generators to exercise various code paths in the semantic analysis stages of programs. It first converts a QuickCheck-like random-input generator to a parametric generator, which can generate a syntactically valid input from a byte sequence. It then uses a coverage-guided fuzzing technique with the parametric generator in order to produce syntactically valid input that can increase code coverage in the semantic analysis stages. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? The authors integrated Zest into the open-source JQF framework and evaluated Zest on five real-world Java benchmarks, comparing it to QuickCheck and AFL. They evaluated the three techniques on two fronts: The amount of code coverage achieved in the semantic analysis stage after a fixed amount of time. Their effectiveness in triggering bugs in the semantic analysis stage. QuickCheck and Zest make use of generators for synthesizing syntactically valid input, and do not exercise code paths corresponding to parse errors in the syntax analysis stage. In contrast, AFL performs mutations directly on raw input strings, and spends most of its time testing error paths within the syntax analysis stages. The experimental results suggest that when given QuickCheck-like random-input generators, Zest excels at exercising semantic analyses and is very effective at discovering semantic bugs. The paper's evaluation matches well with the proposed problem statement, as the experimental design accurately assesses factors directly correlated with the problem of \"generating random, syntactically valid inputs to exercise various code paths in the semantic analysis stages of programs and leveraging feedback to generate new inputs via mutations\", and the experimental results support the effectiveness of the proposed approach. Which technical innovations are most compelling to you? The most compelling technical innovation is Zest's design of generating a syntactically valid input from a byte sequence given a QuickCheck-like random-input generator, by using bytes from the byte sequence to \"fill in\" randomly generated primitive data types of various length (bool, char, int, etc.) required within the random-input generator. This allows bit-level mutations on byte sequences to correspond to high-level structural mutations in the space of syntactically valid inputs, enabling Zest to leverage the mature coverage-guided fuzzing algorithm originally designed for byte sequence inputs. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? The author states that the Zest algorithm \"extends the CGF algorithm by keeping track of the coverage achieved by semantically valid inputs\", and that \"we hypothesize that this biases the search towards generating even more valid inputs and in turn increases code coverage in the semantic analysis stage\". However, how semantically valid inputs are used is not stated in the description of the algorithm. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? Zest assumes the availability of QuickCheck-like random-input generators to exercise the semantic analysis classes and find semantic bugs, which may be unavailable for specialized data structures. There has also been some recent interest in automatically generating input grammars from existing inputs, using machine learning and language inference algorithms. These techniques are complementary to Zest - the grammars generated by these techniques could be transformed into parametric generators for Zest.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: How We Refactor, and How We Know It","slug":"Paper-Reading-How-We-Refactor-and-How-We-Know-It","date":"2022-10-10T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/10/10/Paper-Reading-How-We-Refactor-and-How-We-Know-It/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/10/Paper-Reading-How-We-Refactor-and-How-We-Know-It/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? How was the work validated? In his book on refactoring, Fowler catalogs 72 different refactorings, ranging from localized changes to more global changes, and Fowler claims that refactoring produces significant benefits. Although case studies have demonstrated that refactoring is a common practice and can improve code metrics, they tend to examine just a few software products. To help put refactoring research on a sound scientific basis, we replicate the study in wider contexts and explore factors that previous authors may not have explored. We analyze four sets of Eclipse IDE usage data and apply different several different refactoring-detection strategies to them. We then use this data to test nine hypotheses about refactoring, casting doubt on several previously stated assumptions about how programmers refactor, while validating others. Refactoring behavior of refactoring tool developers differs from that of their users. Specifically, RENAMEs and MOVEs are more frequent among users. About 40% of refactorings performed using a tool occur in batches (several refactorings of the same kind within a short time period). About 90% of configuration defaults of refactoring tools remain unchanged when programmers use the tools. messages written by programmers in commit logs do not reliably indicate the presence of refactoring. Programmers frequently floss refactor (interleave refactoring with other types of programming activity). About half of refactorings are not high-level, so refactoring detection tools that look exclusively for high-level refactorings will not detect them. Refactorings are performed frequently. Almost 90% of refactorings are performed manually, and the kinds of refactorings performed with tools differ from the kinds performed manually. How could this research be extended? How could this research be applied in practice? For the toolsmith: Most kinds of refactorings will not be used as frequently as the toolsmiths hoped. Improving the under-used tools or their documentation may increase tool use. Programmers often do not configure refactoring tools. Configuration-less refactoring tools, which have recently seen increasing support in Eclipse and other environments, will suit the majority of, but not all, refactoring situations. 30 refactorings did not have tool support, the most popular of these was MODIFY ENTITY PROPERTY, performed 8 times, which would allow developers to safely modify properties such as static or final. For researchers: Questions still remain to answer. Why is the RENAME refactoring tool so much more popular than other refactoring tools? Why do some refactorings tend to be batched while others do not? Our experiments should be repeated in other projects and for other refactorings to validate our findings. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? Of particular interest to me is the inspiration for the hypothesis the authors verify - previous literature (frequently in other software engineering domains), personal experience, anecdotes from programmers, surveys. The benefit from this is twofold. First, it provides a source of inspiration for formulating hypotheses. Second, it endorses the validity of the hypotheses. We hypothesize refactoring behavior of refactoring tool developers differs from that of their users. Toleman and Welsh assume a variant of this hypothesis - that the designers of software tools erroneously consider themselves typical tool users - and argue that the usability of software tools should be objectively evaluated. We hypothesize that programmers typically perform refactoring in batches. Based on personal experience and anecdotes from programmers, we suspect that programmers often refactor several pieces of code because several related program elements may need to be refactored in order to perform a composite refactoring. In previous research, Murphy-Hill and Black built a refactoring tool that supported refactoring several program elements at once, on the assumption that this is common. We hypothesize that programmers do not often configure refactoring tools. We suspect this because tweaking code manually after the refactoring may be easier than configuring the tool. In the past, we have found some limited evidence that programmers perform only a small amount of configuration of refactoring tools. When we did a small survey in September 2007 at a Portland Java Users Group meeting, 8 programmers estimated that, on average, they supply configuration information only 25% of the time. In Xing and Stroulia's automated analysis of the Eclipse codebase, the authors conclude that \"indeed refactoring is a frequent practice\". Although flawed, this becomes one of the authors' hypotheses. Furthermore, some hypotheses are formed from a critique of previous literature, combined with domain expertise and/or other literature. Several researchers have used messages attached to commits into a version control as indicators of refactoring activity. However, we hypothesize that this assumption is false, because refactoring may be an unconscious activity, and because the programmer may consider it subordinate to some other activity, such as adding a feature. Past research has often drawn conclusions based on observations of high-level refactorings. We hypothesize that in practice programmers also perform many lower-level refactorings. We suspect this because lower-level refactorings will not change the program's interface and thus programmers may feel more free to perform them. Additionally, much of the methodology presented in this paper can be borrowed. The fourth dataset used by the authors is Eclipse CVS, the version history of the Eclipse and JUnit code bases extracted from their Concurrent Versioning System (CVS) repositories. CVS does not maintain records showing which file revisions were committed as a single transaction. The standard approach for recovering transactions is to find revisions committed by the same developer with the same commit message within a small time window; we use a 60 second time window. In our experiments, we randomly sampled from about 3400 source file commits that correspond to the same time period, the same projects, and the same developers represented in Toolsmiths. Using these data, two of the authors inferred which refactorings were performed by comparing adjacent commits manually. Ratzinger describes the most sophisticated strategy for finding refactoring messages: searching for the occurrence of keywords such as \"move\" and \"rename\", and excluding \"needs refactoring\". We replicated Ratzinger's experiment for the Eclipse code base to nullify Ratzinger's conclusions. In order for refactoring activity to be defined as frequent, we seek to apply criteria that require refactoring to be habitual and occurring at regular intervals. First, we examined the Toolsmiths data to determine how refactoring activity was spread throughout development. Second, we examined the Users data to determine how often refactoring occurred within a programming session and whether there was significant variation among the population. We hypothesize that programmers often do not use refactoring tools, because existing tools may not have a sufficiently usable user-interface. To validate this hypothesis, we correlated the refactorings that we observed by manually inspecting Eclipse CVS commits with the refactoring tool usages in the Toolsmiths data set.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: \"Cloning Considered Harmful: Considered Harmful","slug":"Paper-Reading-Cloning-Considered-Harmful-Considered-Harmful","date":"2022-10-05T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/10/05/Paper-Reading-Cloning-Considered-Harmful-Considered-Harmful/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/05/Paper-Reading-Cloning-Considered-Harmful-Considered-Harmful/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How was the work validated? Current literature on the topic of duplicated code in software systems often considers duplication harmful to the system quality, and the reasons commonly cited for duplicating code often have a negative connotation. While these positions are sometimes correct, during our case studies we have found that this is not universally true, and we have found several situations where code duplication seems to be a reasonable or even beneficial design option. This paper introduces eight cloning patterns that we have uncovered during case studies on large software systems, and discusses the advantages and disadvantages associated with using them. Forking, cloning used to bootstrap development of similar solutions, with the expectation that evolution of the code will occur somewhat independently Hardware variation Platform variation Experimental variation Templating, directly copy behavior of existing code but appropriate abstraction mechanisms are unavailable Boiler-plating due to language in-expressiveness API/Library protocols General language or algorithmic idioms Customization, currently existing code does not adequately meet a new set of requirements Bug workarounds Replicate and specialize What were the main contributions of the paper as you (the reader) see it? How does this work move the research forward? How could this research be extended? This paper introduces the notion of categorizing high level patterns of cloning in a similar fashion to the cataloging of design patterns or anti-patterns. There are several benefits that can be gained from this characterization. It provides a flexible framework on top of which we can document our knowledge about how and why cloning occurs in software. This documentation crystallizes a vocabulary that researchers and practitioners can possibly use to communicate about cloning. This categorization is a first step towards formally defining these patterns to aid in automated detection and classification. These classifications can then be used to define metrics concerning code quality and maintenance efforts. Automatic classifications will also provide us with better measures of code cloning in software systems and severity of the problem in general. How could this research be applied in practice? In each uncovered cloning pattern, the author describes its advantages, disadvantages, how it can be managed, issues to be aware of when deciding to use it as a long-term solution, as well as real examples in large software systems. These provide practical guidelines when considering a trade-off between code cloning and formulating abstractions for code reuse, as well as how to manage code cloning should it be used, when developing a software project.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Go To Statement Considered Harmful","slug":"Paper-Reading-Go-To-Statement-Considered-Harmful","date":"2022-10-04T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/10/04/Paper-Reading-Go-To-Statement-Considered-Harmful/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/04/Paper-Reading-Go-To-Statement-Considered-Harmful/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. The author has been familiar with the observation that the quality of programmers is a decreasing function of the density of go to statements in the programs they produce, and in this paper, he explains why the use of the go to statement has negative effects. He first remarks that the process taking place under control of the program, instead of the program itself, is the true subject matter of a programmer's activity, and it is this process whose behavior has to satisfy the desired specifications. He then argues that our intellectual powers can better master static relations than visualize processes evolving in time, for which reason we should shorten the conceptual gap between the static program and the dynamic progress. The author continues characterizing the progress of a progress, explaining that it can be uniquely characterized by a mixed sequence of textual and/or dynamic indices, when conditionals, procedures, and repetition clauses are considered. However, the unbridled use of the go to statement has an immediate consequence that it becomes terribly hard to find a meaningful set of coordinates in which to describe the process progress, which will in turn \"make a mess of one's program\". However, in my opinion, although the go to statement is considered harmful, abolishing the go to statement from all \"higher level\" programming languages is an overstatement. As the author himself stated: The exercise to translate an arbitrary flow diagram more or less mechanically into a jump-less one, is not to be recommended. Then the resulting flow diagram cannot be expected to be more transparent than the original one. There exist situations where an \"arbitrary flow diagram\" has to be implemented (especially when implementing Finite-State Machines in lexers, regex engines, and protocols), and in these situations, implementing the flow diagram using go to statements is much more direct, straightforward and easier to reason about (not to mention more efficient) than mashing up structured programming constructs.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs","slug":"Paper-Reading-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs","date":"2022-10-04T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/10/04/Paper-Reading-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/04/Paper-Reading-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? How was it addressed by prior work? Many classes of errors are difficult to find without executing a piece of code. The importance of such testing, combined with the difficulty and poor performance of random and manual approaches, has led to much work in using symbolic execution to automatically generate test inputs. It has been an open question whether the approach has any hope of consistently achieving high coverage on real applications, facing the challenges in handling code that interacts with the environment, and the exponential number of paths through code. Traditional symbolic execution systems either cannot handle programs interacting with the environment or require a complete working model. More recent work in test generation does allow external interactions, but forces them to use entirely concrete procedure call arguments, which limits the behaviors they can explore. For the path explosion problem, search strategies proposed in the past include Best First Search, Generational Search, and Hybrid Concolic Testing. Orthogonal to search heuristics, researchers have addressed the path explosion problem by testing paths compositionally, and by tracking the values read and written by the program. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you? KLEE interprets programs compiled to LLVM IR, and typically requires no source modification. It functions as a hybrid between an operating system for symbolic processes and an interpreter. Each symbolic process has a register file, stack, heap, program counter, and path condition. Unlike a normal process, storage locations for a symbolic process - registers, stack and heap objects - refer to expression trees instead of raw data values. The leaves of an expression are symbolic variables or constants, and the interior nodes come from LLVM IR operations. Conditional branches take a boolean expression and alter the instruction pointer of the symbolic process based on whether the condition is true or false. KLEE queries the constraint solver to determine if the branch condition is either provably true or false along the current path. If so, the instruction pointer is updated to the appropriate location. Otherwise, both branches are possible. KLEE forks the symbolic process so that it can explore both paths. The number of forked symbolic processs grows quite quickly in practice. KLEE implements the heap as an immutable map, and portions of the heap structure itself can also be shared amongst multiple symbolic processs. Additionally, this heap structure can be forked in constant time, which is important given the frequency of this operation. Potentially dangerous operations implicitly generate branches that check if any input value exists that could cause an error. For example, a division instruction generates a branch that checks for a zero divisor. If so, KLEE solves the current path's constraints to produce a test case that will follow the same path when rerun on an unmodified version of the checked program, and terminates the current symbolic process. KLEE will then continue execution on the false path, which adds the negation of the check as a constraint (e.g., making the divisor not zero). The core of KLEE is an interpreter loop which selects a symbolic process to run and then symbolically executes a single instruction in the context of that symbolic process. Given more than one symbolic process, KLEE must pick which one to execute first. KLEE selects the symbolic process to run at each instruction by uses each strategy in a round robin fashion. - Random Path Selection: Use a binary tree to record the program path followed for all active symbolic processs. A symbolic process is selected by traversing this tree from the root and randomly selecting the path to follow at branch points. This strategy has two important properties. - Favors symbolic processs high in the branch tree. They have less constraints on their symbolic inputs and have greater freedom to reach uncovered code. - Avoids starvation when some part of the program is rapidly creating new symbolic processs (\"fork bombing\") as it happens when a tight loop contains a symbolic condition. - Coverage-Optimized Search: Select symbolic processs likely to cover new code in the immediate future using heuristics. This loop continues until there are no symbolic processs remaining, or a user-defined timeout is reached. KLEE ensures that a symbolic process which frequently executes expensive instructions will not dominate execution time by running each symbolic process for a \"time slice\" defined by both a maximum number of instructions and a maximum amount of time. KLEE uses STP as its constraint solver. KLEE maps every memory object in the checked code to a distinct STP array. This representation dramatically improves performance since it lets STP ignore all arrays not referenced by a given expression. Furthermore, there are tricks to simplify expressions and ideally eliminate queries before they reach STP, including: Expression Rewriting Constraint Set Simplification Implied Value Concretization Constraint Independence Counter-example Cache: Redundant queries are frequent, and a simple cache is effective at eliminating a large number of them. However, it is possible to build a more sophisticated cache due to the particular structure of constraint sets. The counter-example cache maps sets of constraints to counter-examples (i.e., variable assignments), along with a special sentinel used when a set of constraints has no solution. This mapping is stored in a custom data structure — derived from the UBTree structure of Hoffmann and Hoehler, which allows efficient searching for cache entries for both subsets and supersets of a constraint set. By storing the cache in this fashion, the counter-example cache gains three additional ways to eliminate queries. When a subset of a constraint set has no solution, then neither does the original constraint set. When a superset of a constraint set has a solution, that solution also satisfies the original constraint set. When a subset of a constraint set has a solution, it is likely that this is also a solution for the original set. KLEE handles the environment by redirecting library calls to models that understand the semantics of the desired action well enough to generate the required constraints. The real environment can fail in unexpected ways. Such failures can often lead to unexpected and hard to diagnose bugs. To help catch such errors, KLEE will optionally simulate environmental failures by failing system calls in a controlled manner. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? Four sets of experiments are conducted. We do intensive runs to both get high coverage and find bugs on Coreutils and BusyBox tools, do a comparision with random tests and developer test suites, and discuss the bugs found. To demonstrate KLEE's applicability to bug finding, we used KLEE to check all 279 BusyBox tools and 84 MINIX tools in a series of short runs. Thus far, we have focused on finding generic errors that do not require knowledge of a program's intended behavior. We now show how to do much deeper checking, including verifying full functional correctness on a finite set of explored paths. We use KLEE to find deep correctness errors by cross-checking purportedly equivalent Coreutils and BusyBox tool implementations. We have also applied KLEE to checking non-application code by using it to check the HiStar kernel. We chose line coverage as reported by gcov as a conservative measure of KLEE-produced test case effectiveness, because it is widely-understood and uncontroversial. The results of the experiments are very positive, and convincingly prove the proposed problem statement. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Coverage-Optimized Search tries to select symbolic processs likely to cover new code in the immediate future. It uses heuristics to compute a weight for each symbolic process and then randomly selects a symbolic process according to these weights. How these heuristics work, which is critical for performance, is not symbolic processd, and remains unclear. KLEE ensures that a symbolic process which frequently executes expensive instructions will not dominate execution time by running each symbolic process for a \"time slice\" defined by both a maximum number of instructions and a maximum amount of time. Precisely how this \"time slice\" is calculated is also unclear. KLEE handles the environment by redirecting library calls to models that understand the semantics of the desired action well enough to generate the required constraints. These models are written in normal C code which the user can readily customize, extend, or even replace without having to understand the internals of KLEE. However, what \"understand the semantics of the desired action well enough\" means is unclear. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? KLEE does not currently support symbolic floating point, longjmp, threads, and assembly code. Additionally, memory objects are required to have concrete sizes. These block KLEE's application towards floating point-heavy scientific computation and data science code, and may also limit KLEE to simple programming languages such as C, not supporting the numerous dynamics, including exception handling, within C++.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: CUTE: A Concolic Unit Testing Engine for C","slug":"Paper-Reading-CUTE-A-Concolic-Unit-Testing-Engine-for-C","date":"2022-10-02T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/10/02/Paper-Reading-CUTE-A-Concolic-Unit-Testing-Engine-for-C/","link":"","permalink":"https://jifengwu2k.github.io/2022/10/02/Paper-Reading-CUTE-A-Concolic-Unit-Testing-Engine-for-C/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. NOTE: I believe the paper to be written very obscurely, so I will explain the ideas of the paper in my own words. What is the problem being tackled? How was it addressed by prior work? Unit testing is a method for modular testing of a program's functional behavior. Such testing requires specification of values for the inputs (or test inputs) to the unit. Manual specification of such values is labor intensive and cannot guarantee that all possible behaviors of the unit will be observed during the testing. Several techniques have been proposed to automatically generate values for the inputs. Randomly choose the values over the domain of potential inputs Many values may lead to the same behavior and are redundant. The probability of selecting inputs causing buggy behavior may be astronomically small. Symbolic Exection Addresses the problem of redundant executions and increases test coverage For large or complex units, it is intractable to maintain and solve the constraints required for test generation Incrementally generating test inputs by combining concrete and symbolic execution During a concrete execution, a conjunction of symbolic constraints along the path of execution is generated. These constraints are modified and then solved to generate further test inputs to direct the program along alternative paths. If it is not feasible to solve, simply substitute random concrete values. This problem is particularly complex for programs with dynamic data structures using pointer operations. Pointers may have aliases. In this paper, we provide a method for representing and solving approximate pointer constraints to generate test inputs. Our method is thus applicable to a broad class of sequential programs. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you? We consider the execution of a function to be determined by all the stack variables, global variables, and heap objects it exercises. Only primitive types and pointer types are taken into consideration. For structures and arrays, each member is considered to be a separate variable. External OS services are not modelled. We associate the following properties with each stack variable, global variable, and heap object. Concrete Value Symbolic Value Concrete Address Symbolic Address The branches taken within an execution can be described with a predicate sequence called a path constraint. Each predicate is described using the aforementioned stack variables, global variables, and/or heap objects. Symbolic values are used when available, otherwise, concrete values are used. Predicates involving primitive types are of the form \\(a_1 x_1 + \\dots + a_n x_n + c~R~0, R \\in \\{&lt;, &gt;, \\le, \\ge, =, \\ne\\}\\), where \\(a_i, \\dots, a_n, c\\) are integer constants. (Essentially considers only linear combinations of primitive types) Predicates involving pointers are of the form \\(x~R~y\\) or \\(x~R~NULL\\), \\(R \\in \\{=, \\ne\\}\\). (Essentially considers only being able to assign to a pointer NULL or another previously known address, and does not allow converting integers to pointers) Running process of CUTE. while True: Execute, in the process: When allocating a stack variable, global variable, or heap object without initialization (incl. function parameters): Modify \"known stack variables, global variables, and heap objects\" if needed. If its concrete value has been stored, initialize it to its stored concrete value. Otherwise, generate a random concrete value for it. Record its concrete value and concrete address. When allocating a stack variable, global variable, or heap object with initialization: Modify \"known stack variables, global variables, and heap objects\" if needed. Record its concrete value and concrete address. Record its symbolic value and symbolic address. When assigning an existing stack variable, global variable, or heap object: Update its concrete value. Update its symbolic value. When taking a branch, add a new predicate to the path constraint. After execution, negate the last predicate within the path constraint, and solve for the concrete values of \"stack variables, global variables, and heap objects allocated without initialization\". Update their recorded concrete values. Solving optimizations: Check if the last predicate is syntactically the negation of any preceding predicate Identify and eliminate common arithmetic subconstraints. Identify dependencies between predicates and exploit them. The path constraints from two consecutive concolic executions, \\(C\\) and \\(C&#39;\\) differ only in a small number of predicates, and their respective solutions are similar. The solver collects all the predicates in C that are dependent on the negation of the last and solves for them. In practice, we have found that the size of this set is almost one eighth the size of \\(C\\) on average. Generated random concrete values: Primitive Type: random number Pointer Type: NULL We next consider testing of functions that take data structures as inputs. We want to test such functions with valid inputs only. There are two main approaches to obtaining valid inputs: Generating inputs with call sequences Use the functions that check if an input is a valid data structure by solving them, i.e., generating input for which they return true. Previous techniques include a search that uses purely concrete execution and a search that uses symbolic execution for primitive data but concrete values for pointers. CUTE, in contrast, uses symbolic execution for both primitive data and pointers. This allows it to solve these functions asymptotically faster than the fastest previous techniques. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? We illustrate two case studies that show how CUTE can detect errors. We applied CUTE to test its own data structures. Our goal in this case study was to detect memory leaks in addition to standard errors such as segmentation faults, assertion violation etc. We also applied CUTE to unit test SGLIB version 1.0.1, a popular, open-source C library for generic data structures. We chose SGLIB as a case study primarily to measure the efficiency of CUTE. We found two bugs in SGLIB using CUTE. The case studies showcase the power of CUTE's concolic unit testing approach, and match well with the proposed problem statement. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? After execution, negate the last predicate within the path constraint, and solve for the concrete values of \"stack variables, global variables, and heap objects allocated without initialization\". A solving optimization that the author proposed is \"identifing and eliminating common arithmetic subconstraints\". However, how this is done is not explained. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? For structures and arrays, each member is considered to be a separate variable. Although this facilicates analysis, this could incur significant overhead and impede scalability. External OS services are not modelled. Predicates involving primitive types are of the form \\(a_1 x_1 + \\dots + a_n x_n + c~R~0, R \\in \\{&lt;, &gt;, \\le, \\ge, =, \\ne\\}\\), where \\(a_i, \\dots, a_n, c\\) are integer constants. This essentially considers only linear combinations of primitive types. The author shows preference to using the technique of \"using the functions that check if an input is a valid data structure by solving them\" to solve the problem of testing of functions that take data structures as inputs. However, such an approach may be impossible for object-oriented languages such as C++, in which data structures are encapsulated in classes, and the logic of validness is enforced with the constructor and public methods of the classes.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Selection and Presentation Practices for Code Example Summarization","slug":"Paper-Reading-Selection-and-Presentation-Practices-for-Code-Example-Summarization","date":"2022-09-28T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/09/28/Paper-Reading-Selection-and-Presentation-Practices-for-Code-Example-Summarization/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/28/Paper-Reading-Selection-and-Presentation-Practices-for-Code-Example-Summarization/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? How could this research be applied in practice? Code examples are important in modern software development. As part of the first steps toward automatic source-to-source summarization, the authors studied how humans summarize examples to understand how to automate the process, and propose empirically-supported hypotheses justifying the use of specific practices. Selection Practices - Practices Related to Language Constructs - Practices Based on Query Term - Practices Considering the Human Reader Presentation Practices - Trimming a Line When Needed - Compressing a Large Amount of Code - Truncating Code - Formatting Code for Readability - Improving Code The results provide a grounded basis for the development of code example summarization and presentation technology. How was the work validated? We chose a well-defined corpus of programming documents, The Official Android API Guides, which contains a mix of natural-language text and code fragments. We collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. We analyzed common practices behind these decisions across the hand-generated representations, as well as the rationale behind the practices. What were the main contributions of the paper as you (the reader) see it? In my opinion, aside from the obvious contributions of the paper presented by the author, there is a lot to learn from the study set-up and the conceptual framework for interpreting the results. To understand the rationale behind the practices, we instructed the participants to verbalize their thought process using the think-aloud protocol. We distinguished practices concerning the type of content selected and the way the content was presented in a summary, because even summaries with content associated with the same part of the original fragment could vary on how to present the summary. To make hypotheses justifying the use of different practices, we relied on a quantitative analysis of the distribution of each practice across code fragments and participants. In-lined histograms presents the distribution of observations of a given practice for the participants over the code fragments. This provides a convenient and compact assessment of the amount of evidence for a practice. Furthermore, the authors have borrowed a lot from related domains of research, including natural language generation, natural language summarization of code, etc. Some examples: The separation of content selection from presentation is typical in a natural language generation system. The comments demonstrated a number of different ways to abstract content, including aggregating lexically and aggregating semantically - natural language generation terminology. Seven participants injected additional natural language into the code summaries. This motivates a novel type of transformations that mix code and text. The only work we know of in this area is the natural summaries generated by Rastkar et al. This gives revelations on exploiting knowledge from related domains when doing our own research. How could this research be extended? The goal of the study was to inform the design of concise representations of source code and automatic summarization algorithms. A natural future direction is to implement these representations and algorithms, and conduct empirical studies assessing their usefulness in summarizing source code.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Finding and Understanding Bugs in C Compilers","slug":"Paper-Reading-Finding-and-Understanding-Bugs-in-C-Compilers","date":"2022-09-24T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/09/24/Paper-Reading-Finding-and-Understanding-Bugs-in-C-Compilers/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/24/Paper-Reading-Finding-and-Understanding-Bugs-in-C-Compilers/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? Finding compiler bugs, especially bugs in the \"middle end\" of a compiler that performs transformations on an intermediate representation, to improve the quality of C compilers. How was it addressed by prior work? Compilers have been tested using randomized methods for nearly 50 years. In 1998, McKeeman coined the term \"differential testing\". His work resulted in DDT, a family of program generators that conform to the C standard at various levels. However, DDT avoided only a small subset of all undefined behaviors, and only then during test-case reduction, not during normal testing. Thus, it is not a suitable basis for automatic bug-finding. Lindig used randomly generated C programs to find several compiler bugs related to calling conventions. His tests are self-checking, but far less expressive than Csmith. Sheridan also used a random generator to find bugs in C compilers. Sheridan's tool produces self-checking tests. However, it is less expressive than Csmith and it fails to avoid undefined behavior such as signed overflow. Zhao et al. created an automated program generator for testing an embedded C++ compiler, which allows a general test requirement, such as which optimization to test, to be specified. What are the innovation(s) proposed in this paper? The paper proposes Csmith, a randomized test-case generation tool which generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. This advances the state of the art in compiler testing. Csmith supports compiler bug-hunting using differential testing. Csmith generates a C program, a test harness then compiles the program using several compilers, runs the executables, and compares the outputs. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? The authors conducted five experiments. Finding and reporting bugs in a a variety of C compilers over a three-year period. They have found and reported more than 325 bugs in mainstream C compilers including GCC, LLVM, and commercial tools. Compiling and running one million random programs using several years' worth of versions of GCC and LLVM, to understand how their robustness is evolving over time. Evaluating Csmith's bug-finding power as a function of the size of the generated C programs. Comparing Csmith's bug-finding power to that of four previous random C program generators. Investigating the effect of testing random programs on branch, function, and line coverage of the GCC and LLVM source code. The experiments thoroughly evaluate and demonstrate Csmith's bug-finding power and provide guidelines for using Csmith to find bugs. Which technical innovations are most compelling to you? Csmith uses randomized differential testing. This has the advantage that no oracle for test results is needed. It exploits the idea that if one has multiple, deterministic implementations of the same specification, all implementations must produce the same result from the same valid input. When two implementations produce different outputs, one of them must be faulty. Given three or more implementations, a tester can use voting to heuristically determine which implementations are wrong. How Csmith designs the results used for differential testing is also worthwhile. A Csmith-generated program prints a value summarizing the computation performed by the program, which is implemented as a checksum of the program's non-pointer global variables at the end of the program's execution. Thus, if changing the compiler or compiler options causes the checksum emitted by a Csmith-generated program to change, a compiler bug has been found. Also compelling are the mechanisms that Csmith uses to avoid generating C programs that execute undefined behaviors or depend on unspecified behaviors, including performing incremental pointer and dataflow analysis in the process of generating programs. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? In the process of randomly generating programs, Csmith randomly selects an allowable production from its grammar for the current program point. To make the choice, it consults a probability table and a filter function specific to the current point: there is a table/filter pair for statements, another for expressions, and so on. The table assigns a probability to each of the alternatives, where the sum of the probabilities is one. However, how this probability table is constructed and maintained, which obviously is critical to generating high-quality random programs, is not stated in the paper, and requires clarification. Do you forsee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? Which problems remain unsolved after this paper? The most important language features not currently supported by Csmith are strings, dynamic memory allocation, floating-point types, unions, recursion, and function pointers. These are language features that are ubiquitous in real-world programs, thus, not supporting them is a serious barrier to the applicability of Csmith. The authors plan to add some of these features to future versions of our tool. Although Csmith-generated programs allowed discovering bugs missed by compilers' standard test suites, branch, function, and line coverage of the GCC and LLVM source code did not significantly improve compared to the compilers' existing test suites. 'Coverage-guided' fuzzing may represent a future direction of research to discover more bugs lurking in unvisited sections of compiler source code.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges","slug":"Paper-Reading-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges","date":"2022-09-22T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/09/22/Paper-Reading-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/22/Paper-Reading-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How could this research be applied in practice? The paper conducts an empirical study of the effectiveness and challenges of automatically generated unit tests at finding real faults, and derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. Improving the obtained code coverage so that faulty statements are executed in the first instance. A high code coverage ratio does not necessarily indicate that the bug was covered. Improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, is also required. Improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time. How was the work validated? The authors applied three state-of-the art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. To account for randomness in test generation, we generated 10 test suites for each tool and fault. Tools may generate flaky tests, which may also fail on the fixed version. They are automatically removed. Even if a test is not flaky, it might still fail on the buggy version for reasons unrelated to the actual fault. Such false positives are identified. For each executed test, we collected information on whether it passed or failed, and the reason of failure. In order to study how code coverage relates to fault detection, we measured statement coverage, and also bug coverage - whether a fault was 1) fully covered (all modified statements covered), 2) partially covered (some modified statements covered), or 3) not covered. To gain insights on how to increase the fault detection rate of test generation tools, the authors did case studies on the challenges that prevent fault detection, and studied the root causes for flaky and false-positive tests. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? How could this research be extended? The revelations from the case studies supporting the primary contributions of the paper as the author sees it are particularly important, as they identify specific challenges and provide plausible solutions for increasing the fault detection rate of test generation tools. Creation of complex objects, such as a control flow graph, which often requires a certain sequence of prior method calls. Viable solutions include seeding objects observed at runtime, mining of common usage patterns of objects to guide object creation, or carving of complex object states from system tests. Complex strings satisfying a certain syntax. Search-based tools are capable in principle of generating string inputs, but doing so can take very long. Symbolic approaches using string solvers or dedicated solvers for regular expressions are generally restricted to fixed length strings. If an input grammar is known, this can be used to generate test data more efficiently. Complex conditions which randomly initialized inputs are unlikely to satisfy. Dynamic symbolic execution would not suffer from this problem. Errors are not propagated. To some extent, this is the result of focusing on simple structural criteria such as branch coverage, rather than aiming to exercise more complex intra-class data flow dependencies. Environmental dependencies and dependencies on the static state of the system under test resulting in flaky tests. Aggressive mocking, which monitors and asserts on the internal state (e.g. the order of method calls) of the class under test, rather than testing the class on what its public method returns, and its side effects.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: The Art of Testing Less without Sacrificing Quality","slug":"Paper-Reading-The-Art-of-Testing-Less-without-Sacrificing-Quality","date":"2022-09-21T04:00:00.000Z","updated":"2025-08-13T04:31:00.458Z","comments":true,"path":"2022/09/21/Paper-Reading-The-Art-of-Testing-Less-without-Sacrificing-Quality/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/21/Paper-Reading-The-Art-of-Testing-Less-without-Sacrificing-Quality/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? For large complex software products, there is a need to check that changes do not negatively impact other parts of the software and they comply with system constraints such as backward compatibility, performance, security etc. Ensuring these system constraints may require complex test procedures, but long tests conflict with strategic aims to shorten release cycles. To accelerate test processes without sacrificing product quality, the paper develops a cost model for test executions based on historic test execution results that causes no test execution runtime overhead. The paper then presents a novel cost based test selection strategy, THEO, which skips test executions where the expected cost of running the test exceeds the expected cost of not running it, while ensuring that all tests will execute on all code changes at least once. How was the work validated? The paper replayed past development periods of Microsoft Windows, Office, and Dynamics with THEO. THEO would have reduced the number of test executions by up to 50%, cutting down test time by up to 47%. At the same time, product quality was not sacrificed as the process ensures that all tests are ran at least once on all code changes. Simulation shows that THEO produced an overall cost reduction of up to $2 million per development year, per product. Furthermore, this paper have convinced an increasing number of Microsoft product teams to explore ways to integrate THEO into their actual live production test environments. This further endorses THEO's effectiveness. How could this research be extended? The paper stated that through reducing the overall test time, THEO would also have other impacts on the product development process, such as increasing code velocity and developer satisfaction. An empirical study on the effects of cost based test selection strategies on these aspects would be a direction for extending this research. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? How could this research be applied in practice? In my opinion, the main contribution of this paper, and the aspect most able to be used as a reference in other projects, is the cost model where each test execution is considered an investment and the expected test result considered as return of investment. Several factors are considered in the cost model, with their values easily derived from past observations. \\(p_{TP}\\), the probability the combination of test and execution context will detect a defect (true positive). \\(p_{FP}\\), the probability the combination of test and execution context will report a false alarm (false positive). \\(engineers\\), the number of engineers whose code changes passed the current code branch. \\(time_{delay}\\), the average time span required to fix historic defects on the corresponding code branch. When a test is executed: \\(cost_{machine}\\): the per-minute infrastructure cost of test execution. \\(cost_{inspect}\\): the average cost per test inspection, equal to inspection time times the salary of the engineer. For simplicity reasons, an average cost of test inspection is used. When a test is skipped: \\(cost_{escaped}\\): the average cost of an escaped defect, per developer and hour of delay. Defect severity is not modeled, as it cannot be determined beforehand, and all defects causing development activity to freeze on the corresponding branch must be considered severe. After collecting these data, two cost functions are calculated: the expected cost of executing a test \\(cost_{exec} = cost_{machine} + p_{FP} \\times cost_{inspect}\\), and the expected cost for not executing a test \\(cost_{skip} = p_{TP} \\times cost_{escaped} \\times time_{delay} \\times engineers\\). Through a reasonable and tested quantization like this, objective decisions can be made, boosting the efficiency of software development.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Boosting Fuzzer Eficiency: An Information Theoretic Perspective","slug":"Paper-Reading-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective","date":"2022-09-20T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/09/20/Paper-Reading-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/20/Paper-Reading-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? Finding a solid theoretical foundation for fuzzing and using it to boost fuzzing efficiency is a direction of research of great practical value. How was it addressed by prior work? Previous works have proposed various heuristics to boost fuzzing efficiency, such as assigning more energy to seeds that have previously been observed to crash, that maximize execution counts to discover algorithmic complexity vulnerabilities, that exercise low-probability paths, etc. Furthermore, there has also been prior research in theoretical aspects of fuzzing, such as conducting a probabilistic analysis on the efficiency of blackbox versus whitebox fuzzing, empirically investigating the scalability of non-deterministic black- and greybox fuzzing, etc. What are the innovation(s) proposed in this paper? First, the paper develops an information-theoretic foundation for non-deterministic fuzzing. Assumptions Fuzzing Heuristics remain constant throughout the fuzzing process. Concepts Neighborhood All inputs generated from mutating a seed. Species A branch within a program. Species Discovery Program execution traverses a previously untraversed branch when some input is provided to the program. Incidence Frequency The number of times a species is covered. Energy The probability the fuzzer chooses a seed for mutation. Power Schedule The procedure of assigning energy to a seed. Local Species Distribution of a Seed Given a seed, the probability of each species being covered, when an input generated by mutation from the seed is fed to the program. Entropy in the Context of Fuzzing Using the metaphor of a \"language\" with \"words\" of varying frequencies, entropy in the context of fuzzing can be understood as: \"Sentences\" of the \"language\": Program executions resulting from generated inputs. \"Words\" of the \"language\": Species. Frequencies of the \"words\": The frequencies of each species being traversed. Entropy can be calculated using the frequencies of the \"words\", and represents the frequency distribution of the \"words\". As high entropy implies that the species of the program have all been well covered, it can be used as a proxy for fuzzing efficiency. Local Entropy of a Seed Still using the metaphor of a \"language\" with \"words\" of varying frequencies, local entropy of a seed can be understood as: \"Sentences\" of the \"language\": Program executions resulting from inputs within the seed's neighborhood. \"Words\" of the \"language\": Species. Frequencies of the \"words\": The frequencies of each species being traversed. The local entropy of a seed quantifies the information that feeding the inputs within the seed's neighborhood into the program reveals about the species. Second, the paper presents the first entropy-based power schedule to boost the efficiency of greybox fuzzers. More energy is assigned to seeds that elicit more information about the program's species. Thus, every time when randomly choosing a seed for mutation, each seed is assigned an energy proportional to its local entropy. However, a new seed that has never been fuzzed will always be assigned zero energy, and they will never be chosen for mutation. To solve this problem, add-one smoothing is used for the frequency of the species. Specifically, the frequency of species \\(i\\) used to calculate local entropy of seed \\(t\\): \\(p_i^t = \\frac{Y_i^t + 1}{S + Y_1^t + \\dots + Y_S^t}\\) Where: \\(Y_i^t\\) is the number of times species \\(i\\) has been traversed by the neighborhood of \\(t\\). \\(S\\) is the total number of species at the time of calculation. Furthermore, in the experiments, the authors noticed that the local entropies for different seeds were almost the same, because a small number of very abundant species had a huge impact on the local entropies. Thus, the authors defined an abundance threshold \\(\\theta\\) which is an upper bound for \\(Y_i^t\\). How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? The paper provides an open-source implementation, Entropic, within LLVM libFuzzer, and presents a substantial empirical evaluation on over 250 widely-used, open-source C/C++ programs producing over 2 CPU years worth of data. Four research questions were asked to evaluate the hypothesis that increasing information per generated input increases fuzzer efficiency. What is the empirical coverage improvement over the baseline? How much faster are bugs detected compared to the baseline? How does the choice of abundance threshold influence the performance of our technique? What is the cost of maintaining incidence frequencies? The answers to these research strongly support the hypothesis, thus the evaluation matches well with the proposed problem statement. Your opinion of the paper Which technical innovations are most compelling to you? Developing an information-theoric foundation for non-deterministic fuzzing, in which entropy in the context of fuzzing is calculated using the probability distribution of species (branches). This is both intuitive and allows us to effectively use entropy, which has \"really nice properties, and a principled origin\" as a \"convenient proxy\" for fuzzing efficiency. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Which problems remain unsolved after this paper? The paper develops an information-theoretic foundation for non-deterministic fuzzing, before presenting the first entropy-based power schedule to boost the efficiency of greybox fuzzers. I have questions regarding both aspects. Entropy is calculated using the probability distribution of species, which are branches. Is is possible to utilize a different definition of \"species\"? The entropy-based power schedule assigns each seed with energy proportional to its local entropy. However, the authors noticed that the local entropies for different seeds were almost the same, because a small number of very abundant species had a huge impact on the local entropies. Thus, the authors defined an abundance threshold \\(\\theta\\) for \\(Y_i^t\\), a task-relevant hyperparameter. Is there a better approach for calculating the local entropies? Do you forsee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? As stated above, regarding the entropy-based power schedule.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: How Effective Developers Investigate Source Code: An Exploratory Study","slug":"Paper-Reading-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study","date":"2022-09-19T04:00:00.000Z","updated":"2025-08-13T04:31:00.456Z","comments":true,"path":"2022/09/19/Paper-Reading-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/19/Paper-Reading-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? The paper provides a set of detailed observations about the characteristics of effective program investigation. These observations are accompanies by hypotheses that can be validated by additional research and practical experience. The paper's results support the intuitive notion that developers should follow a general plan, perform focused searches in the context of this plan, and keep some form of record of their findings when investigating a program. The paper describes a methodology and analysis technique for studying the behavior of software developers. How was the work validated? The authors conducted a study of five developers undertaking an identical software change task on a medium-sized system, where understanding the existing software is a precursor to modification and validation. They did a detailed qualitative analysis of a few replicated cases, rather than a statistical analysis of causality between dependent variables. Many previous studies were based on heavily abstracted characterizations of both developer behavior and success level. It involved a detailed study of the examined code, the methods used to navigate between different locations in the code, and the modified source code. They contrasted the program investigation behavior of successful and unsuccessful developers, and isolated the factors associated with the behavior of a developer, rather than external factors (such as the influence of the workplace, the programming environment, etc.) How could this research be applied in practice? Ensuring that developers in charge of modifying software systems investigate the code of the system effectively can yield important benefits such as decreasing the cost of performing software changes and increasing the quality of the change. Understanding the nature of program investigation behavior that is associated with successful software modification tasks can help us improve the tool support and training programs offered to software developers. How could this research be extended? Researchers can reuse the authors' strategy (stated in \"How was the work validated?\") to help validate the hypotheses the authors' proposed, or to study other aspects of programmer behavior. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? In my opinion, the main contributions of the paper include the primary contributions of the paper as the author sees it, how the work was validated, and how this research could be applied in practice. However, what is most meaningful for me is how the work was validated. Such methodology is of great reference value for conducting studies on other aspects of programmer behavior. There are many technical details within that have left a deep impression on me. Each phase was described entirely through written instructions, and the subjects were given an Eclipse training phase and an investigation phase before the modification phrase. To record the actions of a developer in the investigation and modification phases, they recorded the developers' screens, and transcribed the recordings into a structured list of events. Each event contains the properties time, method, navigation, and modification. To analyze the quality of change, the authors analyzed the source code to determine the characteristics of an ideal solution, and divided the task into five subtasks. The authors examined how each subject had implemented each subtask, and characterized its quality.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Understanding the Formulation of Information Entropy","slug":"Understanding-the-Formulation-of-Information-Entropy","date":"2022-09-16T04:00:00.000Z","updated":"2025-08-13T04:31:00.460Z","comments":true,"path":"2022/09/16/Understanding-the-Formulation-of-Information-Entropy/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/16/Understanding-the-Formulation-of-Information-Entropy/","excerpt":"","text":"NOTE: The terms \"language\" and \"word\" are used metaphorically in this document. A \"language\" often has many \"words\", and the frequency of each \"word\" varies. If a \"language\" \\(X\\) has a total of \\(n\\) \"words\", then we can encode a word with \\(\\log_{2}{n}\\) binary bits. But when transmitting the words, we want to keep the encoding of each \"word\" as short as possible. A common practice is that for those high-frequency \"words\", we can use shorter encodings, and for those \"words\" that we use less frequently, we can allow longer encodings. An example is the Morse code encoding for a \"language\" consisting of 36 \"words\" - 26 Latin letters and 10 Arabic numerals. Morse code. The \"word\" \"e\" occurs frequently, hence a short code So, under some optimal encoding, what limit can the weighted average encoding length of all \"words\" achieve? Suppose our \"language\" has \\(n\\) \"words\", \\(x_1, x_2, \\dots, x_n\\), and their probability of occurrence is \\(p(x_1), p(x_2), \\dots, p(x_n)\\) (known quantities). Assuming that the lengths of the encodings of these \"words\" are \\(L(x_1), L(x_2), \\dots, L(x_n)\\) respectively, the weighted average encoding length of each \"word\" is: \\(\\bar{L} = p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)\\) How do we find the minimum value of \\(\\bar{L}\\)? Constraints Obviously, the encoded length of all \"words\" is greater than 0. But beyond that, there is a hidden constraint. We do not allow one encoding to be a prefix of another encoding, otherwise there will be ambiguity during decoding. For example, assuming that the three \"words\" of \"A\", \"B\", and \"C\" in the alphabet are encoded as \"0\", \"1\", and \"01\" respectively, then for For a code like \"001\", should we decode it as \"AAB\" or \"AC\"? We call a type of code which requires that there is no whole code word in the system that is a prefix of any other code word in the system as a prefix code. This means that if we assign a shorter encoding to a \"word\", it will squeeze a lot of resources out of the encoding space. For example, suppose the \"word\" \"A\" is encoded as \"0\", then it would \"squeeze out\" \"00\", \"01\", etc. from the codewords. Suppose the maximum value in \\(L(x_1), L(x_2), \\dots, L(x_n)\\) is \\(L_{max}\\). Then the encoding of all \"words\" are nodes on a full binary tree with a height of \\(L_{max}\\), and the full binary subtrees below each node have no intersection (otherwise violating the properties of the prefix code), as shown below. The encoding of all \"words\" are nodes on a full binary tree with a height of \\(L_{max}\\), and the full binary subtrees below each node have no intersection It is obvious that, all the full binary subtrees below each node, at most cover all the leaves of the full binary tree with height \\(L_{max}\\). For a \"word\" \\(x_i, i \\in \\{1, 2, \\dots, n\\}\\), the height of the full binary subtree below it is \\(L_{max} - L(x_i)\\), and it covers \\(2^{L_{max} - L(x_i)}\\) leaves. As the full binary tree with height \\(L_{max}\\) has a total of \\(2^{L_{max}}\\), we have: \\(2^{L_{max} - L(x_1)} + 2^{L_{max} - L(x_2)} + \\dots + 2^{L_{max} - L(x_n)} \\le 2^{L_{max}}\\) This simplifies to: \\(2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1\\) This is the Kraft-McMillan inequality. Optimization Therefore, our overall optimization objective is: \\(\\bar{L} = p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)\\) Subject to: \\(p(x_i) \\in (0, 1), i \\in \\{1, 2, \\dots, n\\}\\) are constants \\(p(x_1) + p(x_2) + \\dots + p(x_n) = 1\\) \\(L(x_i) &gt; 0, i \\in \\{1, 2, \\dots, n\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1\\) We can analyze the problem for the case where there are only two words \\(x_1, x_2\\). At this point, we have: \\(\\bar{L} = p(x_1) L(x_1) + p(x_2) L(x_2)\\) Equivalently: \\(\\bar{L} = p(x_1) L(x_1) + (1 - p(x_1)) L(x_2)\\) Subject to: \\(p(x_1) \\in (0, 1)\\) is a constant \\(L(x_i) &gt; 0, i \\in \\{1, 2\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} \\le 1\\) Define \\(a_1 = 2^{- L(x_1)}, a_2 = 2^{- L(x_2)}\\). Now we have: \\(\\bar{L} = - p(x_1) \\log_2{a_1} - (1 - p(x_1)) \\log_2{a_2}\\) Subject to: \\(p(x_1) \\in (0, 1)\\) is a constant \\(0 &lt; a_i &lt;1, i \\in \\{1, 2\\}\\) are independent variables \\(a_1 + a_2 \\le 1\\) At this point, \\(\\bar{L}\\) can be regarded as a binary function whose independent variables are \\(a_1, a_2\\), and the value ranges of the independent variables \\(a_1, a_2\\) are as follows: Value ranges of \\(a_1, a_2\\) We want to find the minimum value of \\(\\bar{L}(a_1, a_2)\\) within this range of values. The gradient of \\(\\bar{L}(a_1, a_2)\\) is as follows: \\(\\nabla\\bar{L}(a_1, a_2) = {(-p(x_1) \\log{2} \\frac{1}{a_1}, -(1 - p(x_1)) \\log{2} \\frac{1}{a_2})}^T\\) Within the value range of the independent variables \\(a_1, a_2\\), \\(\\nabla\\bar{L}(a_1, a_2)\\) is always less than 0, which means that with the growth of \\(a_1, a_2\\), \\(\\bar{L}(a_1, a_2)\\) decreases. Therefore, the maximum value of \\(\\bar{L}(a_1, a_2)\\) must occur when \\((a_1, a_2)\\) is on the boundary line \\(a_1 + a_2 = 1\\). Substituting the boundary line \\(a_1 + a_2 = 1\\) into \\(\\bar{L}(a_1, a_2)\\), you can get a unary function: \\(\\bar{L}(a_1) = - p(x_1) \\log_2{a_1} - (1 - p(x_1)) \\log_2{(1 - a_1)}\\) The constraints include: \\(p(x_1) \\in (0, 1)\\), constant \\(0 &lt; a_1 &lt; 1\\) The derivative of \\(\\bar{L}(a_1)\\) is as follows: \\(\\frac{d \\bar{L}(a_1)}{d a_1} = \\frac{\\log{2} (a_1 - p(x_1))}{a_1 (1 - a_1)}\\) The constraints include: \\(p(x_1) \\in (0, 1)\\) is a constant \\(0 &lt; a_1 &lt; 1\\) When \\(0 &lt; a_1 &lt; p(x_1)\\), \\(\\frac{d \\bar{L}(a_1)}{d a_1} &lt; 0\\), \\(\\bar{L}(a_1)\\) monotonically decreases, and when \\(p(x_1) &lt; a_1 &lt; 1\\), \\(\\frac{d \\bar{L}(a_1)}{d a_1} &gt; 0\\), \\(\\bar{L}(a_1)\\) monotonically increases. Therefore, when \\(a_1 = p(x_1)\\), \\(\\bar{L}(a_1)\\) obtains the minimum value. As \\(a_1 = 2^{- L(x_1)}, a_2 = 2^{- L(x_2)}\\), this means that, for: \\(\\bar{L} = p(x_1) L(x_1) + (1 - p(x_1)) L(x_2)\\) Subject to: \\(p(x_1) \\in (0, 1)\\) is a constant \\(L(x_i) &gt; 0, i \\in \\{1, 2\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} \\le 1\\) \\(\\bar{L}\\)'s minima occurs when \\(L(x_1) = -\\log_2{p(x_1)}, L(x_2) = -\\log_2{p(x_2)}\\), and the minima is \\(- p(x_1) \\log_2{p(x_1)} - p(x_2) \\log_2{p(x_2)}\\). Going back to the multivariate optimization problem: \\(\\bar{L} = p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)\\) Subject to: \\(p(x_i) \\in (0, 1), i \\in \\{1, 2, \\dots, n\\}\\) are constants \\(p(x_1) + p(x_2) + \\dots + p(x_n) = 1\\) \\(L(x_i) &gt; 0, i \\in \\{1, 2, \\dots, n\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1\\) \\(\\bar{L}\\)'s minima occurs when \\(L(x_i) = -\\log_2{p(x_i)}, i \\in \\{1, 2, \\dots, n\\}\\), and the minima is \\(- p(x_1) \\log_2{p(x_1)} - \\dots - p(x_n) \\log_2{p(x_n)}\\). Definition of Information Entropy If a language \"language\" \\(X\\) has \\(n\\) \"words\", \\(x_1, x_2, \\dots, x_n\\), the probability of their occurrence is \\(p(x_1), p(x_2), \\dots, p(x_n)\\), then all \"words\" under a certain optimal encoding, the previously calculated minimum weighted average encoding length, \\(- p(x_1) \\log_2{p(x_1)} - \\dots - p (x_n) \\log_2{p(x_n)}\\), is called the information entropy of the \"language\", denoted as \\(H(X)\\). The reason why it is called \"information entropy\" is mainly due to the following reasons: From von Neumann's naming suggestion for Shannon: My greatest concern was what to call it. I thought of calling it 'information,' but the word was overly used, so I decided to call it 'uncertainty.' When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, 'You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage. In a sense, it does reflect the frequency distribution of the \"words\" of \"language\" \\(X\\), just as entropy in thermodynamics reflects the distribution of microscopic particles. The lower \\(H(X)\\) is, the more the case that only a few words are used frequently in \\(X\\); the higher \\(H(X)\\) is, the more the case that all words in \\(X\\) are used frequency. Links to Explanations of Related Concepts Cross Entropy Joint Entropy Mutual Information How These Concept are Applied in Practice https://colah.github.io/posts/2015-09-Visual-Information/#conclusion References https://colah.github.io/posts/2015-09-Visual-Information/ https://mbernste.github.io/posts/sourcecoding/ https://en.wikipedia.org/wiki/Kraft–McMillan_inequality https://mathoverflow.net/questions/403036/john-von-neumanns-remark-on-entropy","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://jifengwu2k.github.io/categories/Mathematics/"}],"tags":[]},{"title":"Paper Reading: Asking and Answering Questions during a Programming Change Task","slug":"Paper-Reading-Asking-and-Answering-Questions-during-a-Programming-Change-Task","date":"2022-09-14T04:00:00.000Z","updated":"2025-08-13T04:31:00.455Z","comments":true,"path":"2022/09/14/Paper-Reading-Asking-and-Answering-Questions-during-a-Programming-Change-Task/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/14/Paper-Reading-Asking-and-Answering-Questions-during-a-Programming-Change-Task/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? A catalog of 44 types of questions programmers ask during software evaluation tasks, organized into four categories based on the kind and scope of information needed to answer a question. Finding a focus point Expanding a focus point Understanding a subgraph Over groups of subgraphs A description of the observed behavior around answering these questions. A description of how existing deployed and proposed tools do, and do not, support answering programmers' questions. How was the work validated? The author interviewed participants in two studies. 9 participants in academia worked on a code base that was new to them. 16 participants in industry worked on a code base for which they had responsibility. The two studies have allowed us to observe programmers in situations that vary along several dimensions: - the programming tools - the type of change task - the system - paired versus individual programming - prior knowledge of the code base The differences have increased the authors' ability to generate an extensive set of questions programmers ask. They build rather than test theory and the specific result of this process is a theoretical understanding of the situation of interest grounded in the data collected. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? In my opinion, aside from the final results, three important considerations learned from this paper are: Interviewing participants in two very different groups. Developing generic versions of the questions participants asked, which slightly abstract from the specifics of a particular situation and code base. Compared the generic questions and categorized those questions into four categories based on the kind and scope of information needed to answer a question. This is an example of extracting generalized knowledge from specific case studies, which makes it a great example to study for conducting empirical studies. How could this research be extended? How could this research be applied in practice? The research identified clear gaps of tool support in answering programmers' questions. Support for more refined or precise questions. Some questions can he seen as more refined versions of other questions. A programmer's questions also often have an explicit or implicit scope. Due to limited tool support, programmers end up asking questions more globally than they intend, and, the result sets will include many irrelevant items. Support for maintaining context. A particular question is often part of a larger process involving multiple questions. There are missed opportunities for tools to make use of the larger context to help programmers more efficiently scope their questions and to determine what is relevant to their higher level questions. Support for piecing information together. Many questions require considering multiple entities and relationships. In these situations, the burden is on the programmer to assemble the information needed to answer their intended question. Tool support is missing for bringing information together and building toward an answer. Improved tools and an assessment of these tools in answering these questions present directions for future research.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Qualitative Methods in Empirical Studies of Software Engineering","slug":"Paper-Reading-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering","date":"2022-09-14T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/09/14/Paper-Reading-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/14/Paper-Reading-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? With empirical studies of software engineering beginning to address the human aspects of software development, the author presents and reviews a number of different methods for the collection and analysis of qualitative data, and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combines with quantitative methods. Collecting Qualitative Data Participant Observation Interviewing Extracting Quantitative Values from Qualitative Data for Quantitative Analysis (Coding) Analyzing Qualitative Data Theory Generation: extract from a set of field notes a statement or preposition that is supported in multiple ways by the data. Theory Confirmation: confirming a preposition after it has been generated from the data. What were the main contributions of the paper as you (the reader) see it? How could this research be applied in practice? Aside from the primary contributions of the paper as the author sees it, in my opinion, another major contribution of the paper is identifying the four main categories of empirical studies, and explaining in detail how combinations of quantitative and qualitative methods can be designed for each category. The four main categories of empirical studies: - Blocked subject-project study: - Several projects, several subjects. - Reduces bias, but increases the cost of the experiment. - Replicated project study: - One project, several subjects. - Isolates the effect of differences between subjects. - Multiproject variation: - Several projects, one subject. - Observes the performance of the subject on a project before some treatment is applied, and on a different project after that treatment is applied. - Single project study: - One project, one subject. - Similar to a case study. - Certain attributes are examined and possibly compared to some baseline. How combinations of quantitative and qualitative methods can be designed for each category: - Blocked subject-project study, Replicated project study: - When testing hypotheses and finding casual relationships between variables, use qualitative data to illuminate the statistical results. - Multiproject variation study: - Qualitative analysis: revealing new issues and tracking changes relative to other issues. - Quantitative analysis: looking more closely at the issues suggested by the qualitative analysis. - Single project study: - First, data is collected qualitatively through interviews. - A taxonomy of the question under research is generated. - Part of the interview data is coded to yield quantitative variables. - Any relationships found between quantitative variables are checked against qualitative data. How was the work validated? Examples, interviews, quotes from experts, and paper citations are used to validate the points presented when reviewing a number of different methods for the collection and analysis of qualitative data, identifying the four main categories of empirical studies, and explaining in detail how combinations of quantitative and qualitative methods can be designed for each category. How could this research be extended? In the last paragraph, the author points out that \"we must exploit to the fullest every opportunity we do have, by collecting and analyzing as much data of as many different types as possible\". Aside from the examples presented in the paper, what other types of data can be collected, and how they can be analyzed, is a future direction of research.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: No Silver Bullet Essence and Accidents of Software Engineering","slug":"Paper-Reading-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering","date":"2022-09-12T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/09/12/Paper-Reading-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/12/Paper-Reading-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. How does this work move the research forward? What were the primary contributions of the paper as the author sees it? The author concludes that there is no elixir or \"silver bullet\" to the problems software engineering is facing. Furthermore, the author also examines encouraging innovations, and shows that a disciplined, consistent effort to develop, propagate, and exploit them should alleviate the problem. What were the main contributions of the paper as you (the reader) see it? In my opinion, what the paper is most remarkable at is shedding light upon the nature of the software problem and its implications. The essence of a software entity is a construct of interlocking concepts that cannot be accurately visualized. The complexity of software is an essential property, and it increases non-linearly with size. This has many implications. Difficulty of design. Hindrance of communication among team members, which leads to product flaws, cost overruns, schedule delays. Hard to use programs. Difficulty of extending to new functions without creating side effects. Security trapdoors. Personnel turnover incurs tremendous learning and understanding burden. Software is constantly subject to pressure for change. The aforementioned points clarified by the paper illuminates research directions in software engineering aimed at ameliorating the software problem. How could this research be extended? In the last section of the paper, the author examines promising attacks on the essence of the software problem. Buying off-the-shelf software instead of building in-house software. Rapid prototyping and iterative specification of requirements with client feedback. Incremental development of software from a simple and incomplete, yet running, system. Growing great designers who are the core of the development team. The effectiveness of these and other approaches in mitigating the software problem could be assessed in subsequent works. How was the work validated? First, the author examines the nature of the software problem and its implications. Further on, the author recalls the three steps in software technology that have been most fruitful in the past - high-level languages, time-sharing, and unified programming environments, concluding that they have their limits and the difficulties that they attacked are accidental, not essential. The author continues to consider the technical developments that are most often advanced as potential silver bullets - high-level language advances, object-oriented programming, artificial intelligence, automatic programming, graphical programming, program verification, environments and tools, workstations - analyzing the problems they assess, their advantages, and their disadvantages. Finally, the author presents promising attacks on the conceptual essence, explaining why they would be useful. How could this research be applied in practice? The lessons learned from this research are of great practical value. In shedding light upon the nature of the software problem and its implications, the author provides criteria for organizations to assess the effectiveness of their development practices. In considering the technical developments that are most often advanced as potential silver bullets, the author examines their advantages, and their disadvantages, and provide insights into whether to, and how to adequately use them. In presenting promising attacks on the conceptual essence, the author provides meaningful suggestions for organizations to improve their software development processes, and provides convincing rationale for doing so. As this is a classic paper, many promising attacks on the conceptual essence have already materialized and become mainstream. Rapid prototyping and incremental development have been manifested as \"agile development\" and have been widely adopted. With the advent of the open-source revolution and code-hosting platforms such as GitHub, reusing off-the-shelf software instead of building in-house software has become ubiquitous. However, the call for organizations to \"grow great designers who are the core of the development team\" incurs significant requirements on corporate management competency, and sadly, hasn't fully become reality. How does the work apply to you? It sheds light upon the nature of the software problem and its implications, illuminates research directions in software engineering aimed at ameliorating the software problem, and provides a reference research methodology for problems within software engineering.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Software's Chronic Crisis","slug":"Paper-Reading-Software-s-Chronic-Crisis","date":"2022-09-12T04:00:00.000Z","updated":"2025-08-13T04:31:00.457Z","comments":true,"path":"2022/09/12/Paper-Reading-Software-s-Chronic-Crisis/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/12/Paper-Reading-Software-s-Chronic-Crisis/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. How does this work move the research forward? What were the primary contributions of the paper as the author sees it? The author identifies software's chronic crisis and how it is exacerbated by current trends in software engineering. The vast majority of code is handcrafted by artisans using techniques they neither measure nor are able to repeat consistently. The software industry remains short of the mature engineering discipline needed to meet the demands of an information-age society, including getting software right the first time in embedded environments, distributed systems and systems integration, rapid increasing system sizes, and systems becoming so complex that no manager can comprehend the entirety. Later, the author analyzes proposed remedies to the aforementioned problems and points out directions for future work. Remedies: Capability Maturity Model, which quantifies a developer's software engineering and management excellence. Consistent and quantitative measurement of development. Strategies to avoid bugs or attack them early. Recognizing changing requirements Growing software from rapid prototypes and customer feedback Formal verification when necessary Clean-room process Cautious approach to technological innovations such as object-oriented analysis and programming Directions for Future Work: An experimental branch of computer science to separate the general results from the accidental Standard unit of measurement of developer productivity Codified proven solutions for novices Academic-industrial collaboration to gather data and try things Generalized, reusable software components Certifying software engineers Outsourcing More software development-oriented computer science curricula What were the main contributions of the paper as you (the reader) see it? How could this research be applied in practice? Aside for the primary contributions of the paper as the author sees it, in my opinion, a major contribution of the paper in a practical sense are revelations for improving the culture within software developing organizations. For example, Focus on interchangeability. Follow best practices. Fix not just the bug but also the flaw in the testing process that allowed it to slip through. Value verification in addition to innovation. Pay attention to the difference in competence between employees. Furthermore, as a historical paper, many of its proposals have already materialized. For example, the open-source revolution and collaboration platforms such as GitHub have greatly facilitated gathering data and trying things for research, and has provided a wealth of generalized, reusable software components. How could this research be extended? Implementing and assessing the proposed directions for future work represents a natural extension of this research. How was the work validated? The authors validate their arguments on software's chronic crisis and base their proposals for remedies and future work by analyzing real cases in software engineering, as well as compiling the opinions of experts in the field, including university professors and corporate managers. How does the work apply to you? From a theoretical perspective, as a milestone paper in the domain of software engineering, this paper provides a model research methodology for practical problems within software engineering - analyzing real cases and compiling the opinions of experts. From a practical perspective, this paper identifies core values and skills that us, as practitioners of software engineering, should firmly grasp.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: An empirical study of the reliability of UNIX utilities","slug":"Paper-Reading-An-empirical-study-of-the-reliability-of-UNIX-utilities","date":"2022-09-10T04:00:00.000Z","updated":"2025-08-13T04:31:00.454Z","comments":true,"path":"2022/09/10/Paper-Reading-An-empirical-study-of-the-reliability-of-UNIX-utilities/","link":"","permalink":"https://jifengwu2k.github.io/2022/09/10/Paper-Reading-An-empirical-study-of-the-reliability-of-UNIX-utilities/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. \"An empirical study of the reliability of UNIX utilities\" is the work that spawned research into the domain of software fuzzing. It proposes a technique later known as random fuzzing, testing the reliability of UNIX utilities by feeding them a stream of randomly generated characters and checking whether the program crashed with a core dump or hangs. Although the technique is simple and is not a substitute for formal verification or testing, it is inexpensive and easy to apply. Its effectiveness in identifying bugs and increasing overall system reliability has been proven in many ways. It crashed 25-33% of the utility programs considered to be \"reliable\" on each platform. It was able to find recurring security bugs resulting from bad programming practices that even the best static analysis tools have limited success in detecting, including: Accessing outside the bounds of a buffer Dereferencing a null pointer Unintentionally overwriting data or code Ignoring return codes, especially error-indicating return codes Faulty communication with subprocesses Unintended interaction between modules Improper error handling Signed characters Race conditions during signal handling Its relevance has remained strong over the years. Subsequent studies using the same technique showed that similar problems also existed within other operating systems, such as Microsoft Windows. Even after thirty years, the utility programs in the modern Unix distributions of Linux, macOS, and FreeBSD are still crashing at a noticeable rate and not getting better, as evidenced in \"The Relevance of Classic Fuzz Testing: Have We Solved This One?\" The contributions of this work is multi-fold. As mentioned before, it proposed random fuzzing, an inexpensive, easy to apply, and time-proven way of finding security bugs which is complimentary with formal verification and testing. It spawned research into the domain of software fuzzing. New fuzz tools usually take a gray- or white-box approach, diving deeper into a program's control flow, and they have been applied to many new contexts. However, they often require more advanced specification of the input and/or long execution times to explore the input and program control-flow space. It provides revelations for software engineering: good design, good education, ongoing training, testing integrated into the development cycle, and most importantly, a culture that promotes and rewards reliability. Some personal thoughts after reading the paper. Given the source code of a program and an input, what is the mechanism through which the researchers determine the position where the program crashes and hangs when given the input? This is mentioned in neither \"An empirical study of the reliability of UNIX utilities\" nor its sequel \"The Relevance of Classic Fuzz Testing: Have We Solved This One?\", but is of great practical value. There is a surprising number of security bugs stemming from language defects such as not checking array bounds and dereferencing null pointers, as well as ad-hoc, hacky solutions to recurring problems such as lexical analysis, syntax analysis, structured error handling, as well as graph algorithms including cycle detection, topological sort, etc. Personally, this is not my style of coding. I make extensive a lot of \"safe\" language constructs such as null coalescing, heavily exploit performant and well-tested algorithms within standard libraries and widely-adapted third-party libraries (such as boost in C++ and networkx in Python), and use theoretically sound tools (such as automatically generated LALR parsers for syntax analysis) in software projects. The efficiency, effectiveness, and practical value of these and other solutions, as well as how they can be improved, is an interesting question that comes to my mind after reading this paper.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]}],"categories":[{"name":"Reference","slug":"Reference","permalink":"https://jifengwu2k.github.io/categories/Reference/"},{"name":"Data Science","slug":"Data-Science","permalink":"https://jifengwu2k.github.io/categories/Data-Science/"},{"name":"Fashion","slug":"Fashion","permalink":"https://jifengwu2k.github.io/categories/Fashion/"},{"name":"Health","slug":"Health","permalink":"https://jifengwu2k.github.io/categories/Health/"},{"name":"Multimedia","slug":"Multimedia","permalink":"https://jifengwu2k.github.io/categories/Multimedia/"},{"name":"Linguistics","slug":"Linguistics","permalink":"https://jifengwu2k.github.io/categories/Linguistics/"},{"name":"Development Environments","slug":"Development-Environments","permalink":"https://jifengwu2k.github.io/categories/Development-Environments/"},{"name":"Food and Drink","slug":"Food-and-Drink","permalink":"https://jifengwu2k.github.io/categories/Food-and-Drink/"},{"name":"DevOps","slug":"DevOps","permalink":"https://jifengwu2k.github.io/categories/DevOps/"},{"name":"System Administration","slug":"System-Administration","permalink":"https://jifengwu2k.github.io/categories/System-Administration/"},{"name":"Process Automation","slug":"Process-Automation","permalink":"https://jifengwu2k.github.io/categories/Process-Automation/"},{"name":"Software Design","slug":"Software-Design","permalink":"https://jifengwu2k.github.io/categories/Software-Design/"},{"name":"Python","slug":"Software-Design/Python","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Python/"},{"name":"Reflections","slug":"Reflections","permalink":"https://jifengwu2k.github.io/categories/Reflections/"},{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/"},{"name":"PL Reading Group","slug":"Paper-Reading/PL-Reading-Group","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/PL-Reading-Group/"},{"name":"Mathematics","slug":"Mathematics","permalink":"https://jifengwu2k.github.io/categories/Mathematics/"},{"name":"Research Programming","slug":"Paper-Reading/Research-Programming","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Research-Programming/"},{"name":"Conference and Meeting Minutes","slug":"Conference-and-Meeting-Minutes","permalink":"https://jifengwu2k.github.io/categories/Conference-and-Meeting-Minutes/"},{"name":"Bash","slug":"Software-Design/Bash","permalink":"https://jifengwu2k.github.io/categories/Software-Design/Bash/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"},{"name":"C++","slug":"Software-Design/C","permalink":"https://jifengwu2k.github.io/categories/Software-Design/C/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://jifengwu2k.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]}